[["index.html", "R for Conditional Process Analysis General information on the course How to use this tutorial What can I do if questions arise? What can I do if I have problems with my R code?", " R for Conditional Process Analysis Lara Kobilke, IKMZ, University of Zurich 2022-04-13 General information on the course This online tutorial will accompany the seminar Using Conditional Process Analysis to evaluate communication theories, a B.A. Seminar at the IKMZ (University of Zurich, FS2022). The course takes place via Zoom (Wednesday 10:15-11.45 am, ID: 645 8007 2506, password: lavaan). If you dont want or cant participate in this course from your home office, you can always visit the room AND-2-02 (Andreasstrasse 15, 8050 Zurich) during lecture hours. This is also a great opportunity to meet fellow students in person! You can access all necessary information on the seminars syllabus, important dates and assessments/evaluations via OLAT. There, additional resources (e.g. Powerpoint slides, helpful book resources) are uploaded to the materials folder. Special thanks Very special thanks goes to Valerie Hase, as the first part of this tutorial is a very slightly adapted version of her highly recommendable Text as Data tutorial. You should definitely check out her work, too! How to use this tutorial After completing this tutorial, you will have acquired two important skills: Know how to use R and R studio to complete your data management. Know how to use R and R studio to do Conditional Process Analysis. In the seminar, each session will consist of two parts. In the first part, we will look at communication theories and discuss relationships between communication variables. In the second part, you will work through this tutorial, the accompanying videos, and solve exercises together with fellow students. The tutorial is designed to provide the R skills that you need to put the seminars theoretical discussions into real-world practice. Beginners in R will work through the entire tutorial, while advanced users can skip individual chapters. The goal is to make the individual seminar sessions diverse and to advance your programming skills with as much ease and fun as possible. Each tutorial includes introduction to new functions/analysis methods in R, including corresponding R code information on other tutorials/sources on how to learn these methods exercises which will help you understand and apply your new knowledge (youll work through these with your peers) What can I do if questions arise? Questions are always welcome! Since youll be learning a lot of new things, its perfectly normal to have a lot of questions. If you do not understand something in this tutorial, have questions about exercises, or just want some repetition during the Zoom seminar: Please do ask! The most important thing when learning R is to understand that it is completely normal to feel lost sometimes. Dont worry - its highly likely that everyone else feels the same. Therefore, its key that you ask questions. There are three channels through which you may pose question you have (preferably in this order): Our Zoom sessions: Wednesday, 10:15-11:45. The OLAT forum: Outside of those sessions, please use the OLAT forum to ask any questions. This way, every participant will be able to see your questions, provide answers and see my answers. Thus, everybody will be provided with the same information. I recommend you turn on notifications about new entries in the forum to be informed about ongoing discussions. Image: How to turn on notifications: Email: If you have very specific questions about your own project etc. (or things you may not want to discuss with everyone in class), write me an email: l.kobilke@ikmz.uzh.ch. What can I do if I have problems with my R code? Besides asking a question during the seminar (see What can I do if questions arise?), there are some great places to have a look at when you encounter problems. Ill cover them in the section Help?! in greater detail. To give you a head start, heres a quick rundown of the three best places to look if you have a problem with your code: Rs integrated help function: Use the ?-function whenever possible. Lets assume you struggle with creating a histogram for your data (hist function in R). You can open the R documentation of the hist function in R by writing: ?hist Preview of ?hist in R: Search engines: Like Bing or Google. Yup, programmers and data scientists google all the time! Nobody knows all the code and errors by heart. Often you can find perfect answers to your questions on Stackoverflow, StatsExchange, or Rseek because other people had exactly the same problems. And more importantly, the communities on these websites are very friendly and helpful. Packages reference manuals: Finally, problems with R packages (well get to packages later, see: Packages) can often be solved by looking at their reference manuals (an overview document containing all of a packages functions). For example, you can learn more about dplyr (a data management package that we are going to use later in this tutorial) by visiting its reference manual on a website called CRAN: https://cran.r-project.org/web/packages/dplyr/dplyr.pdf. Thats it. Lets start with our first tutorial: Tutorial: Installing &amp; Understanding R/R Studio "],["tutorial-installing-understanding-rr-studio.html", " 1 Tutorial: Installing &amp; Understanding R/R Studio 1.1 Installing R 1.2 Installing R Studio 1.3 Updating R and R Studio 1.4 How does R work? 1.5 How does R Studio work? 1.6 Take-Aways 1.7 More tutorials on this", " 1 Tutorial: Installing &amp; Understanding R/R Studio After working through Tutorial 1, youll know how to install R and R Studio know how to update R and R Studio understand the main set-up of R Studio 1.1 Installing R R is the programming language well use to import, edit, and analyze data. Please watch one of these two video tutorials before installing R yourself. Video Tutorial for Windows Video Tutorial for Mac When you are ready to install R, use Cran to install the newest version of R (4.1.2, Bird Hippie). Youll have to specify your operation system to download the right version: Installer for Windows Installer for Mac 1.2 Installing R Studio Next, install R Studio. R Studio is a desktop application with a graphical interface that facilitates programming with R. The newest version of R Studio (1.4.1717) can be downloaded via this Link. 1.3 Updating R and R Studio If you have already installed R and RStudio (for example, because you already needed it for a previous seminar), please update your version to the latest version. This way, well all know that our versions are compatible. 1.3.1 On Windows Updating on Windows is tricky. Therefore, you can use a package called installr, which helps you manage your update. First, install the installr package if you dont have it. Use the following code: # installing/loading the package: if(!require(installr)) { install.packages(&quot;installr&quot;); require(installr) } #load / install+load installr After you have installed or loaded the installr package, lets start the updating process of your R installation by using the updateR() function. It will check for newer versions, and if one is available, will guide you through the decisions youd need to make: # using the package: updateR() Finally, update R Studio. Updating RStudio is easy, just open RStudio and go to Help &gt; Check for Updates to install a newer version. 1.3.2 On MAC Go to CRAN and install the newer package intaller. After that update R Studio. Updating RStudio is easy, just open RStudio and go to Help &gt; Check for Updates to install a newer version. 1.4 How does R work? First things first: R is an object- and function-oriented programming language. Chambers (2014, p. 4) has nicely summarized what this means: Everything that exists is an object. Everything that happens is a function call. We assign values (for instance, single numbers/letters, several numbers/letters, or whole data sets) to objects in R to work with them/do computations. For instance, the following command in R would assign the word hello to the object word by using the &lt;- sign (a function used for assigning values to objects): word &lt;- &quot;hello&quot; Objects have specific properties that determine which types of calculations can be done with them (and which cannot). For instance, the object word is characterized by the fact that it consists of characters (i.e., a word) - which may, for instance, prohibit you to calculate the mean of this object (which is something only possible for objects consisting of numeric data). This already hints towards the second, important aspect of R: It is influenced by functional programming, meaning that everything we do in R is a function call. R uses functions to assign specific values (for instance, a single number or word, several numbers or words, whole data sets) to objects. In short, what you will learn by learning programming in R is how to write functions for making R do the calculations you need. Lets take SPSS - which, I assume, many of you have worked with previously - as a comparison. If you import, edit, or analyze a data set in SPSS, youll use the click-and-point menu to change variable values, calculate variables means, or export data. R works differently: You assign the data set to an object - for instance an object called word, as done previously. This assignment enables you to work with the data: You can now call specific functions to work with or change this object. For instance, if you want to add another word to the object word, for instance and good morning, you could do that by using a specific function called paste0(), which takes the original object word and adds your additional words  and good morning to overwrite the old object word: word &lt;- paste0(word, &quot; and good morning&quot;) word ## [1] &quot;hello and good morning&quot; Different to using the click-and-point menu in SPSS, you thus need to write your own code to import, edit, or analyze any type of data in R. Luckily, R already includes a lot of predefined functions meaning that we do not have write all of these functions ourselves. Ouch, that seems awfully complicated. Why should I use R? There are several reasons why Im an advocate of R (or similar programming languages such as Python) over programs such as SPSS. R is free. Other than most other (statistical) programs, you do not need to buy it (or rely on an university license, that is likely to run out once you leave your department). R is an open source program. Other than most other programs, the source code - i.e., the basis of the program - is freely available. So are the hundred of packages (well get to those later - these are basically additional functions you may need for more specific analyses) on CRAN that you can use to extend Rs base functions. R offers you flexibility. You can work with almost any type of data and rely on a large (!) set of functions to import, edit, or analyze such data. And if the function you need to do so hasnt been implemented (or simply does not exist yet), you can write it yourself! Learning R increases your chances on the job market. For many jobs (academia, market research, data science, data journalism), applicants should know at least one programming language. 1.5 How does R Studio work? As mentioned, R studio is a graphical interface which facilitates programming with R. It contains up to four main windows, which allow for different things: Writing your own code (Window 1: Source). Important: When first installing R/R Studio and opening R studio, you may not see this window right away. In this case, simply open it by clicking on File/New File/R Script. Executing your own code (Window 2: Console) Inspecting objects (Window 3: Environment) Visualizing data, searching for help, updating packages etc. (Window 4: Files/Plots/Packages etc.) Image: Four main windows in R Please note that the specific set-up of your R Studio may look different (the order of windows may vary and so may the windows names). I have made the experience that having these four windows open works best for me. This may be different for you. If you want to modify the appearance of your R Studio, simply choose Tools/Global Options/Pane Layout. Image: Changing the Layout 1.5.1 Source: Writing your own code Using the window Source, youll write your own code to execute whichever task you want R to fulfill. 1.5.1.1 Writing Code Lets start with an easy example: Assume you simply want R to print the word hello. In this case, you would first write a simple command that assigns the word hello to an object called word. The assigment of values to named objects is done via either the operator &lt;- or the operator =. The left side of that command contains the object that should be created; its right side the values that should be assigned to this object. In short, this command tells R to assign the world hello to an object called word. word &lt;- &quot;hello&quot; Image: Source 1.5.1.2 Annotating Code Another helpful aspect of R is that you can comment your own code. Oftentimes, this is very helpful for understanding your code later (if you write several hundred lines of codes, you may not remember their exact meaning months later). Comments or notes can be made via hashtags #. Anything following a hashtag will not be considered code by R but be ignored instead. word &lt;- &quot;hello&quot; #this line of code assigns the word &quot;hello&quot; to an object called word 1.5.1.3 Executing Code We now want to execute our code. Doing so is simple: Mark the parts of the code you want to run (for instance, single rows of code or blocks of code across several rows) Either press Run (see upper right side of the same window) or press Ctrl + Enter (On Mac OS X, hold the command key and press return instead). R should now execute exactly those lines of codes that you marked (hereby creating the object word). If you havent marked any specific code, all lines of code will be executed. Image: Executing Code 1.5.2 Console: Printing results Results of executing code are printed in a second window called Console, which includes the code you ran and the object you may have called when doing so. Previously, we defined an object called word, which consists of the single word hello. Thus, R prints our code as well as objects called when running this code (here, the object word) in the console. word &lt;- &quot;hello&quot; word ## [1] &quot;hello&quot; Image: Window Console 1.5.3 Environment: Overview of objects The third window is called Environment1. This windows displays all the objects currently existing - in our case, only the object word. As soon as you start creating more objects, this environment will fill up. If youre an SPSS user, this window is very similar to what is called the Datenansicht / Data overview in SPSS. However, the R version of this is much more flexible, given that our environment can contain several data sets, for example, at the same time. Image: Window Environment It is important to know that we can visually inspect any object using the View() command (with a new tab then opening in the Source window). This isnt super helpful right now - but if you work with bigger data sets with several observations/variables later on, it is often useful to inspect data visually. View(word) Image: Window View 1.5.4 Plots/Help/Packages: Do everything else Lastly, the standard R Studio interface contains a fourth window (if you opted for this layout). In my case, the window contains several sub-sections called Files, Plots, or Packages among others. Youll understand their specific functions later - the window can, for instance, be used to plot/visualize results or see which packages are currently loaded. Image: Window Files/Plots/Packages 1.6 Take-Aways Window Source: used to write/execute code in R Window Console: used to return results of executed code Window Environment: used to inspect objects on which to use functions Window Files/Plots/Packages etc.: used for additional functions, for instance visualizations/searching for help/activating or updating packages 1.7 More tutorials on this You still have questions? The following tutorials &amp; papers can help you with that: YaRrr! The Pirates Guide to R by N.D.Phillips, Tutorial 2 Computational Methods in der politischen Kommunikationsforschung by J. Unkel, Tutorial 1 SICSS Boot Camp by C. Bail, Video 1 wegweisR by M. Haim, Video 1 R Cookbook by Long et al., Tutorial 1 Lets keep going: [Tutorial 2: Workflow in R] again, this only applies for the way I set up my R Studio. You can change this via Tools/Global Options/Pane Layout "],["tutorial-workflow-in-r.html", " 2 Tutorial: Workflow in R 2.1 Defining your working directory 2.2 Packages 2.3 Help?! 2.4 Saving, loading &amp; cleaning code/results 2.5 More tutorials on this", " 2 Tutorial: Workflow in R Tutorial 2 will not yet deal with how to write code and do your own analyses in R (dont worry, well get there soon!). Before you write your own code, you should understand the basic workflow when working with R and R Studio - independent of whether you want to calculate a regression model, do an automated content analysis, or visualize results of an analysis. After working through Tutorial 2, youll understand the basic work flow in R. 2.1 Defining your working directory The first step of any type of analysis is to define your working directory. You may wonder: Whats that? Your working directory is the folder from which data can be imported into R or to which you can export and save data created with R. Create a folder that you want to use as your working directory for this tutorial (or use an existing one, that also works). Go to that folder and copy the path to it: Image: Working Directory Image: Copy Working Directory Now you know where this working directory is located - but R should know, too! Telling R from which folder to import data or where to export data to is also called setting your working directory. We call a function called setwd() (you guessed right: short for setting you working directory) which allows us to do exactly that. Important: The way this working directory is set differs between Windows- and Mac-Operating Systems. Windows: The dashes need to be pointing towards the right direction (if you simply copy the path to the folder, you may need to replace these signs \\ with /) setwd(&quot;C:/Users/LaraK/Documents/CPA-Seminar&quot;) Mac: You may need to add a / at the beginning like so: setwd(&quot;/Users/LaraK/Documents/CPA-Seminar&quot;) If you have forgotten where you set your working directory, you can also ask R about the path of your current working directory with getwd(): getwd() ## [1] &quot;C:/Users/LaraK/Documents/CPA-Seminar&quot; 2.2 Packages Ive been talking about packages before: While Base R, i.e., the standard version of R, already includes many helpful functions, you may at times need other, additional functions. For instance, in the case of conditional process analysis - the method, well focus on in this seminar - well need to use specific packages including additional functions. Packages are collections of topic-specific functions that extend the functions implemented in Base R. In the spirit of open science, anyone can write and publish these additional functions and related packages and anyone can also access the code used to do so. Youll find a list of all of R packages here. In this seminar, well for instance use packages like ProcessR or lavaan for conditional process analysis. 2.2.1 Installing packages To use a package, you have to install it first. Lets say youre interested in using the package ProcessR. Using the command install.packages(), you can install the package on your computer. Youll have to give the function the name of the package you are interested in installing. install.packages(&quot;processR&quot;) Now the package has been installed on your computer and is accessible locally. We only have to use install.packages() for any package once. Afterwards, the only thing youll have to do after open R is to activate the already installed package - which well learn next. 2.2.2 Activating packages Before we are able to use a package, we need to activate it in each session. Thus, you should not only define a working directory at the beginning of each session but also activate the packages you want to use via the library()_ command. Again, youll have to give R the name of the package you want to activate: library(&quot;processR&quot;) Else, you can also use the name of the package followed by two colons :: to activate a package directly before calling one of its function. For instance, I do not need use to activate the ProcessR package (by using the library() function) to use the function meanCentering() if I use the following code: processR::meanCentering() 2.2.3 Getting information about packages The package is installed and activated - but how can we use it? To get an overview of functions included in a given package, you can consult its corresponding reference manual (overview document containing all of a packages functions) or, if available, its vignette (tutorials on how to use selected functions for the corresponding package) provided by a packages author on a website called CRAN. The easiest way to finding these manuals/vignettes is Google: Simply google CRAN ProcessR, for instance, and youll be guided to the following website: Image: Cran Overview ProcessR package The first paragraph (circled in red) gives you an overview of aspects for which this package may be useful. The second red-circled area links to the reference manual and the vignette. You can, for instance, check out the reference manual to get an idea of the many functions the processR package contains. Another way of getting there is to simply use the help()-function provided by R, which well get to now. 2.3 Help?! The one thing you can count on in this seminar is that many things will not work right away: Youll forget commands or what to use them for, the name of packages you need, or be confronted with errors messages that you need to understand to fix a given problem. This happens to anyone: from beginners to those having worked with R for many years. In this case, you need: help(). 2.3.1 Finding information about packages If you have a package installed and youre interested in this specific package, you can also use R and the help() function (or simply use ?, which leads to the same result): help(dplyr) #Version 1 of asking for help ?dplyr #Version 2 of asking for help In turn, youll get more information via the window Help: Image: Overview for the dplyr package 2.3.2 Finding information about functions Oftentimes, you need help with a specific function. Ill give you an example: Lets say I teach a seminar with 10 students. I have asked all of them about their age. I have now saved their answers (i.e., 10 different numbers) in an object called age. This object is a vector, i.e. an object that consists of several values of the same data type - well get to this in Tutorial: Objects &amp; structures in R. age &lt;- c(23, 26, 19, 28, 24, 22, 21, 27, 24, 24) Now we want R to compute the mean age of students in the seminar using the mean() function. We thus ask R to compute the mean of the vector age like so: We call the function mean(). We specify all necessary conditions to run it - here that x = age, i.e. that R should compute the mean of all values in the vector age: mean(x = age) ## [1] 23.8 That looks good - R tells us that the mean age of our students is 23.8 years. Lets say I did the same thing for a different seminar: I also asked students about their age. while most chose to answer, some refused to answer. Thus, I recorded missing answers as NA (NA is used to record missing values, short for not available). age &lt;- c(23, 26, NA, 28, 24, 22, 21, NA, 24, NA) mean(x = age) ## [1] NA However, when trying to get the students mean age, R tells us that the mean is NA (i.e., missing). But do we really only have missing values? Lets inspect our data again: age ## [1] 23 26 NA 28 24 22 21 NA 24 NA Thats not true: 7 out of 10 students told us their age; only 3 refused to answer (here recorded as NA). So why does R tell us that the overall mean is missing - shouldnt the function simply ignore NAs and tells us the mean age of all of those 7 students who answered our question? To do some troubleshooting, we use the help() function. We specify for which function we need help: ?mean This is where our fourth window comes into place as results for our search for help are depicted here (the paragraph depicted here is the reference manual including information on the mean() function). Image: Help for error with mean()-function It includes important information on the function (of which well discuss only some, namely those circled in red): Description: explains for which types of tasks the function mean() should be used Usage: explains how the function mean() should be used Arguments: explains which elements need to be or can be defined for using mean() and how these elements need to be specified Examples: exemplifies how the function mean() can be used When inspecting the section Arguments, well soon discover something very important: mean() is a function that needs an object x for which the mean should be calculated. In this case, we specified x to consist of the vector age by typing x = age. mean(x = age) Upon further inspection, however, we see something else: The mean() function needs more information. In particular, we have to specify how R should deal with missing values, here NAs (see the section circled in red). This wasnt a problem in the first example (since we had no NAs), but seems to be a problem for the second example. The manual reads as follows: na.rm: a logical value indicating whether NA values should be stripped before the computation proceeds. This indicates that if our x contains any NAs, we need to tell R and the mean() function how to deal with these. We havent specified this yet, which is why R includes all missing values for calculation and thus tells us that - given that some values are missing - the mean is missing. If we want R to ignore all NAs, we need to actively set na.rm (short for removal of NAs) to TRUE. This tells R that the mean should be computed for all of those values for x that are not missing. The following command therefore gives us the mean age of all those students who chose to answer the question: mean(age, na.rm = TRUE) ## [1] 24 2.3.3 Searching for help online For some questions, using the help()-function wont cut it. In this case, Google is your new best friend. Ive almost never encountered a problem that others havent had and asked for answers online (and usually received a helpful response). When googling, look out for the following websites that often offer help for statistical/programming issues: Stackoverflow StatsExchange Rseek 2.3.3.1 Make sure to use relevant search terms When googling, make sure to use all relevant search terms. This includes at least: parts of the error message you are receiving or descriptions of the error the search term R (there are a lot of other programming languages and you should make sure that your answers are tailored to R) the function throwing the error Lets say you are trying to find out how to set your working directory since your R throws the following error: cannot find directory. Googling for help via search terms such as directory programming define will likely lead to insufficient results because: (a) the specific command you are having trouble with is missing, (b) the specific error message you are getting is missing, (c) the search request does not specify that you need answers for the programming language R. A better way to go around this would be something like: setwd() R error message cannot find directory: (a) you are specifying the command that gives you trouble, (b) you are specifying the error message, and (c) you are specifying that you want answers for R. 2.3.3.2 Dont trust every result you get While most Google searches will get you a multitude of different answers for your questions, not all of them are necessarily right for your specific problem. Moreover, there may be different solutions for the same problem - so dont be confused when people are proposing different approaches. Contrary to common conception, the internet is not always right - you may also get answers that are wrong or inefficient. Its often best to scroll through some search results and then try the solution that seems most understandable and/or suitable for you. 2.3.3.3 Make your problem reproducible It is often vital that others can reproduce your problem: Others need to see which lines of codes exactly created an error message, what the error message looked liked, which data you used, and on which type of machine/system you ran the analysis to help. For instance: Nobody is likely going to be able to help you with a request like this &quot;If I try to set my working directory, my computer tells me that I can&#39;t (the error says: Error: unpexted input in setwd(C:\\. What is the problem?&quot; This isnt great because no one knows the code that created the problem or the machine/system you used. Thus, you need to make your error replicable by giving the exact command and potentially information about your machine via sessionInfo(): &quot;I am trying to set my working directory on a Windows System using the following code: setwd(C:\\Users\\LaraK\\Documents\\CPA-Seminar) While the path to the folder that I want to be my working directory is definitely correct, R gives me the following error message: Error: unpexted input in setwd(C:\\.&quot; sessionInfo() ## R version 4.1.2 (2021-11-01) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 19044) ## ## Matrix products: default ## ## locale: ## [1] LC_COLLATE=English_United States.1252 LC_CTYPE=English_United States.1252 LC_MONETARY=English_United States.1252 ## [4] LC_NUMERIC=C LC_TIME=English_United States.1252 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] lavaan_0.6-11 ggpubr_0.4.0 processR_0.2.6 forcats_0.5.1 stringr_1.4.0 purrr_0.3.4 readr_2.1.2 ## [8] tibble_3.1.6 tidyverse_1.3.1 ggplot2_3.3.5 tidyr_1.2.0 dplyr_1.0.8 extrafont_0.17 bookdown_0.24 ## [15] rsconnect_0.8.25 ## ## loaded via a namespace (and not attached): ## [1] utf8_1.2.2 tidyselect_1.1.2 lme4_1.1-28 htmlwidgets_1.5.4 grid_4.1.2 ztable_0.2.3 ## [7] editData_0.1.8 diagram_1.6.5 miscTools_0.6-26 jtools_2.1.4 munsell_0.5.0 effectsize_0.6.0.1 ## [13] DT_0.21 miniUI_0.1.1.1 ggiraphExtra_0.3.0 withr_2.5.0 colorspace_2.0-3 highr_0.9 ## [19] knitr_1.37 uuid_1.0-3 rstudioapi_0.13 stats4_4.1.2 ggsignif_0.6.3 officer_0.4.1 ## [25] shinyWidgets_0.6.4 Rttf2pt1_1.3.10 labeling_0.4.2 Rdpack_2.3 emmeans_1.7.2 mnormt_2.0.2 ## [31] farver_2.1.0 datawizard_0.2.3 moonBook_0.3.1 coda_0.19-4 vctrs_0.3.8 generics_0.1.2 ## [37] xfun_0.29 R6_2.5.1 ggiraph_0.8.2 predict3d_0.1.3.3 assertthat_0.2.1 promises_1.2.0.1 ## [43] scales_1.1.1 nnet_7.3-16 gtable_0.3.0 sandwich_3.0-1 rlang_1.0.1 systemfonts_1.0.4 ## [49] sjPlot_2.8.10 splines_4.1.2 rstatix_0.7.0 extrafontdb_1.0 mycor_0.1.1 broom_0.7.12 ## [55] checkmate_2.0.0 rgl_0.108.3 yaml_2.3.5 reshape2_1.4.4 prediction_0.3.14 abind_1.4-5 ## [61] modelr_0.1.8 backports_1.4.1 httpuv_1.6.5 Hmisc_4.6-0 tools_4.1.2 collapse_1.7.6 ## [67] psych_2.1.9 ellipsis_0.3.2 jquerylib_0.1.4 RColorBrewer_1.1-2 polynom_1.4-0 devEMF_4.0-2 ## [73] Rcpp_1.0.8 plyr_1.8.6 base64enc_0.1-3 rpart_4.1-15 zoo_1.8-9 haven_2.4.3 ## [79] ggrepel_0.9.1 cluster_2.1.2 fs_1.5.2 magrittr_2.0.2 data.table_1.14.2 openxlsx_4.2.5 ## [85] flextable_0.6.10 reprex_2.0.1 lmtest_0.9-40 tmvnsim_1.0-2 mvtnorm_1.1-3 sjmisc_2.8.9 ## [91] hms_1.1.1 mime_0.12 evaluate_0.15 xtable_1.8-4 rio_0.5.29 sjstats_0.18.1 ## [97] jpeg_0.1-9 readxl_1.3.1 gridExtra_2.3 ggeffects_1.1.1 shape_1.4.6 compiler_4.1.2 ## [103] bdsmatrix_1.3-4 crayon_1.5.1 minqa_1.2.4 htmltools_0.5.2 mgcv_1.8-38 later_1.3.0 ## [109] tzdb_0.2.0 semTools_0.5-5 Formula_1.2-4 lubridate_1.8.0 DBI_1.1.2 rrtable_0.2.1 ## [115] sjlabelled_1.1.8 dbplyr_2.1.1 ppcor_1.1 MASS_7.3-54 boot_1.3-28 Matrix_1.3-4 ## [121] car_3.0-12 cli_3.2.0 rbibutils_2.2.7 parallel_4.1.2 insight_0.16.0 pkgconfig_2.0.3 ## [127] foreign_0.8-81 xml2_1.3.3 pbivnorm_0.6.0 bslib_0.3.1 plm_2.6-1 estimability_1.3 ## [133] rvg_0.2.5 rvest_1.0.2 digest_0.6.29 parameters_0.16.0 rmarkdown_2.11 cellranger_1.1.0 ## [139] htmlTable_2.4.0 gdtools_0.2.4 maxLik_1.5-2 curl_4.3.2 shiny_1.7.1 nloptr_2.0.0 ## [145] lifecycle_1.0.1 nlme_3.1-153 jsonlite_1.8.0 carData_3.0-5 interactions_1.1.5 fansi_1.0.2 ## [151] pillar_1.7.0 lattice_0.20-45 httr_1.4.2 fastmap_1.1.0 survival_3.2-13 glue_1.6.2 ## [157] bayestestR_0.11.5 zip_2.2.0 png_0.1-7 pander_0.6.4 stringi_1.7.6 sass_0.4.0 ## [163] performance_0.8.0 latticeExtra_0.6-29 2.3.4 Interrupting R Some commands run for a longer time - and you may realize while they are running that the code still contains an error. In this case, you may want to stop R in executing the command. If you want to do this manually, you can use the stop button in the window Console (only visible while R is executing code). Image: Interrupting R Else, you can use the menu via Session / Interrupt R. 2.4 Saving, loading &amp; cleaning code/results 2.4.1 Saving your code A great feature of R is that it makes analyses easily reproducible - given that you save your code. When reopening R Studio and you script, you can simply rerun the code with one click and your analysis will be reproduced. To save code, you have two options: Choose the menu option File/Save as. Important: Code needs to be saved with the ending .R. Chose the Save-button in the source window and save your code in the correct format, for instance as MyCode.R. Image: Saving code 2.4.2 Saving your results You have successfully executed all commands and now want R to save your results/working environment? Saving your results is especially useful if it takes some time to have R run through the code and reproduce results - in this case, you only need to save results once and can then load them for the next session. Again, there are several options for saving your results: Use the save.image()-command: save.image(&quot;MyData.RDATA&quot;) Use the save-button in the environment window and save your results in the correct format, for instance as MyData.RDATA. Image: Saving results 2.4.3 Loading working spaces Having saved results in a previous session, you can now easily import them in a new session. Using the load()-command, you can import working spaces into a new R session. Here, we first define the working directory in which R may find our results to then import results: setwd(&quot;C:/Users/LaraK/Documents/CPA-Seminar&quot;) load(&quot;MyData.RDATA&quot;) 2.4.4 Clean your working space After some time, your environment may be a bit messy: You may have defined objects you no longer need, which may lead to loosing sight of the things that are important. In this case, you can easily sort through relevant data and clean your working space. For instance, the rm()-command deletes specific objects in your environment. Say we want to save the object age since we no longer need it: rm(age) If you want a fresh start, you can also delete all objects in your environment. By specifying an empty lists of objects, ls(), as the element to be deleted via rm(), all objects get deleted: rm(list = ls()) 2.4.5 Take Aways Working Directory: The folder from which data can be imported into R or to which you can export and save data created with R. Should be defined at the beginning of each session. Commands: setwd(), getwd() Packages: Collections of topic-specific functions that extend the functions implemented in Base R. You only need to install them once on your computer - but you have to activate packages at the beginning of each session. Otherwise, R will not be able to find related functions. Commands: install.packages(), library() Help: The thing everyone working with R needs. Its normal to run into errors when working with R - dont get frustrated too easily. Commands: ?, help() Saving, loading, and cleaning code/results: You should save your code/results from time to time to be able to replicate analyses. Commands: save.image, load(), rm() 2.5 More tutorials on this You still have questions? The following tutorials &amp; papers can help you with that: Computational Methods in der politischen Kommunikationsforschung by J. Unkel, Tutorial 5 &amp; 6 R Cookbook by Long et al., Tutorial 2 YaRrr! The Pirates Guide to R by N.D.Phillips, Tutorial 2 &amp; 9 Cheat sheet: Base R The tidyverse style guide Lets keep going: Tutorial: Using R as a calculator "],["tutorial-using-r-as-a-calculator.html", " 3 Tutorial: Using R as a calculator 3.1 Using variables for calculation 3.2 Using vectors for calculation 3.3 Take-Aways", " 3 Tutorial: Using R as a calculator After working through Tutorial 3, youll be able to work with mathematical operators in R be able to use mathematical operators on variables and vectors One of the first things everyone learns in R is to use R as a calculator. You have access to many mathematical operators in R (e.g. +, -, *, /, ^). Lets try some of them. Addition: 5+7 ## [1] 12 Subtraction: 12-7 ## [1] 5 Exponentiation: 3^3 ## [1] 27 3.1 Using variables for calculation You can also assign numbers to variables with the assign operator &lt;-. We have already talked about assigning word or numbers to variables in the chapter Writing Code. Please remember that a variable name in R can include numeric and alphabets along with special characters like dot (.) and underline (_). Therefore, these are good options to name your variables: my_1st_number &lt;- 3 my.1st.numer &lt;- 3 Do !not! use these variable names because they will cause errors and throw warning messages. I have therefore put the code as annotation to avoid the warning messages (with #): # _number &lt;- 3 # .number &lt;- 3 # my-1st-number &lt;- 3 You can use variables in your calculations by assigning the numbers to variables (i.e. store the numerical value in the variable). five &lt;- 5 seven &lt;- 7 twelve &lt;- five + seven # here you add the two variables in which the numbers are stored. The result of the addition is stored in the variable &quot;twelve&quot; twelve # now you have to retrieve the content of the variable, so that the result is printed to the console ## [1] 12 The names of the variables are freely selectable. For example, you can also proceed like this: three &lt;- 5 three # print the content of the variable to the console ## [1] 5 3.2 Using vectors for calculation You can also store more than one number in a variable. We call this process creating vectors because variables that contain more than one number are called vectors in R (well get to vectors in Tutorial: Objects &amp; structures in R). Vectors are created using the combine function c() in R. twelve &lt;- c(1,2,3,4,5,6,7,8,9,10,11,12) twelve # print the content of the variable to the console ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 Again, the variable name is chosen arbitrarily. You can also do this: twelve &lt;- c(4,10,15,21,33) twelve# print the content of the variable to the console ## [1] 4 10 15 21 33 You can use mathematical operations on vectors (e.g., +, -, * and /). Lets create two vectors weight and height that contain the weight and height measures of 6 individuals. For example, the first individual weighs 60 kg and is 1.75 m tall: weight &lt;- c(60, 72, 57, 90, 95, 72) height &lt;- c(1.75, 1.80, 1.65, 1.90, 1.74, 1.91) Now we can calculate the Body Mass Index (BMI) using the BMI formula: BMI &lt;- weight/height^2 BMI # print the content of the BMI variable to the console ## [1] 19.59184 22.22222 20.93664 24.93075 31.37799 19.73630 Now we know that the first person has a BMI of 19.59, which is within the range of normality (18.5 and 24.9). 3.3 Take-Aways Mathematical operators: use +, -, *, /, ^ Use case: use these operators on numbers, variables, and vectors Now that we can use R as a calculator and have been introduced to the concept of variables and vectors, lets delve into the concept of objects an structures in R: Tutorial: Objects &amp; structures in R "],["tutorial-objects-structures-in-r.html", " 4 Tutorial: Objects &amp; structures in R 4.1 Types of data 4.2 Types of objects 4.3 Take Aways 4.4 More tutorials on this", " 4 Tutorial: Objects &amp; structures in R After working through Tutorial 4, youll know about types of data in R (numbers, text, factors, dates, logical/other operators) know about types of objects in R (scalars, vectors, matrices, data frames, lists) 4.1 Types of data Objects in R can contain a variety of different types of data. Well get to know a selection of these, namely Numbers Text Factors Dates Missing data/NAs Logical/other operators To understand and work with these types of data, well now import a data set. The data set is called data_tutorial3.csv via OLAT/Materials/02_02.03.22/Data. The data set consists of data that is completely made up - a survey with 20 fictional students in a fictional seminar. We only use this data here so you can understand differences between types of data and types of objects. The datafile data_tutorial3.csv is structured as follows: Each row contains the answer for a single student. Each column contains all values given by students for a single variable. The variables included here are: name: the name of each student age: the age of each student (in years) date: the date on which each student was surveyed (in YYYY-MM-DD) outlet: the type of media outlet each students mainly uses to get information outlet_use: the time each student uses this media outlet daily (in hours) outlet_trust: how much each student trusts this media outlet (from 1 = not at all to 5 = very much) Well read in the file with read.csv(). Here, we specify where to find the data file with the argument x as well as that the first row contains variable names with the argument header = TRUE2. survey &lt;- read.csv2(&quot;data_tutorial3.csv&quot;, header = TRUE) survey ## X.2 X.1 X name age date outlet outlet_use outlet_trust ## 1 1 1 1 Alexandra 20 2021-09-09 TV 2 5 ## 2 2 2 2 Alex 25 2021-09-08 Online 3 5 ## 3 3 3 3 Maximilian 29 2021-09-09 Zeitung 4 1 ## 4 4 4 4 Moritz 22 2021-09-06 TV 2 2 ## 5 5 5 5 Vanessa 25 2021-09-07 Online 1 3 ## 6 6 6 6 Andrea 26 2021-09-09 Online 3 4 ## 7 7 7 7 Fabienne 26 2021-09-09 TV 3 2 ## 8 8 8 8 Fabio 27 2021-09-09 Online 0 1 ## 9 9 9 9 Magdalena 8 2021-09-08 Online 1 4 ## 10 10 10 10 Tim 26 2021-09-07 TV NA 2 ## 11 11 11 11 Alex 27 2021-09-09 Online NA 2 ## 12 12 12 12 Tobias 26 2021-09-07 Online 2 2 ## 13 13 13 13 Michael 25 2021-09-09 Online 3 2 ## 14 14 14 14 Sabrina 27 2021-09-08 Online 1 2 ## 15 15 15 15 Valentin 29 2021-09-09 TV 1 5 ## 16 16 16 16 Tristan 26 2021-09-09 TV 2 5 ## 17 17 17 17 Martin 21 2021-09-09 Online 1 2 ## 18 18 18 18 Anna 23 2021-09-08 TV 3 3 ## 19 19 19 19 Andreas 24 2021-09-09 TV 2 5 ## 20 20 20 20 Florian 26 2021-09-09 Online 1 5 4.1.1 Accessing variables in data sets Variables that are part of a data set  also called: data frame (well come back to data frames a bit later in Data frames &amp; matrices)  can be accessed by their name, but we need to specify the data set and the variable name and combine them with the access operator: $. This takes the form of: dataframe$variablename For instance, we could retrieve the variable name in our survey data set by simply using its variable name: We specify the object we want to access, the data frame survey and then retrieve the column name via the operator $: survey$name ## [1] &quot;Alexandra&quot; &quot;Alex&quot; &quot;Maximilian&quot; &quot;Moritz&quot; &quot;Vanessa&quot; &quot;Andrea&quot; &quot;Fabienne&quot; &quot;Fabio&quot; &quot;Magdalena&quot; ## [10] &quot;Tim&quot; &quot;Alex&quot; &quot;Tobias&quot; &quot;Michael&quot; &quot;Sabrina&quot; &quot;Valentin&quot; &quot;Tristan&quot; &quot;Martin&quot; &quot;Anna&quot; ## [19] &quot;Andreas&quot; &quot;Florian&quot; 4.1.2 Numbers A typical type of data you may come is numeric data. In our data set, the age of respondents is saved as numbers. In R, this type of data is called numeric. To check if age is really saved as numbers, you can use mode(). The function tells you an objects type. mode(survey$age) ## [1] &quot;numeric&quot; 4.1.3 Text Letters and text is saved in the character format. In our data set, we know that the variable name is likely to contain textual content. Well check this with the same command mode(): #find out which type of data the object word consists of mode(survey$name) ## [1] &quot;character&quot; You may have already spotted something else: If you want to save something in character format, you have to set quotation marks  before and after the respective values. For instance, if you save the number 1 with quotation marks, R will use the character format: word &lt;- &quot;1&quot; mode(word) ## [1] &quot;character&quot; However, if you save the number 1 without quotation marks, R will use the numeric format: word &lt;- 1 mode(word) ## [1] &quot;numeric&quot; Why is this important? In R, many functions for conducting statistical analyses, for example, can only be executed with objects containing numeric data, not character data. For instance: Lets say you want R to calculate the mean of some numbers: numbers &lt;- c(&quot;1&quot;, &quot;8&quot;, &quot;9&quot;, &quot;4&quot;) mean(numbers) ## [1] NA R will throw you an error message. Why? Because you saved these numbers as character values, not as numeric values: mode(numbers) ## [1] &quot;character&quot; If we save these numbers in a numeric format, i.e., as numbers not text, the command works just fine: numbers &lt;- c(1,8,9,4) mean(numbers) ## [1] 5.5 4.1.4 Factors Factors constitute an additional type of data. Factors can include both numeric data and characters. What is important, however, is that they treat any types of data as categorical. We can, for instance, convert the variable name - which now contains each students name in character format - to a factor using as.factor(): survey$name &lt;- as.factor(survey$name) As you can see, the variable age now contains each name as an independent level. You can get each unique level with the levels() command: levels(survey$name) ## [1] &quot;Alex&quot; &quot;Alexandra&quot; &quot;Andrea&quot; &quot;Andreas&quot; &quot;Anna&quot; &quot;Fabienne&quot; &quot;Fabio&quot; &quot;Florian&quot; &quot;Magdalena&quot; ## [10] &quot;Martin&quot; &quot;Maximilian&quot; &quot;Michael&quot; &quot;Moritz&quot; &quot;Sabrina&quot; &quot;Tim&quot; &quot;Tobias&quot; &quot;Tristan&quot; &quot;Valentin&quot; ## [19] &quot;Vanessa&quot; You may have noted that the data set survey contains 20 observations but that the variable age only included 19 levels. Why? If you inspect the data set, you can see that two students have the same name (here, Alex in row 2 and 11) - which is why we have 20 observations, but only 19 unique levels for the variable name: survey ## X.2 X.1 X name age date outlet outlet_use outlet_trust ## 1 1 1 1 Alexandra 20 2021-09-09 TV 2 5 ## 2 2 2 2 Alex 25 2021-09-08 Online 3 5 ## 3 3 3 3 Maximilian 29 2021-09-09 Zeitung 4 1 ## 4 4 4 4 Moritz 22 2021-09-06 TV 2 2 ## 5 5 5 5 Vanessa 25 2021-09-07 Online 1 3 ## 6 6 6 6 Andrea 26 2021-09-09 Online 3 4 ## 7 7 7 7 Fabienne 26 2021-09-09 TV 3 2 ## 8 8 8 8 Fabio 27 2021-09-09 Online 0 1 ## 9 9 9 9 Magdalena 8 2021-09-08 Online 1 4 ## 10 10 10 10 Tim 26 2021-09-07 TV NA 2 ## 11 11 11 11 Alex 27 2021-09-09 Online NA 2 ## 12 12 12 12 Tobias 26 2021-09-07 Online 2 2 ## 13 13 13 13 Michael 25 2021-09-09 Online 3 2 ## 14 14 14 14 Sabrina 27 2021-09-08 Online 1 2 ## 15 15 15 15 Valentin 29 2021-09-09 TV 1 5 ## 16 16 16 16 Tristan 26 2021-09-09 TV 2 5 ## 17 17 17 17 Martin 21 2021-09-09 Online 1 2 ## 18 18 18 18 Anna 23 2021-09-08 TV 3 3 ## 19 19 19 19 Andreas 24 2021-09-09 TV 2 5 ## 20 20 20 20 Florian 26 2021-09-09 Online 1 5 4.1.5 Dates Since well be working with textual data and are often interested in when these texts are published, well need to know about dates. The date format can include any type of information on time. Lets check out an example using the variable date in our data set, which specifies the day on which students were surveyed. survey$date ## [1] &quot;2021-09-09&quot; &quot;2021-09-08&quot; &quot;2021-09-09&quot; &quot;2021-09-06&quot; &quot;2021-09-07&quot; &quot;2021-09-09&quot; &quot;2021-09-09&quot; &quot;2021-09-09&quot; &quot;2021-09-08&quot; ## [10] &quot;2021-09-07&quot; &quot;2021-09-09&quot; &quot;2021-09-07&quot; &quot;2021-09-09&quot; &quot;2021-09-08&quot; &quot;2021-09-09&quot; &quot;2021-09-09&quot; &quot;2021-09-09&quot; &quot;2021-09-08&quot; ## [19] &quot;2021-09-09&quot; &quot;2021-09-09&quot; When inspecting our data set, we can see that the variable is saved in the following format: YYYY-MM-DD (i.e., the year followed by the month and day on which a student was surveyed). However, as of yet this date is saved in character format. For dates and times, that format is not too helpful. If we want to work with dates in a more flexible way, we should save them in the date format using the as.Date() format: survey$date &lt;- as.Date(survey$date) 4.1.6 Missing data/NAs Something we will often encounter is missing data, also called NAs. In our data set, the variable outlet_use which describes how many hours a day a student uses a specific outlet contains missing data: survey$outlet_use ## [1] 2 3 4 2 1 3 3 0 1 NA NA 2 3 1 1 2 1 3 2 1 When working with data, you should always check how much of it may be missing (and of course, why). To do so, you can use the is.na() command: It tells you which values in a specific objects are missing (TRUE) or not missing (FALSE): is.na(survey$outlet_use) ## [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE For instance, the first 9 observations do not contain any missing values for the variable age, but the 10th observation does. With larger data sets, you obviously would not want to check this manually for every single observation. We can use a little trick here: Since the value FALSE counts as 0 and the value TRUE counts as a 1 as.numeric(FALSE) ## [1] 0 as.numeric(TRUE) ## [1] 1  we can simply calculate the sum of these values for survey$outlet_use to see whether how many observations are missing: sum(is.na(survey$outlet_use)) ## [1] 2 4.1.7 Logical/other operators Lastly, you should know about logical (or other) operators. These are often used to check whether certain statements are true/false, whether some values take on a certain value or not, or whether some values are higher/lower than others. Since we wont need this right away,I wont go into details here - but we will need these later: Table 4.1: Logical/other operators Operator Meaning TRUE indicates that a certain statement applies, i.e., is true FALSE indicates that a certain statement does not apply, i.e., is not true &amp; connects two statements which should both be true | connects two statements of which at least one should be true == indicates that a certain value should equal another one != indicates that a certain value should not equal another one &gt; indicates that a certain value should be larger than another one &lt; indicates that a certain value should be smaller than another one &gt;= indicates that a certain value should be larger than or equal to another one &lt;= indicates that a certain value should be smaller than or equal to another one You may wonder: Why should I use this? Glad you asked: Well need these operators, for instance, to filter data sets by specific values or apply functions if some conditions (but not others) are true. For instance, you could use a logical operator to only keep those students in the data set that are older than 21 (17 out of 20 students). We filter the data set using the &gt; operator. We can get basic information about this data set using the str() command: survey_subset &lt;- survey[survey$age&gt;21,] str(survey_subset) ## &#39;data.frame&#39;: 17 obs. of 9 variables: ## $ X.2 : int 2 3 4 5 6 7 8 10 11 12 ... ## $ X.1 : int 2 3 4 5 6 7 8 10 11 12 ... ## $ X : int 2 3 4 5 6 7 8 10 11 12 ... ## $ name : Factor w/ 19 levels &quot;Alex&quot;,&quot;Alexandra&quot;,..: 1 11 13 19 3 6 7 15 1 16 ... ## $ age : int 25 29 22 25 26 26 27 26 27 26 ... ## $ date : Date, format: &quot;2021-09-08&quot; &quot;2021-09-09&quot; &quot;2021-09-06&quot; &quot;2021-09-07&quot; ... ## $ outlet : chr &quot;Online &quot; &quot;Zeitung&quot; &quot;TV&quot; &quot;Online &quot; ... ## $ outlet_use : int 3 4 2 1 3 3 0 NA NA 2 ... ## $ outlet_trust: int 5 1 2 3 4 2 1 2 2 2 ... 4.2 Types of objects Great, now you know all about types of data. As promised, this tutorial will also teach you about different types of objects: Scalars Vectors Data frames &amp; matrices Lists Other types of objects 4.2.1 Scalars The smallest type of data you will encounter are scalars. Scalars are objects consisting of a single value - for example, a letter, a word, a sentence, a number, etc. Youve already seen what a scalar looks like - remember the very first time you were Writing Code? Here, we defined an objects word which only consisted of the word hello. word &lt;- &quot;hello&quot; word ## [1] &quot;hello&quot; We could also save a single number this way: number &lt;- 1 number ## [1] 1 The same applies to a single sentence: sentence &lt;- &quot;I would like to be saved as a scalar&quot; sentence ## [1] &quot;I would like to be saved as a scalar&quot; 4.2.2 Vectors The next type of data you should know are vectors: Vectors are objects that consist of several values of the same type of data. For instance, if we want to save several words or numbers separately, we can save them as vectors. This means that the first element of our vector will be our first word, the second the second word and so forth. Importantly, a vector can only contain data of the same type - for instance, you cannot save data in numeric and character format in the same format. In principle, you can often (but not always) compare vectors with variables in data sets: They contain values for all observations in your data set (with all of these values being of the same data type). An example would be the numbers from 1 to 20 that we worked with before. Now, it becomes apparent what the c() stands for - it specifies the vector format. We define the object numbers to consist of the a vector c() which contains the values 1 to 20. Here, we ask R to include all values from 1 to 20 by inserting a colon between both numbers: numbers &lt;- c(1:20) numbers ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 If you want the vector numbers to only contain selected numbers from 1 to 20, you can specify these numbers and separate them using a comma: numbers &lt;- c(1,5,8,19,20) numbers ## [1] 1 5 8 19 20 We can do the same thing for data in character format: fruits &lt;- c(&quot;Apple&quot;, &quot;Banana&quot;, &quot;Orange&quot;, &quot;Lemon&quot;) fruits ## [1] &quot;Apple&quot; &quot;Banana&quot; &quot;Orange&quot; &quot;Lemon&quot; In practice, a vector is nothing else than scalars of the same typed chained together - in this case, the words Apple, Banana, Orange, and Lemon. If you love to write inefficient code, you could also define separate objects (here: scalars) consisting of every element of the vector you want to define and then chain them together (10/10 wouldnt recommend because this is unnecessarily complicated): apple &lt;- &quot;Apple&quot; banana &lt;- &quot;Banana&quot; orange &lt;- &quot;Orange&quot; lemon &lt;- &quot;Lemon&quot; fruits &lt;- c(apple, banana, orange, lemon) fruits ## [1] &quot;Apple&quot; &quot;Banana&quot; &quot;Orange&quot; &quot;Lemon&quot; This would lead to very similar results, but you would have to write 4 more lines of code compared to the previous example. Subsetting vectors: You can select elements from a vector. By inserting an index vector in square brackets directly behind the name of the vector, you may inform R that you wish to choose certain specific elements (i.e., asubset) from this vector. Try numbers[3] to see the third element (which is 8) of the numbers vector and try fruits[3] to see the third element (which is Orange) of the fruits vector: numbers[3] ## [1] 8 fruits[3] ## [1] &quot;Orange&quot; You can also index inversely, i.e. select all elements except the element of the index vector. For this you have to put a - in front of the index vector. numbers[-3] ## [1] 1 5 19 20 fruits[-3] ## [1] &quot;Apple&quot; &quot;Banana&quot; &quot;Lemon&quot; Finally, you can chose to subset more than one element by providing a range. Lets have a look at the first three elements of the numbers and fruit vector: numbers[1:3] ## [1] 1 5 8 fruits[1:3] ## [1] &quot;Apple&quot; &quot;Banana&quot; &quot;Orange&quot; 4.2.3 Data frames &amp; matrices An even bigger type of data, in this sense, are data frames and matrices. Data frames &amp; matrices consist of several vectors of the same length. Matrices and data frames come closest to how you may understand data sets (for instance, in SPSS). Our survey data set is one example for a data frame: survey ## X.2 X.1 X name age date outlet outlet_use outlet_trust ## 1 1 1 1 Alexandra 20 2021-09-09 TV 2 5 ## 2 2 2 2 Alex 25 2021-09-08 Online 3 5 ## 3 3 3 3 Maximilian 29 2021-09-09 Zeitung 4 1 ## 4 4 4 4 Moritz 22 2021-09-06 TV 2 2 ## 5 5 5 5 Vanessa 25 2021-09-07 Online 1 3 ## 6 6 6 6 Andrea 26 2021-09-09 Online 3 4 ## 7 7 7 7 Fabienne 26 2021-09-09 TV 3 2 ## 8 8 8 8 Fabio 27 2021-09-09 Online 0 1 ## 9 9 9 9 Magdalena 8 2021-09-08 Online 1 4 ## 10 10 10 10 Tim 26 2021-09-07 TV NA 2 ## 11 11 11 11 Alex 27 2021-09-09 Online NA 2 ## 12 12 12 12 Tobias 26 2021-09-07 Online 2 2 ## 13 13 13 13 Michael 25 2021-09-09 Online 3 2 ## 14 14 14 14 Sabrina 27 2021-09-08 Online 1 2 ## 15 15 15 15 Valentin 29 2021-09-09 TV 1 5 ## 16 16 16 16 Tristan 26 2021-09-09 TV 2 5 ## 17 17 17 17 Martin 21 2021-09-09 Online 1 2 ## 18 18 18 18 Anna 23 2021-09-08 TV 3 3 ## 19 19 19 19 Andreas 24 2021-09-09 TV 2 5 ## 20 20 20 20 Florian 26 2021-09-09 Online 1 5 Lets say we dont have a data set yet but want to create one ourselves. For instance, we have three vectors of the same length that contain the following data: a vector called fruits consisting of the names of four different fruits (character) a vector called price consisting of the amount of money each fruit costs (numeric) a vector called color consisting of the color of each fruit (character) fruits &lt;- c(&quot;Apple&quot;, &quot;Banana&quot;, &quot;Orange&quot;, &quot;Lemon&quot;) price &lt;- c(0.8,0.6,0.78,0.9) color &lt;- c(&quot;red&quot;, &quot;yellow&quot;, &quot;orange&quot;, &quot;yellow&quot;) Since these vectors describe the same data, we want them to be saved as one data set. To create the data set data_fruits, we use the command data.frame(), which contains: an vector called fruits as its first column a vector called price as its second column a vector color as its third column Here, columns describe different variables and rows observations. data_fruits &lt;- data.frame(&quot;fruits&quot;= fruits, &quot;price&quot; = price, &quot;color&quot; = color) data_fruits ## fruits price color ## 1 Apple 0.80 red ## 2 Banana 0.60 yellow ## 3 Orange 0.78 orange ## 4 Lemon 0.90 yellow In difference to data frames that can encompass different types of data, matrices consist of vectors of exactly the same type of data - for instance, only character vectors or numeric vectors. This is also why we will mostly be working with data frames in this seminar - they are just more flexible and more suitable for the data we will work with. For a nice example for comparing scalars, vectors, data frames, and matrices, check out this example here. 4.2.4 Lists Lastly, you should know about lists. Lists can be described as an array of (different) objects. As discussed before, data frames can encompass vectors consisting of different types of data (for instance, character and numeric vectors). However, these all have to be vectors (and of the same length). In some case, you may want to save very different types of objects - scalars, vectors, data frames, and objects -, which will differ in length3. In this case, lists offer the most flexible way of saving very different objects within one object (i.e., the list). Lets say that we want to save our data frame data_fruits as well the data frame survey in a single object. We do so by specifying a list which includes data_fruits as its first and survey as its second element: list &lt;- list(data_fruits, survey) View(list) Image: List As you see, the object list consists of two elements: the first element [[1]] encompasses the data frame data_fruits the second element [[2]] encompasses the data frame survey 4.2.5 Other types of objects Since well be learning how to conduct automated content analysis in this tutorial, well encounter a bunch of other types of data such as the corpus or the document-feature-matrix (dfm). You dont have to know anything about these data types for now - just be aware that other types of objects exist. 4.3 Take Aways Types of data: In R, you can work with different types of data in numeric, character, factor, or data format. Important commands: mode(), str(), as.character(), as.numeric(), as.factor(), levels(), as.Date() &amp; is.na() Types of objects: In R, you can work with different types of objects, for instance scalars, vectors, data frames, matrices, or lists. Important commands: c(), data.frame(), list() Acessing variables: You can access variables in a given data frame by using the $ operator surrounded by the name of the data frame and the name of the variable that you want to access, e.g.: dataframe$variablename 4.4 More tutorials on this You still have questions? The following tutorials &amp; papers can help you with that: Objekte und Syntax by M. Wettstein - OLAT course for UZH students SICSS Boot Camp by C. Bail, Video 2 YaRrr! The Pirates Guide to R by N.D.Phillips, Tutorial 4, 5, 7 und 8 Computational Methods in der politischen Kommunikationsforschung by J. Unkel, Tutorial 2 R Cookbook by Long et al., Tutorial 5 Lets keep going: Tutorial: Reading data in/out Dont worry, well discuss the read.csv2() command in a bit. Ignore it for now. Length means that these objects may contain different numbers of elements (in the case of scalars and vectors, for example) or different numbers of rows/columns (in the case of data frames and matrices, for example) "],["tutorial-reading-data-inout.html", " 5 Tutorial: Reading data in/out 5.1 Getting data into R 5.2 Getting data out of R 5.3 Other packages for getting data into/out of R 5.4 Take Aways 5.5 More tutorials on this", " 5 Tutorial: Reading data in/out After working through Tutorial 5, youll understand how to get data into and out of R For this tutorial, well again use the data set data_tutorial3.csv (via OLAT/Materials/02_02.03.22/Data). The data set has already been introduced and explained in Tutorial: Objects &amp; structures in R, so I wont go into detail here. survey &lt;- read.csv2(&quot;data_tutorial3.csv&quot;, header = TRUE) One of the most frequently encountered external data types youll have to get into R are comma-separated values files, or short, CSV files. You may know CSV files from Excel - oftentimes, such data consists of observations (in rows) and variables (in columns). Values are separated by a separator (oftentimes a comma or a semicolon, depending on your data). 5.1 Getting data into R This tutorial doesnt teach you that much new. In fact, we have already encountered a CSV file when reading in data for tutorial 4. survey &lt;- read.csv2(&quot;data_tutorial3.csv&quot;) What does this command do? Lets see: ?read.csv2 Image: Help for the read.csv2 function The read.csv2() function is part of the utils package. To read in CSV files, two different functions exist: read.csv() reads in CSV files where values are comma separated read.csv2() reads in CSV files where values are semicolon separated Other than that, both functions work the same way and consist of the same arguments: file: Here, you need to identify the name of the CSV files that should be read in (including the file extention, here .csv). The file should be located in your working directory, otherwise R will not be able to identify it: file = data_tutorial3.csv header: This argument specifies whether or not the first row of the CSV file contains the name of variables. It is automatically set to true - thus, if our data wouldnt contain the name of each variable in its first row, we would need to set this argument to FALSE. In our case, we could either ignore this argument (since it is automatically set to TRUE) or actively set it to TRUE. Both leads to the same result. Important: R differentiates between necessary (marked in red) and obligatory (marked in green) arguments. Image: Help Arguments where no default value is given (i.e., those without a =) are necessary. That means you have to specify it once you call the function by passing respective values to the function. For instance, you cannot read in a CSV file without specifying the argument file - R would not even know which file to read in in that case. However, arguments were are default value is given can be ignored. If you do not specify values for these arguments yourself, R will simply take the default value. For instance, the read.csv2() function will automatically use the first row of a CSV file as column names, unless you actively set the argument header to FALSE. Another important thing to know is that you can specify arguments in functions in two ways: explicitly by name, for instance by setting file equal to data_tutorial3.csv implicitly by order, for instance by passing data_tutorial3.csv as the first argument to the read.csv2() function In fact, the following two commands will give the exact same results: survey &lt;- read.csv2(&quot;x = data_tutorial3.csv&quot;) survey &lt;- read.csv2(&quot;data_tutorial3.csv&quot;) 5.2 Getting data out of R In some cases, you may want to also get data out of R - for instance, export a newly created data set as a CSV file. You can easily do that using the write.csv() or the write.csv2() function by specifying the arguments x (i.e., which object should be exported) and file (i.e., how the exported file should be named). write.csv2(x=survey, file=&quot;survey_export.csv&quot;) 5.3 Other packages for getting data into/out of R When working with R, there will be many other data types you may wish to import or export. While we wont cover all of them here, the following packages deliver some very useful additional functions for doing so readxl and xlsx: working with excel files, especially .xls and .xlsx files. readr: working with many other data types, including text files foreign: working with many other data types, including SPSS or STATA files 5.4 Take Aways Importing data: read.csv(), read.csv2() Exporting data: write.csv(), write.csv2() 5.5 More tutorials on this You still have questions? The following tutorials &amp; papers can help you with that: YaRrr! The Pirates Guide to R by N.D.Phillips, Tutorial 9 Now lets see what youve learned so far: Exercise 1: Test your knowledge. "],["exercise-1-test-your-knowledge.html", " 6 Exercise 1: Test your knowledge 6.1 Task 1 6.2 Task 2 6.3 Task 3 6.4 Task 4 6.5 Task 5", " 6 Exercise 1: Test your knowledge After working through Exercise 1, youll have assessed how well you know R and RStudio know what chapters and concepts you might want to repeat again have managed to apply the basic concepts of R to data 6.1 Task 1 Below you will see multiple choice questions. Please try to identify the correct answers. 1, 2, 3 and 4 correct answers are possible for each question. 1. What panels are part of RStudio? source console input packages, files &amp; plots 2. How do you activate R packages after you have installed them? import.packages() install.packages() package() library() 3. How do you create a vector in R with elements 1, 2, 3? cbind(1,2,3) cb(1,2,3) c(1,2,3) cmb(1,2,3) 4. Imagine you have a vector called vector with 10 numeric elements. How do you retrieve the 8th element? vector[-2] vector[-2] vector[8] vector[8] 5. Imagine you have a vector called hair with 5 elements: brown, black, red, blond, other. How do you retrieve the color blond? hair[4] hair[4] hair[blond] hair[blond] 6.2 Task 2 Create a numeric vector with 8 values and assign the name age to the vector. First, display all elements of the vector. Then print only the 5th element. After that, display all elements except the 5th. Finally, display the elements at the positions 6 to 8. 6.3 Task 3 Create a non-numeric, i.e. character, vector with 4 elements and assign the name eye_color to the vector. First, print all elements of this vector to the console. Then have only the value in the 2nd element displayed, then all values except the 2nd element. At the end, display the elements at the positions 2 to 4. 6.4 Task 4 Create a data frame called data. The data frame should contain the following variables (in this order): a vector called food. It should contain 5 elements, namely the names of your five favorite dishes. a vector called description. For every dish mentioned in food, please describe the dish in a single sentence (for instance, if the first food you describe is pizza, you could write: This is an Italian dish, which I prefer with a lot of cheese.) a vector called rating. Rate every dish mentioned in food with 1-5 (using every number only once), i.e., by rating your absolute favorite dish out of all five with a 1 and your least favorite dish out of all five with a 5. Hint: For me, the data frame would look something like this: ## food description Rating ## 1 pizza Italian dish, I actually prefer mine with little cheese 3 ## 2 pasta Another Italian dish 1 ## 3 ice cream The perfect snack in summer 2 ## 4 crisps Potatoes and oil - a luxurious combination 4 ## 5 passion fruit A fruit that makes me think about vacation 5 6.5 Task 5 Can you sort the data in your prior data set by rating - with your favorite dish (i.e., the one rated 1) on top of the list and your least favorite dish (i.e., the one rated 5) on the bottom? Important: You do not yet know this command - youll have to google for the right solution. Please do and note down the exact search terms you used for googling. Hint: For me, the data frame would look something like this: ## food description Rating ## 1 pasta Another Italian dish 1 ## 2 ice cream The perfect snack in summer 2 ## 3 pizza Italian dish, I actually prefer mine with little cheese 3 ## 4 crisps Potatoes and oil - a luxurious combination 4 ## 5 passion fruit A fruit that makes me think about vacation 5 When youre ready to look at the solutions, you can find them here: Solutions for Exercise 1. Now that youre well versed in data types and structures, lets move on to data management in the tidyverse in the Tutorial: Data management with tidyverse. "],["tutorial-data-management-with-tidyverse.html", " 7 Tutorial: Data management with tidyverse 7.1 Why not stick with Base R? 7.2 Tidyverse packages 7.3 Tidy data 7.4 The pipe operator 7.5 Data transformation with dplyr 7.6 Take-Aways 7.7 More tutorials on this", " 7 Tutorial: Data management with tidyverse After working through Tutorial 7, youll know the advantages of the tidyverse vs. Base R know about different formats of tabular data understand what packages are included in the tidyverse meta-package know how to do data modifications and transformations with dplyr 7.1 Why not stick with Base R? You might wonder why weve spent so much time exploring functions in Base R to now learn data management with tidyverse. After all, data management can also be done in Base R, cant it? I personally recommend that all R beginners should work with the tidyverse as early as possible. There are three reasons supporting my argument: Ease of use: The tidyverse is very accessible for R beginners, i.e. its syntax is very easy to understand. It allows you to set goals (i.e. what you want to do with your data) and get you working on these goals very quickly. Definitely more quickly than in Base R! Standard for data management: A few years ago, the tidyverse has become the de facto standard for data management in R. It is a meta-package, which means that it is a collection of distinct packages that all follow the same design principles to make code reading and writing as simple as possible. For example, all functions are named after verbs that indicate exactly what they perform (e.g. filter or summarize). Beautiful graphs: With the tidyverse, all data management steps can be swiftly transferred into beautiful graphs. This is because the most popular graph package in R, ggplot2, is part of the tidyverse. Are you excited now? Then lets get started! 7.2 Tidyverse packages The tidyverse comes with a great arsenal of topic-specific packages and their respective functions. It includes packages for: tibble: creating data structures like tibbles, which is an enhanced type of data frame readr, haven, readxl: reading data (e.g. readr for CSV, haven for SPSS, Stata and SAS, readxl for Excel) tidyr, dplyr: data transformation, modification, and summary statistics stringr, forcats, lubridate: create special, powerful object types (e.g. stringr for working with text objects, forcats for factors, lubridate for time data) purrr: programming with R ggplot2: graphing/charting The most frequently used packages of the tidyverse can be installed and activated in one go (less frequently used packages like haven still need to be installed and activated separately): install.packages(&quot;tidyverse&quot;) # install the package (only on the first time) library(tidyverse) # active the package 7.3 Tidy data Dataframes, which we learned about in Types of objects, are tabular data. However, data can also have other formats, for example as nested, i.e. hierarchical, lists. In communication research, these other data formats are mainly used by social media and their respective APIs (perhaps you have heard of the JSON format before). In our course, however, well focus on tabular data.The same data can be represented differently in tables. We perceive some of these representations as tidy, others as messy. While tidy data principles establish a standard for organizing data values inside a data frame and thus all tidy data look the same, every messy dataset is messy in its own way. Take a look at the table below. It shows a Starwars data set that comes pre-installed with the dplyr package. Do you feel the tabled data is messy? Why (not)? ## # A tibble: 10 x 3 ## name body_feature value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Anakin Skywalker height 188 ## 2 Anakin Skywalker mass 84 ## 3 Chewbacca height 228 ## 4 Chewbacca mass 112 ## 5 Darth Vader height 202 ## 6 Darth Vader mass 136 ## 7 Jabba Desilijic Tiure height 175 ## 8 Jabba Desilijic Tiure mass 1358 ## 9 Leia Organa height 150 ## 10 Leia Organa mass 49 Overall, this data is messy. It comes with three messy problems: This body_feature column comprises information relating to both height and weight, i.e. both variables are stored in a single column. As a result, the value column is reliant on the body_feature column; we cant tell the stored values apart by merely looking at the value column. We always need to check the body_feature column. Consequently, we have issues with vectorized functions (remember, in R, columns in data sets are vectors): We cant, for example, use the mean() function on the value column to determine the average weight of the Star Wars characters since the height values are also stored there. What do you think of this table? Is it messy? ## # A tibble: 5 x 3 ## name height mass ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Anakin Skywalker 188 84 ## 2 Chewbacca 228 112 ## 3 Darth Vader 202 136 ## 4 Jabba Desilijic Tiure 175 1358 ## 5 Leia Organa 150 49 This table looks tidy! Tidy data is a standard way of mapping the meaning of a dataset to its structure. We determine whether a dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types. We consider a table tidy when it follows the following golden rules: Columns: Every column is one variable. Rows: Every row is one observation. Cells: Every cell contains one single value. Image: The tidy data principle (Source: R for Data Science) In messy data sets, on the other hand Column headers are values, not variable names. Multiple variables are stored in one column. Variables are stored in both rows and columns. Multiple types of observational units are stored in the same table. A single observational unit is stored in multiple tables. Why should you be concerned about tidy data organization? There are two major advantages: When you have a consistent data structure, it is easier to learn the respective tools that work well with this data structure. dplyr, ggplot2, and all the other tidyverse packages are designed for working with tidy data. Putting variables in columns makes Rs vectorized nature shine. The majority of built-in R-functions (like the mean() function) works with vectors of values. As a result, the tidy reorganization of data seems only natural for a good work flow in R. If you have been working mainly with survey data, then you will already be familiar with these basic rules, as data export from survey software usually follows these principles. However, real-world data from databases or social media often does not follow these principles. Thats why its sometimes true to say that 80% of data analysis is spent on cleaning and transforming data. 7.4 The pipe operator Truly, dplyr is my favorite tidyverse package (even more so than ggplot2, which well cover later!). It allows you to perform powerful data transformations in just a few simple steps. To this end, dplyr relies on the pipe operator (%&gt;%).4 The %&gt;% operator allows functions to be applied sequentially to the same source object in a concise manner, so that step-by-step transformations can be applied to the data. Therefore, we always call the source object first and then add each transformation step separated by the %&gt;% operator. Lets illustrate this concept with an example. Well use the Starwars data set that you are already familiar with. starwars_data %&gt;% # First, we define the source object, i.e. the data frame that we want to transform, followed by the pipe operator plot() # Second, we specify which function should be performed on the source object, here: scatterplot Now, thats not very impressive. We could do the same in Base R like this: plot(starwars_data) However, dplyr gets really impressive when you chain functions sequentially. You can apply certain selection criteria to your data and plot it in one go. For example, we might exclude the variable name from our scatter plot, since its not a metric variable anyway. Also, we might want to look only at those Star Wars characters taller than 170 cm. Lets try it in a single run! starwars_data %&gt;% # Define the source object select(height, mass) %&gt;% # Keep only the height and mass column filter(height &gt; 170) %&gt;% # Filter all observations that are taller than 170cm plot() # Plot! Now try to do the same in Base R: plot(starwars_data[starwars_data$height&gt;170,]$mass~starwars_data[starwars_data$height&gt;170,]$height, xlab=&quot;height&quot;, ylab=&quot;mass&quot;) The Base R code is longer, more nested, and not as readable as the code written in dplyr. And the more selection criteria and functions you need to implement, the worse it gets. For example, imagine you would also want to exclude Star Wars characters with a mass bigger than 1200kg. Peace of cake with dplyr: starwars_data %&gt;% select(height, mass) %&gt;% filter(height &gt; 170) %&gt;% filter(mass &lt; 1200) %&gt;% plot() 7.5 Data transformation with dplyr dplyrcomes with five main functions: select(): select variables column by column, i.e. pick columns / variables by their names filter(): filter observations row by row, i.e. pick observations by their values arrange(): sort / reorder data in ascending or descending order mutate(): calculate new variables or transform existing ones summarize(): summarize variables (e.g. mean, standard deviation, etc.), best combined with group_by() 7.5.1 select() Scientists will frequently provide you with large data sets including hundreds of variables (often even more!). The first problem in this scenario is narrowing down the variables you are truly interested in. select() helps you to easily choose a suitable subset of variables. In this selection process, the name of the data frame is the source object, followed by the pipe %&gt;% operator. The expression that selects the columns that you are interested in comes after that. Take the Star Wars data, for example. The original data set has 87 observations (Star Wars characters) and 14 columns / variables (traits of these characters, e.g., birth_year, gender, and species). Yes, 14 columns is not a lot and you could get an overview of this data without subsetting columns. Lets take a look at the original data frame: library(dplyr) # load dplyr starwars_data &lt;- starwars # assign the pre-installed starwars data from dplyr to a source object / variable starwars_data # print the content of the data frame to the console ## # A tibble: 87 x 14 ## name height mass hair_color skin_color eye_color birth_year sex gender homeworld species films vehicles starships ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;lis&gt; &lt;list&gt; &lt;list&gt; ## 1 Luke Skywal~ 172 77 blond fair blue 19 male mascu~ Tatooine Human &lt;chr&gt; &lt;chr&gt; &lt;chr [2]&gt; ## 2 C-3PO 167 75 &lt;NA&gt; gold yellow 112 none mascu~ Tatooine Droid &lt;chr&gt; &lt;chr&gt; &lt;chr [0]&gt; ## 3 R2-D2 96 32 &lt;NA&gt; white, bl~ red 33 none mascu~ Naboo Droid &lt;chr&gt; &lt;chr&gt; &lt;chr [0]&gt; ## 4 Darth Vader 202 136 none white yellow 41.9 male mascu~ Tatooine Human &lt;chr&gt; &lt;chr&gt; &lt;chr [1]&gt; ## 5 Leia Organa 150 49 brown light brown 19 fema~ femin~ Alderaan Human &lt;chr&gt; &lt;chr&gt; &lt;chr [0]&gt; ## 6 Owen Lars 178 120 brown, gr~ light blue 52 male mascu~ Tatooine Human &lt;chr&gt; &lt;chr&gt; &lt;chr [0]&gt; ## 7 Beru Whites~ 165 75 brown light blue 47 fema~ femin~ Tatooine Human &lt;chr&gt; &lt;chr&gt; &lt;chr [0]&gt; ## 8 R5-D4 97 32 &lt;NA&gt; white, red red NA none mascu~ Tatooine Droid &lt;chr&gt; &lt;chr&gt; &lt;chr [0]&gt; ## 9 Biggs Darkl~ 183 84 black light brown 24 male mascu~ Tatooine Human &lt;chr&gt; &lt;chr&gt; &lt;chr [1]&gt; ## 10 Obi-Wan Ken~ 182 77 auburn, w~ fair blue-gray 57 male mascu~ Stewjon Human &lt;chr&gt; &lt;chr&gt; &lt;chr [5]&gt; ## # ... with 77 more rows For the sake of practice, lets say we only want to analyze the species, birth_year, mass, and height of these characters. To simplify data handling, we want to keep only the respective columns. starwars_data %&gt;% # define the source object select(name, species, birth_year, mass, height) # keep only the name, species, birth_year, mass and height column ## # A tibble: 87 x 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Luke Skywalker Human 19 77 172 ## 2 C-3PO Droid 112 75 167 ## 3 R2-D2 Droid 33 32 96 ## 4 Darth Vader Human 41.9 136 202 ## 5 Leia Organa Human 19 49 150 ## 6 Owen Lars Human 52 120 178 ## 7 Beru Whitesun lars Human 47 75 165 ## 8 R5-D4 Droid NA 32 97 ## 9 Biggs Darklighter Human 24 84 183 ## 10 Obi-Wan Kenobi Human 57 77 182 ## # ... with 77 more rows At the moment you have only printed the transformed data to the console. However, most of the time we want to keep the transformed data ready for further calculations. In this case we should assign the transformed data into a new source object, which we can access later. starwars_short &lt;- starwars_data %&gt;% # assign a new source object and define the old source object select(name, species, birth_year, mass, height) # keep only the name, species, birth_year, mass and height column Lets print the new source object, starwars_short, to the console. starwars_short ## # A tibble: 87 x 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Luke Skywalker Human 19 77 172 ## 2 C-3PO Droid 112 75 167 ## 3 R2-D2 Droid 33 32 96 ## 4 Darth Vader Human 41.9 136 202 ## 5 Leia Organa Human 19 49 150 ## 6 Owen Lars Human 52 120 178 ## 7 Beru Whitesun lars Human 47 75 165 ## 8 R5-D4 Droid NA 32 97 ## 9 Biggs Darklighter Human 24 84 183 ## 10 Obi-Wan Kenobi Human 57 77 182 ## # ... with 77 more rows You can also delete columns by making a reverse selection with the - symbol. This means that you select all columns except the one whose name you specify. starwars_short %&gt;% select(-name) # keep all columns except the name column (i.e. delete name column) ## # A tibble: 87 x 4 ## species birth_year mass height ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Human 19 77 172 ## 2 Droid 112 75 167 ## 3 Droid 33 32 96 ## 4 Human 41.9 136 202 ## 5 Human 19 49 150 ## 6 Human 52 120 178 ## 7 Human 47 75 165 ## 8 Droid NA 32 97 ## 9 Human 24 84 183 ## 10 Human 57 77 182 ## # ... with 77 more rows You can delete more than one column in one go: starwars_short %&gt;% select(-c(name,species)) # keep all columns except the name &amp; species column (i.e. delete these columns) ## # A tibble: 87 x 3 ## birth_year mass height ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 19 77 172 ## 2 112 75 167 ## 3 33 32 96 ## 4 41.9 136 202 ## 5 19 49 150 ## 6 52 120 178 ## 7 47 75 165 ## 8 NA 32 97 ## 9 24 84 183 ## 10 57 77 182 ## # ... with 77 more rows Tip for advanced users: You can select columns and rename them at the same time. starwars_short %&gt;% select(&quot;character&quot;=name, &quot;age&quot;=birth_year) # select columns that you want to keep &amp; rename them ## # A tibble: 87 x 2 ## character age ## &lt;chr&gt; &lt;dbl&gt; ## 1 Luke Skywalker 19 ## 2 C-3PO 112 ## 3 R2-D2 33 ## 4 Darth Vader 41.9 ## 5 Leia Organa 19 ## 6 Owen Lars 52 ## 7 Beru Whitesun lars 47 ## 8 R5-D4 NA ## 9 Biggs Darklighter 24 ## 10 Obi-Wan Kenobi 57 ## # ... with 77 more rows 7.5.2 filter() filter() divides observations into groups depending on their values. The name of the data frame is the source object, followed by the pipe %&gt;% operator. Then follow the expressions that filter the data. Lets only select human Star Wars characters in our transformed data set starwars_short: starwars_short %&gt;% filter(species==&quot;Human&quot;) ## # A tibble: 35 x 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Luke Skywalker Human 19 77 172 ## 2 Darth Vader Human 41.9 136 202 ## 3 Leia Organa Human 19 49 150 ## 4 Owen Lars Human 52 120 178 ## 5 Beru Whitesun lars Human 47 75 165 ## 6 Biggs Darklighter Human 24 84 183 ## 7 Obi-Wan Kenobi Human 57 77 182 ## 8 Anakin Skywalker Human 41.9 84 188 ## 9 Wilhuff Tarkin Human 64 NA 180 ## 10 Han Solo Human 29 80 180 ## # ... with 25 more rows And now lets only select Star Wars character who are younger than 24 or exactly 24 years old. starwars_short %&gt;% filter(birth_year&lt;=24) ## # A tibble: 7 x 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Luke Skywalker Human 19 77 172 ## 2 Leia Organa Human 19 49 150 ## 3 Biggs Darklighter Human 24 84 183 ## 4 Wedge Antilles Human 21 77 170 ## 5 IG-88 Droid 15 140 200 ## 6 Wicket Systri Warrick Ewok 8 20 88 ## 7 Plo Koon Kel Dor 22 80 188 Chaining some functions, lets look at Star Wars character who are a Droid and older than 24. starwars_short %&gt;% filter(species==&quot;Droid&quot; &amp; birth_year &gt; 24) # &amp; --&gt; filter all observations to which both logical statements apply ## # A tibble: 2 x 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 C-3PO Droid 112 75 167 ## 2 R2-D2 Droid 33 32 96 Alternatively, you can also write these filters like this: starwars_short %&gt;% filter(species==&quot;Droid&quot;) %&gt;% filter(birth_year &gt; 24) ## # A tibble: 2 x 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 C-3PO Droid 112 75 167 ## 2 R2-D2 Droid 33 32 96 Besides the &amp; operator, there are many more logical operators that you can choose from to optimize your filter choices. Here is an overview: Image: Logical, i.e. boolean, operators (Source: R for Data Science) []!(C:/Users/LaraK/Desktop/To-Do/Verwendung von Conditional Process Analysis zur Bewertung von Kommunikationstheorien/LaraKobilke/images/Tut7_logical_operators.JPG) Tip for advanced users 1: You can negate filters. This means that you keep all observations except the one that you have specified with the != operator (read != as: is not or is unequal to). For example, you can choose to include only non-human Star Wars characters. starwars_short %&gt;% filter(species!=&quot;Human&quot;) ## # A tibble: 48 x 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 C-3PO Droid 112 75 167 ## 2 R2-D2 Droid 33 32 96 ## 3 R5-D4 Droid NA 32 97 ## 4 Chewbacca Wookiee 200 112 228 ## 5 Greedo Rodian 44 74 173 ## 6 Jabba Desilijic Tiure Hutt 600 1358 175 ## 7 Yoda Yoda&#39;s species 896 17 66 ## 8 IG-88 Droid 15 140 200 ## 9 Bossk Trandoshan 53 113 190 ## 10 Ackbar Mon Calamari 41 83 180 ## # ... with 38 more rows Alternatively, you achieve the same goal by negating the entire function call. Negating the entire function call can be handy at times. starwars_short %&gt;% filter(!(species==&quot;Human&quot;)) ## # A tibble: 48 x 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 C-3PO Droid 112 75 167 ## 2 R2-D2 Droid 33 32 96 ## 3 R5-D4 Droid NA 32 97 ## 4 Chewbacca Wookiee 200 112 228 ## 5 Greedo Rodian 44 74 173 ## 6 Jabba Desilijic Tiure Hutt 600 1358 175 ## 7 Yoda Yoda&#39;s species 896 17 66 ## 8 IG-88 Droid 15 140 200 ## 9 Bossk Trandoshan 53 113 190 ## 10 Ackbar Mon Calamari 41 83 180 ## # ... with 38 more rows Tip for advanced users 2: You can filter for missing values (NAs) with the is.na() function. starwars_short %&gt;% filter(is.na(birth_year)) ## # A tibble: 44 x 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 R5-D4 Droid NA 32 97 ## 2 Jek Tono Porkins Human NA 110 180 ## 3 Arvel Crynyd Human NA NA NA ## 4 Nien Nunb Sullustan NA 68 160 ## 5 Nute Gunray Neimodian NA 90 191 ## 6 Roos Tarpals Gungan NA 82 224 ## 7 Rugor Nass Gungan NA NA 206 ## 8 Ric Olié &lt;NA&gt; NA NA 183 ## 9 Watto Toydarian NA NA 137 ## 10 Sebulba Dug NA 40 112 ## # ... with 34 more rows And you can negate that filter to get rid of all observation that have missing values (NAs). starwars_short %&gt;% filter(!is.na(birth_year)) ## # A tibble: 43 x 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Luke Skywalker Human 19 77 172 ## 2 C-3PO Droid 112 75 167 ## 3 R2-D2 Droid 33 32 96 ## 4 Darth Vader Human 41.9 136 202 ## 5 Leia Organa Human 19 49 150 ## 6 Owen Lars Human 52 120 178 ## 7 Beru Whitesun lars Human 47 75 165 ## 8 Biggs Darklighter Human 24 84 183 ## 9 Obi-Wan Kenobi Human 57 77 182 ## 10 Anakin Skywalker Human 41.9 84 188 ## # ... with 33 more rows Tip for advanced users 3: Watch out for the | operator (read: or). This one can be tricky to negate! For example, with this code you get all characters that are NEITHER human NOR older than 33 years. I.e. you get all non-human characters who are younger than 33 or exactly 33 years old. starwars_short %&gt;% filter(!((species == &quot;Human&quot;) | (birth_year &gt; 33))) ## # A tibble: 4 x 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 R2-D2 Droid 33 32 96 ## 2 IG-88 Droid 15 140 200 ## 3 Wicket Systri Warrick Ewok 8 20 88 ## 4 Plo Koon Kel Dor 22 80 188 But with this code, youll get all observations that are either non-human (regardless of their age) OR humans who are older than 33 years old. starwars_short %&gt;% filter((species != &quot;Human&quot;) | (birth_year &gt; 33)) ## # A tibble: 67 x 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 C-3PO Droid 112 75 167 ## 2 R2-D2 Droid 33 32 96 ## 3 Darth Vader Human 41.9 136 202 ## 4 Owen Lars Human 52 120 178 ## 5 Beru Whitesun lars Human 47 75 165 ## 6 R5-D4 Droid NA 32 97 ## 7 Obi-Wan Kenobi Human 57 77 182 ## 8 Anakin Skywalker Human 41.9 84 188 ## 9 Wilhuff Tarkin Human 64 NA 180 ## 10 Chewbacca Wookiee 200 112 228 ## # ... with 57 more rows 7.5.3 arrange() arrange() and filter() are like two brothers: both look similar, but they also differ in at least one essential aspect. Both functions change the rows of the data frame, but unlike filter(), arrange() does not select or delete rows, it only changes their order (either ascending or descending). By default, arrange() will sort in ascending order, i.e. from 1:100 (numeric vector) and from A:Z (character vector). arrange() must always be applied to at least one column that is to be sorted. starwars_short %&gt;% arrange(birth_year) ## # A tibble: 87 x 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Wicket Systri Warrick Ewok 8 20 88 ## 2 IG-88 Droid 15 140 200 ## 3 Luke Skywalker Human 19 77 172 ## 4 Leia Organa Human 19 49 150 ## 5 Wedge Antilles Human 21 77 170 ## 6 Plo Koon Kel Dor 22 80 188 ## 7 Biggs Darklighter Human 24 84 183 ## 8 Han Solo Human 29 80 180 ## 9 Lando Calrissian Human 31 79 177 ## 10 Boba Fett Human 31.5 78.2 183 ## # ... with 77 more rows To get a descending order: starwars_short %&gt;% arrange(desc(birth_year)) ## # A tibble: 87 x 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Yoda Yoda&#39;s species 896 17 66 ## 2 Jabba Desilijic Tiure Hutt 600 1358 175 ## 3 Chewbacca Wookiee 200 112 228 ## 4 C-3PO Droid 112 75 167 ## 5 Dooku Human 102 80 193 ## 6 Qui-Gon Jinn Human 92 89 193 ## 7 Ki-Adi-Mundi Cerean 92 82 198 ## 8 Finis Valorum Human 91 NA 170 ## 9 Palpatine Human 82 75 170 ## 10 Cliegg Lars Human 82 NA 183 ## # ... with 77 more rows If you specify more than one column, then subsequent columns are used to break ties. Also note that missing values are always displayed last: starwars_short %&gt;% arrange(species, birth_year) ## # A tibble: 87 x 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Ratts Tyerell Aleena NA 15 79 ## 2 Dexter Jettster Besalisk NA 102 198 ## 3 Ki-Adi-Mundi Cerean 92 82 198 ## 4 Mas Amedda Chagrian NA NA 196 ## 5 Zam Wesell Clawdite NA 55 168 ## 6 IG-88 Droid 15 140 200 ## 7 R2-D2 Droid 33 32 96 ## 8 C-3PO Droid 112 75 167 ## 9 R5-D4 Droid NA 32 97 ## 10 R4-P17 Droid NA NA 96 ## # ... with 77 more rows 7.5.4 mutate() Often you want to add new columns to a data set, e.g. when you calculate new variables or when you want to store re-coded values of other variables. With mutate(), new columns will be added to the end of you data frame. For example, we can resize the height column to provide the body height in m instead of cm. Lets call that variable m_height. Well assign our transformed data (with the newly created m_height column) back into our source object (starwars_short) to keep the changes for the future (and not just print it to the console). starwars_short &lt;- starwars_short %&gt;% # assigns your source object, i.e. data, back to itself to save changes mutate(m_height=height/100) # creates the new variable &quot;m_height&quot; and adds it to the end of the data frame starwars_short # print the data to your console to inspect the new column ## # A tibble: 87 x 6 ## name species birth_year mass height m_height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Luke Skywalker Human 19 77 172 1.72 ## 2 C-3PO Droid 112 75 167 1.67 ## 3 R2-D2 Droid 33 32 96 0.96 ## 4 Darth Vader Human 41.9 136 202 2.02 ## 5 Leia Organa Human 19 49 150 1.5 ## 6 Owen Lars Human 52 120 178 1.78 ## 7 Beru Whitesun lars Human 47 75 165 1.65 ## 8 R5-D4 Droid NA 32 97 0.97 ## 9 Biggs Darklighter Human 24 84 183 1.83 ## 10 Obi-Wan Kenobi Human 57 77 182 1.82 ## # ... with 77 more rows Lets calculate the BMI of the Star Wars characters with the BMI formula and the newly created m_height variable. Save the changes to your data frame by assigning the source object back to itself. starwars_short &lt;- starwars_short %&gt;% # assigns your source object, i.e. data, back to itself to save changes mutate(BMI= mass/m_height^2) # creates the new variable &quot;BMI&quot; and adds it to the end of the data frame starwars_short # print the data to your console to inspect the new column ## # A tibble: 87 x 7 ## name species birth_year mass height m_height BMI ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Luke Skywalker Human 19 77 172 1.72 26.0 ## 2 C-3PO Droid 112 75 167 1.67 26.9 ## 3 R2-D2 Droid 33 32 96 0.96 34.7 ## 4 Darth Vader Human 41.9 136 202 2.02 33.3 ## 5 Leia Organa Human 19 49 150 1.5 21.8 ## 6 Owen Lars Human 52 120 178 1.78 37.9 ## 7 Beru Whitesun lars Human 47 75 165 1.65 27.5 ## 8 R5-D4 Droid NA 32 97 0.97 34.0 ## 9 Biggs Darklighter Human 24 84 183 1.83 25.1 ## 10 Obi-Wan Kenobi Human 57 77 182 1.82 23.2 ## # ... with 77 more rows mutate() does not merely work with mathematical operators. You can also categorize numeric variables with the case_when function, which is part of the mutate() function. starwars_short &lt;- starwars_short %&gt;% mutate(age_cat= case_when( # &quot;cat&quot; is short for &quot;categorized&quot; birth_year &lt; 20 ~ &quot;very young&quot;, birth_year &lt; 40 ~ &quot;young&quot;, birth_year &lt; 70 ~ &quot;mid-aged&quot;, birth_year &lt;= 100 ~ &quot;old&quot;, birth_year &gt; 100 ~ &quot;very old&quot;) ) starwars_short ## # A tibble: 87 x 8 ## name species birth_year mass height m_height BMI age_cat ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Luke Skywalker Human 19 77 172 1.72 26.0 very young ## 2 C-3PO Droid 112 75 167 1.67 26.9 very old ## 3 R2-D2 Droid 33 32 96 0.96 34.7 young ## 4 Darth Vader Human 41.9 136 202 2.02 33.3 mid-aged ## 5 Leia Organa Human 19 49 150 1.5 21.8 very young ## 6 Owen Lars Human 52 120 178 1.78 37.9 mid-aged ## 7 Beru Whitesun lars Human 47 75 165 1.65 27.5 mid-aged ## 8 R5-D4 Droid NA 32 97 0.97 34.0 &lt;NA&gt; ## 9 Biggs Darklighter Human 24 84 183 1.83 25.1 young ## 10 Obi-Wan Kenobi Human 57 77 182 1.82 23.2 mid-aged ## # ... with 77 more rows Finally, you can recode variables by using the recode() function, which is part of the mutate() function. Lets be crazy and recode all droids as robots5 and save the result in a new variable called crazy_species! Please note that recode() has an unusual syntax because it follows the order of old_var = new_var instead of the usual order: new_var = old_var. Therefore, recode() is likely to be retired in the future (use case_when instead). starwars_short &lt;- starwars_short %&gt;% mutate(crazy_species=recode( # alternatively, you could also recode directly back into the species variable species, &quot;Droid&quot;=&quot;Robot&quot;) ) starwars_short ## # A tibble: 87 x 9 ## name species birth_year mass height m_height BMI age_cat crazy_species ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Luke Skywalker Human 19 77 172 1.72 26.0 very young Human ## 2 C-3PO Droid 112 75 167 1.67 26.9 very old Robot ## 3 R2-D2 Droid 33 32 96 0.96 34.7 young Robot ## 4 Darth Vader Human 41.9 136 202 2.02 33.3 mid-aged Human ## 5 Leia Organa Human 19 49 150 1.5 21.8 very young Human ## 6 Owen Lars Human 52 120 178 1.78 37.9 mid-aged Human ## 7 Beru Whitesun lars Human 47 75 165 1.65 27.5 mid-aged Human ## 8 R5-D4 Droid NA 32 97 0.97 34.0 &lt;NA&gt; Robot ## 9 Biggs Darklighter Human 24 84 183 1.83 25.1 young Human ## 10 Obi-Wan Kenobi Human 57 77 182 1.82 23.2 mid-aged Human ## # ... with 77 more rows Tip for advanced users 1: There is a special case of recoding: Sometimes you will receive data (e.g. through an import from SPSS) in which missings are not marked as NA, but with -9 (or any other number). Unfortunately, you will have to tell R that these are missing values and should be set to NA. In this case, use the na_if() function, which is also part of the mutate() function. Luke Skywalker, the first observation in our data frame, is 172cm tall. For the sake of practice, lets set all heights that are equal to 172cm to NA. This time, we wont save this transformation for later use (by reassigning the source object back to itself) since this transformation does not make a lot of sense. starwars_short %&gt;% mutate(height= na_if(height, 172)) ## # A tibble: 87 x 9 ## name species birth_year mass height m_height BMI age_cat crazy_species ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Luke Skywalker Human 19 77 NA 1.72 26.0 very young Human ## 2 C-3PO Droid 112 75 167 1.67 26.9 very old Robot ## 3 R2-D2 Droid 33 32 96 0.96 34.7 young Robot ## 4 Darth Vader Human 41.9 136 202 2.02 33.3 mid-aged Human ## 5 Leia Organa Human 19 49 150 1.5 21.8 very young Human ## 6 Owen Lars Human 52 120 178 1.78 37.9 mid-aged Human ## 7 Beru Whitesun lars Human 47 75 165 1.65 27.5 mid-aged Human ## 8 R5-D4 Droid NA 32 97 0.97 34.0 &lt;NA&gt; Robot ## 9 Biggs Darklighter Human 24 84 183 1.83 25.1 young Human ## 10 Obi-Wan Kenobi Human 57 77 182 1.82 23.2 mid-aged Human ## # ... with 77 more rows 7.5.5 simmarize() [+ group_by()] Instead of using summarize(), you could omit the American English and write summarise(). This function collapses a data frame into a single row that shows you summary statistics about your variables. Be careful not to overwrite your source object with the collapsed data frame, i.e. do not reassign the source object to itself when you use summarize() (at least unless you have a good reason to do so). starwars_short %&gt;% summarize(mean_height = mean(height, na.rm=TRUE)) # collapses the data frame into one variable called &quot;mean_height&quot; ## # A tibble: 1 x 1 ## mean_height ## &lt;dbl&gt; ## 1 174. # na.rm = TRUE -&gt; removes the missing values prior to the computation of the summary We now know that the average Star Wars character is 174cm tall. But the summarize()function grows especially powerful when it is combined with `group_by to display summary statistics for groups. starwars_short %&gt;% group_by(species) %&gt;% # every unique species becomes its own group summarize(mean_height = mean(height, na.rm=TRUE), # collapses the data frame into one row with one variable called &quot;mean_height&quot;... count = n() # and a second variable that shows the group size (i.e. count) ) ## # A tibble: 38 x 3 ## species mean_height count ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Aleena 79 1 ## 2 Besalisk 198 1 ## 3 Cerean 198 1 ## 4 Chagrian 196 1 ## 5 Clawdite 168 1 ## 6 Droid 131. 6 ## 7 Dug 112 1 ## 8 Ewok 88 1 ## 9 Geonosian 183 1 ## 10 Gungan 209. 3 ## # ... with 28 more rows We learn from the 6 droids in our data set that droids are small, 131cm on average. But Ewoks are even smaller (88cm on average). Pro tip: you can even group by two groups at the same time with the method group_by(x1, x2, .add=TRUE). Finally, we can also retrieve all relevant summary statistics of a classic box plot: starwars_short %&gt;% group_by(species) %&gt;% # every unique species becomes its own group summarize(MAX = max(height, na.rm = TRUE), UQ= quantile(height, 0.75, na.rm = TRUE), M = mean(height, na.rm = TRUE), # SD = sd(height, na.rm = TRUE), # calculating the standard deviation is useless because we often have only 1 observation per species LQ= quantile(height, 0.25, na.rm = TRUE), MIN = min(height, na.rm = TRUE), count = n() # shows the group_size (i.e. count) ) ## # A tibble: 38 x 7 ## species MAX UQ M LQ MIN count ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Aleena 79 79 79 79 79 1 ## 2 Besalisk 198 198 198 198 198 1 ## 3 Cerean 198 198 198 198 198 1 ## 4 Chagrian 196 196 196 196 196 1 ## 5 Clawdite 168 168 168 168 168 1 ## 6 Droid 200 167 131. 96 96 6 ## 7 Dug 112 112 112 112 112 1 ## 8 Ewok 88 88 88 88 88 1 ## 9 Geonosian 183 183 183 183 183 1 ## 10 Gungan 224 215 209. 201 196 3 ## # ... with 28 more rows Tip for advances users 2: If you want to count the unique values of variables, then data %&gt;% group_by(a, b) %&gt;% summarize(n = n()) might not be the best solution (its a lot of code, isnt it?). starwars_short %&gt;% group_by(species, age_cat) %&gt;% summarize(count = n()) ## # A tibble: 51 x 3 ## # Groups: species [38] ## species age_cat count ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Aleena &lt;NA&gt; 1 ## 2 Besalisk &lt;NA&gt; 1 ## 3 Cerean old 1 ## 4 Chagrian &lt;NA&gt; 1 ## 5 Clawdite &lt;NA&gt; 1 ## 6 Droid very old 1 ## 7 Droid very young 1 ## 8 Droid young 1 ## 9 Droid &lt;NA&gt; 3 ## 10 Dug &lt;NA&gt; 1 ## # ... with 41 more rows For more efficient code, you can use the count() function instead: data %&gt;% count(a, b). starwars_short %&gt;% count(species, age_cat) ## # A tibble: 51 x 3 ## species age_cat n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Aleena &lt;NA&gt; 1 ## 2 Besalisk &lt;NA&gt; 1 ## 3 Cerean old 1 ## 4 Chagrian &lt;NA&gt; 1 ## 5 Clawdite &lt;NA&gt; 1 ## 6 Droid very old 1 ## 7 Droid very young 1 ## 8 Droid young 1 ## 9 Droid &lt;NA&gt; 3 ## 10 Dug &lt;NA&gt; 1 ## # ... with 41 more rows 7.5.6 Chaining functions in a pipe All of the dplyr functions can be chained in one single pipe. Using the original starwars_data, well only analyze Star Wars characters who are older than 25 years (filter()), calculate the BMI (mutate()), group them by their species (group_by()) and summarize the average BMI (summarize()). Well display the final result in an ascending order (arrange()). starwars_data %&gt;% mutate(BMI = mass/(height/100)^2) %&gt;% filter(birth_year&gt;25) %&gt;% group_by(species) %&gt;% summarize(mean_BMI = mean(BMI, na.rm = TRUE)) %&gt;% arrange(mean_BMI) ## # A tibble: 14 x 2 ## species mean_BMI ## &lt;chr&gt; &lt;dbl&gt; ## 1 Gungan 17.2 ## 2 Twi&#39;lek 17.4 ## 3 Mirialan 18.8 ## 4 Cerean 20.9 ## 5 Wookiee 21.5 ## 6 Rodian 24.7 ## 7 Human 25.3 ## 8 Mon Calamari 25.6 ## 9 Zabrak 26.1 ## 10 Droid 30.8 ## 11 Trandoshan 31.3 ## 12 Yoda&#39;s species 39.0 ## 13 Hutt 443. ## 14 &lt;NA&gt; NaN 7.6 Take-Aways Tidy data: is a tabular in which each column represents one single variable, each row represents a single observation and each cell contains only one single value Pipe operator: %&gt;% is used to chain functions and apply them to a source object. We call these chains of functions pipes dplyr functions: there are five main dplyr functions that you should know of: select, filter, arrange, mutate, and summarize [+ group_by]. 7.7 More tutorials on this You still have questions? The following tutorials &amp; papers can help you with that: Computational Methods in der politischen Kommunikationsforschung by J. Unkel, Tutorial 7, 9 &amp; 10 R for Data Science, Chapter 12 R for Data Science, Chapter 5.1.3 and the following YaRrr! The Pirates Guide to R by N.D.Phillips, Tutorial 10.4 The tidyverse style guide Data wrangling with dplyr &amp; tidyr Cheat Sheet Now lets see what youve learned so far: Exercise 2: Test your knowledge. To be precise, the pipe operator was introduced to R with the package magrittr, not with dplyr. Nowadays, the %&gt;% operator can be used outside the tidyverse package if magrittr is installed and loaded: library(magrittr). Please, dont hate me for this. "],["exercise-2-test-your-knowledge.html", " 8 Exercise 2: Test your knowledge 8.1 Task 1 8.2 Task 2 8.3 Task 3 8.4 Task 4 8.5 Task 5 8.6 Task 6 8.7 Task 7", " 8 Exercise 2: Test your knowledge After working through Exercise 2, youll have assessed how well you know dplyr know what dplyr functions and concepts you might want to repeat again have managed to apply the dplyr concepts to data 8.1 Task 1 Below you will see multiple choice questions. Please try to identify the correct answers. 1, 2, 3 and 4 correct answers are possible for each question. 1. What are the main characteristics of tidy data? Every cell contains values. Every cell contains a variable. Every observation is a column. Every observation is a row. 2. What are dplyr functions? summary() describe() mutate() manage() 3. How can you sort the eye_color of Star Wars characters from Z to A? starwars_data %&gt;% arrange(desc(eye_color)) starwars_data %&gt;% arrange(eye_color) starwars_data %&gt;% select(arrange(eye_color)) starwars_data %&gt;% select(eye_color) %&gt;% arrange(desc(eye_color)) 4. Imagine you want to recode the height of the these characters. You want to have three categories from small and medium to tall. What is a valid approach? starwars_data %&gt;% mutate(height = case_when(height&lt;=150~\"small\",height&lt;=190~\"medium\",height&gt;190~\"tall\")) starwars_data %&gt;% mutate(height = case_when(height&lt;=150~small,height&lt;=190~medium,height&gt;190~tall)) starwars_data %&gt;% recode(height = case_when(height&lt;=150~\"small\",height&lt;=190~\"medium\",height&gt;190~\"tall\")) starwars_data %&gt;% recode(height = case_when(height&lt;=150~small,height&lt;=190~medium,height&gt;190~tall)) 5. Imagine you want to provide a systematic overview over all hair colors and what species wear these hair colors frequently (not accounting for the skewed sampling of species)? What is a valid approach? starwars_data %&gt;% group_by(hair_color) %&gt;% group_by(species) %&gt;% summarize(count = n()) %&gt;% arrange(hair_color) starwars_data %&gt;% group_by(hair_color, species) %&gt;% summarize(count = n()) %&gt;% arrange(hair_color) starwars_data %&gt;% group_by(hair_color &amp; species) %&gt;% summarize(count = n()) %&gt;% arrange(hair_color) starwars_data %&gt;% group_by(hair_color + species) %&gt;% summarize(count = n()) %&gt;% arrange(hair_color) 8.2 Task 2 Its you turn now. Load the starwars data like this: library(dplyr) # to activate the dplyr package starwars_data &lt;- starwars # to assign the pre-installed starwars data set (dplyr) into a source object in our environment How many humans are contained in the starwars data overall? (Hint: use summarize(count = n()) or count()) 8.3 Task 3 How many humans are contained in starwars by gender? 8.4 Task 4 What is the most common eye_color among Star Wars characters? (Hint: use arrange()) 8.5 Task 5 What is the average mass of Star Wars characters that are not human and have yellow eyes? (Hint: remove all NAs) 8.6 Task 6 Compare the mean, median, and standard deviation of mass for all humans and droids. (Hint: remove all NAs) 8.7 Task 7 Create a new variable in which you store the mass in gram (gr_mass). Add it to the data frame. Test whether your solution works by printing your data to the console, but only show the name, species, mass, and your new variable gr_mass. When youre ready to look at the solutions, you can find them here: Solutions for Exercise 2. Are you ready for some beautiful graphs? Then check out the next Tutorial: Data visualization with ggplot. "],["tutorial-data-visualization-with-ggplot.html", " 9 Tutorial: Data visualization with ggplot 9.1 Why not stick with Base R? 9.2 Components of a ggplot graph 9.3 Installing &amp; activating ggplot 9.4 Building your first plot 9.5 Other common plot types 9.6 Take Aways 9.7 More tutorials on this", " 9 Tutorial: Data visualization with ggplot After working through Tutorial 9, youll know what each graphical component of a ggplot graph contributes to the final visualization understand the grammer of graphics (or simply: the ggplot2 syntax) to combine graphical components know how to make your own data visualizations using ggplot2 9.1 Why not stick with Base R? The ggplot2 package, i.e. the data visualization package of tidyverse, has become the R package for data visualization. While Base R can be used to visualize data, the ggplot2 package makes data visualization so much easier that I recommend starting with ggplot2 right away and skipping data visualization in Base R altogether. The gg in ggplot2 stands for grammar of graphics, which means that we can describe each component of a graph layer by layer and component by component. You only have to provide ggplot() with a source object (i.e. data) and specify what variables it should map to the aesthetical attributes (color, shape, size) of certain geometric objects (points, lines, bars)  and ggplot will take care of the rest! The inventor of ggplot2, Hadley Wickham, describes the benefits of ggplot2 like this: In order to unlock the full power of ggplot2, youll need to master the underlying grammar. By understanding the grammar, and how its components fit together, you can create a wider range of visualizations, combine multiple sources of data, and customise to your hearts content The grammar makes it easier for you to iteratively update a plot, changing a single feature at a time. The grammar is also useful because it suggests the high-level aspects of a plot that can be changed, giving you a framework to think about graphics, and hopefully shortening the distance from mind to paper. It also encourages the use of graphics customised to a particular problem, rather than relying on specific chart types. (Wickham et al., 2021, no page; bold words inserted) Just as dplyr simplifies data manipulation, ggplot2 simplifies data visualization. In addition, ggplot2 and dplyr work hand in hand: You can prepare your data selection and manipulation with dplyr and pipe it directly into ggplot to turn your transformed data into a beautiful graph. With only a few lines of code, you can produce graphs like this one: This is the code. Right now, it might still look a bit overwhelming to you, but once youve understood the grammar of graphics, it really is a just a small jigsaw puzzle. Moreover, you dont usually start with graphs that are this complicated, but with basic scatter or bar plots. library(ggplot2) plot &lt;- starwars_data %&gt;% filter(species == &quot;Human&quot; | species == &quot;Droid&quot;) %&gt;% ggplot(aes(x = height, y = mass, size = birth_year, fill = species)) + geom_point(shape = 21, alpha = 0.25, color = &quot;black&quot;) + scale_y_continuous(limits = c(30, 140)) + scale_x_continuous(limits = c(90, 210)) + scale_y_continuous(breaks = c(40, 60, 80, 100, 120, 140, 160)) + scale_x_continuous(breaks = c(100, 120, 140, 160, 180, 200)) + scale_size(range = c(1, 11), name = &quot;age&quot;) + ggrepel::geom_text_repel(aes(label = name), size = 2.3) + theme_bw() + labs(title = &quot;Mass vs. height of humans and droids in Star Wars&quot;, x = &quot;Height (cm)&quot;, y = &quot;Weight (kg)&quot;) + facet_wrap(~species) To visit the official documentation of ggplot2: - type ?ggplot2 in your console - visit the ggplot documentation - visit the ggplot homepage of the tidyverse 9.2 Components of a ggplot graph As mentioned before, the main idea behind ggplot is to generate a statistical plot by combining layers that represent geometric objects (e.g. points and lines). By linking data to the aesthetic features of these geometric objects (e.g. colors, size, transparency), the aesthetic properties of the geometric objects may be controlled. In the words of Wickham: A graphic maps the data to the aesthetic attributes (colour, shape, size) of geometric objects (points, lines, bars). Wickham et al., 2021, no page; bold words inserted Image: The logic of adding layer by layer in ggplot (Source: R @ Ewah 2020): The necessary components of a ggplot graph are: Source object / data: The data that you would like to visualize. Geometries geom_: Geom options allow you to specify what geometric objects will represent the data (i.e. points, bars, lines, and many more). Aesthetics aes(): Aesthetics allows you to map variables to the x- and y-axis and to the aesthetics of those geometric objects (i.e. position, color, size, shape, linetype, and transparency). The complementary, but not necessary components of a ggplot graph are: Scales scale_: Scale options allow you to fine-tune the mapping from the variables to the aesthetics. You can fine-tune axis limits, tick breaks, grid lines, or any other axis/geometric object transformations that depend on the range of a specific scale. Statistical transformations stat_: Allows you to produce statistical summaries of the data for visualization (i.e. means and standard deviations, fitted curves, and many more). Coordinate system coord_: Allows you to change the appearance of your coordinate system (i.e. flip the coordinates to turn horizontal bar chart into a vertical one). Position: to adjust overlapping objects, e.g. jittering, stacking or dodging. Facets facet_: Allows you to divide your plot into multiple subplots. Visual themes theme(): Allows you to specify the visual basics of a plot, such as background, default typeface, sizes, and colors. Axis labels labs(): Allows you to change the plots main title and the axis labels. 9.3 Installing &amp; activating ggplot You can always activate ggplot2 by activating the meta-package tidyverse: library(tidyverse) If for some reason you do not want to activate the whole tidyverse, you should install ggplot2 and activate this package separately: install.packages(&quot;ggplot2&quot;) # install the package (only on the first time) library(ggplot2) # active the package 9.4 Building your first plot In the next sections, you will create your very first plot  layer by layer. We will look at some of the most important components that you will regularly add to graphs and you will learn how to make use of them. 9.4.1 Data Obviously, you need data to perform data visualization. Therefore, our first step is to load the starwars data, but lets keep only humans and droids for now. To this end, assign your transformed data to a new data frame called human_droid_data. human_droid_data &lt;- dplyr::starwars %&gt;% filter(species == &quot;Human&quot; | species == &quot;Droid&quot;) The function ggplot() can only create a plot if we explicitly tell the function what data to use, so this graphical component is necessary. Using our dplyr skills, lets use the human_droid_data as our source object and apply the ggplot() function to it by using a pipe (i.e. %&gt;%). human_droid_data %&gt;% ggplot() The ggplot() function creates a blank canvas (i.e. first layer). We now have to draw on it. 9.4.2 Aesthetics To draw on this blank canvas, we must at least tell the ggplot() function which variables to assign to the x- and y-axis by using the aes() function. Thus, the Aesthetics graph component is also necessary in every single plot. human_droid_data %&gt;% ggplot(aes(x = height, y = mass)) The aes() function allows you to specify the following arguments (and many more, as you will learn over time): x: the variable that should be mapped to the x axis y: the variable that should be mapped to the y size: the variable that should be used for determining the size of a geometric object fill: the variable that should be used for filling a geometric object with a specific color color: the variable that should be used for outlining a geometric object with a specific color 9.4.3 Geometrics Finally, we can turn to the last necessary component of any ggplot graph: the geometric objects that fill your canvas. The choice of these geometric objects determines what kind of chart you create. The geom_ component of the ggplot() function allows you to create the following chart types (and many more, as you will learn over time): geom_bar(): to create a bar chart geom_histogram: to create a histogram geom_line(): to create a line graph geom_point(): to create a scatter or bubble plot geom_boxplot(): to create a box plot Now lets add the data points (x,y) with geom_point() to our canvas to make it a scatter plot: human_droid_data %&gt;% ggplot(aes(x = height, y = mass)) + geom_point() Thats a scatter plot for sure! And you only needed three necessary components to create it: data (i.e. a source object), aesthetics aes(), and geometric objects geom_. 9.4.4 Scales A scale is a mapping from data to the final values that computers can use to actually show the aesthetics. In this sense, a scale regulates the aesthetic mapping of variables to aesthetics. Providing a scale_ is not necessary to create a graph, but it allows you to fine-tune aesthetic mappings to customize your graph. scale_ is very powerful and over time, you will learn about a lot of things that you can customize with it. For now, we will only focus on a few of these. We will use scale_ to : change the limits and ticks of the x and y axis change how a third variable (besides x and y) is mapped to the aesthetics of our geometric object First, we will use scale_ to modify the x and the y axis by providing the graph with new axis limits. human_droid_data %&gt;% ggplot(aes(x = height, y = mass)) + geom_point() + scale_y_continuous(limits = c(30, 140)) + # modify the y axis limits scale_x_continuous(limits = c(90, 210)) # modify the x axis limits Second, well add more ticks to make the graph better readable. human_droid_data %&gt;% ggplot(aes(x = height, y = mass)) + geom_point() + scale_y_continuous(limits = c(30, 140)) + scale_x_continuous(limits = c(90, 210)) + scale_y_continuous(breaks = c(40, 60, 80, 100, 120, 140, 160)) + # choose where the ticks of the y axis appear scale_x_continuous(breaks = c(100, 120, 140, 160, 180, 200)) # choose where the ticks of the x axis appear Until now, we have used scale_ to transform only the axes. But we can also use it to change the mapping of variables to geometric objects. To demonstrate this, we now add another variable to our graph, namely the age (birth_year) of the humanoid and droid Star Wars characters. Lets map age to our data points (i.e. geom_point()) so that larger bubbles reflect older age. human_droid_data %&gt;% ggplot(aes(x = height, y = mass, size=birth_year)) + # map birth_year (age) to the size of the following geometric objects geom_point() + scale_y_continuous(limits = c(30, 140)) + scale_x_continuous(limits = c(90, 210)) + scale_y_continuous(breaks = c(40, 60, 80, 100, 120, 140, 160)) + scale_x_continuous(breaks = c(100, 120, 140, 160, 180, 200)) Personally, I feel like these bubbles could use a little bit of rescaling to make age differences stand out more. In addition, you could get a nicer title for the size legend than birth_year. Lets try that. human_droid_data %&gt;% ggplot(aes(x = height, y = mass, size=birth_year)) + geom_point() + scale_y_continuous(limits = c(30, 140)) + scale_x_continuous(limits = c(90, 210)) + scale_y_continuous(breaks = c(40, 60, 80, 100, 120, 140, 160)) + scale_x_continuous(breaks = c(100, 120, 140, 160, 180, 200)) + scale_size(range = c(1, 11), name = &quot;age&quot;) # sets the bubbles&#39; size in a range between 1 and 11 and renames the respective legend title to &quot;age&quot; Perfect! The legend spells age and age differences seem a bit more obvious now. Unfortunately, some data points are now overlapping. I think this is a good time to introduce you to the differences between using scale_ and adding aesthetics to the geom_ objects directly. While the former allows you to change the mapping from variables to the aesthetics of geometric objects, the latter one allows you to provide a constant. This means that the aesthetic mapping does not depend on the values of a variable, but is set to a single default value. To demonstrate this and fix the overlap of our bubbles, we change the transparency value of the bubbles so that they become transparent. Note that all the values given are constants, which means that they do not depend on a third variable like birth_year. human_droid_data %&gt;% ggplot(aes(x = height, y = mass, size=birth_year)) + geom_point(shape = 21, fill = &quot;black&quot;, alpha = 0.25, color = &quot;black&quot;) + # shape = 21 is creating bubbles that have a border (i.e. outline), fill = &quot;black&quot; fills the bubble with black ink, alpha = 0.25 to make the bubbles` black ink 25% transparent and color = &quot;black&quot; to make the border (i.e. outline) pitch black scale_y_continuous(limits = c(30, 140)) + scale_x_continuous(limits = c(90, 210)) + scale_y_continuous(breaks = c(40, 60, 80, 100, 120, 140, 160)) + scale_x_continuous(breaks = c(100, 120, 140, 160, 180, 200)) + scale_size(range = c(1, 11), name = &quot;age&quot;) This looks way more readable. I think we are ready to move on to Themes. 9.4.5 Themes Just like scale_, theme_is an optional ggplot component, i.e. not necessary. Themes are visually appealing presets for charts, e.g., they influence whether grid lines are visible or whether certain color palettes are applied to the data. By using themes you can make your graphs more beautiful and give them a consistent style without any effort, which is especially useful for longer texts like theses. To familiarize yourself with the various options, take a look at this overview of all ggplot2 themes. If you dont like grid lines, for example, theme_classic() might be to your taste: human_droid_data %&gt;% ggplot(aes(x = height, y = mass, size=birth_year)) + geom_point(shape = 21, fill = &quot;black&quot;, alpha = 0.25, color = &quot;black&quot;) + scale_y_continuous(limits = c(30, 140)) + scale_x_continuous(limits = c(90, 210)) + scale_y_continuous(breaks = c(40, 60, 80, 100, 120, 140, 160)) + scale_x_continuous(breaks = c(100, 120, 140, 160, 180, 200)) + scale_size(range = c(1, 11), name = &quot;age&quot;) + theme_classic() Personally, I really enjoy the theme_bw() (black-and-white theme). So lets apply it to our graph: human_droid_data %&gt;% ggplot(aes(x = height, y = mass, size=birth_year)) + geom_point(shape = 21, fill = &quot;black&quot;, alpha = 0.25, color = &quot;black&quot;) + scale_y_continuous(limits = c(30, 140)) + scale_x_continuous(limits = c(90, 210)) + scale_y_continuous(breaks = c(40, 60, 80, 100, 120, 140, 160)) + scale_x_continuous(breaks = c(100, 120, 140, 160, 180, 200)) + scale_size(range = c(1, 11), name = &quot;age&quot;) + theme_bw() 9.4.6 Labs Again, labs() is not a necessary, but an optional component of your graph. Using the labs() function allows you to set a main title for your plot and to change the labels of the x and y axis. Lets try it: human_droid_data %&gt;% ggplot(aes(x = height, y = mass, size=birth_year)) + geom_point(shape = 21, fill = &quot;black&quot;, alpha = 0.25, color = &quot;black&quot;) + scale_y_continuous(limits = c(30, 140)) + scale_x_continuous(limits = c(90, 210)) + scale_y_continuous(breaks = c(40, 60, 80, 100, 120, 140, 160)) + scale_x_continuous(breaks = c(100, 120, 140, 160, 180, 200)) + scale_size(range = c(1, 11), name = &quot;age&quot;) + theme_bw() + labs(title = &quot;Mass vs. height of humans and droids in Star Wars&quot;, x = &quot;Height (cm)&quot;, y = &quot;Weight (kg)&quot;) Now that weve added a main title, it becomes clear that we cant really distinguish the data points that represent humans from those that represent droids. 9.4.7 Facets Faceting divides plot into tiny subplots, which display different subsets of your data. Facets are an effective way to explore your data because they allow you to rapidly detect divergent patterns in these subsets. Of course, faceting is optional, i.e. not necessary. You dont need faceting if you dont want to compare different groups within your data. The two approaches to faceting are: facet_wrap(): uses the levels of one (or more) variable(s) to create groups + panels for each group; useful if you have a single categorical variable with many levels facet_grid(): produces a matrix of panels defined by two variables which form the rows and columns Image: The logic of faceting (Source: Wickham et al., 2021): Lets use the facet_wrap() function to create two subplots for our two different levels of the species variable: Droid and Human. You can provide two arguments to facet_wrap(): ~, followed by the grouping variable nrow: the number of rows in which panels should be placed human_droid_data %&gt;% ggplot(aes(x = height, y = mass, size=birth_year)) + geom_point(shape = 21, fill = &quot;black&quot;, alpha = 0.25, color = &quot;black&quot;) + scale_y_continuous(limits = c(30, 140)) + scale_x_continuous(limits = c(90, 210)) + scale_y_continuous(breaks = c(40, 60, 80, 100, 120, 140, 160)) + scale_x_continuous(breaks = c(100, 120, 140, 160, 180, 200)) + scale_size(range = c(1, 11), name = &quot;age&quot;) + theme_bw() + labs(title = &quot;Mass vs. height of humans and droids in Star Wars&quot;, x = &quot;Height (cm)&quot;, y = &quot;Weight (kg)&quot;) + facet_wrap(~species, nrow=2) # using ~grouping_variable and nrow = 2 shows the two panels on top of each other Great, finally we can distinguish the data points representing humans from those representing droids! On the left side, however, the panel with the humans looks a bit empty. Maybe we should put them next to each other. human_droid_data %&gt;% ggplot(aes(x = height, y = mass, size=birth_year)) + geom_point(shape = 21, fill = &quot;black&quot;, alpha = 0.25, color = &quot;black&quot;) + scale_y_continuous(limits = c(30, 140)) + scale_x_continuous(limits = c(90, 210)) + scale_y_continuous(breaks = c(40, 60, 80, 100, 120, 140, 160)) + scale_x_continuous(breaks = c(100, 120, 140, 160, 180, 200)) + scale_size(range = c(1, 11), name = &quot;age&quot;) + theme_bw() + labs(title = &quot;Mass vs. height of humans and droids in Star Wars&quot;, x = &quot;Height (cm)&quot;, y = &quot;Weight (kg)&quot;) + facet_wrap(~species) # nrow=1 is the default, so you don´t have to call it explicitly I like that! 9.4.8 Saving graphs I think its time that we save this plot. To finish of this masterpiece (and make it less triste), lets add some final colors before saving. Well fill our bubbles with colorful ink based on the species variable, so we need to add fill=species to the aes() and remove the default black ink provided in the geom_point() function. human_droid_data %&gt;% ggplot(aes(x = height, y = mass, size=birth_year, fill=species)) + geom_point(shape = 21, alpha = 0.25, color = &quot;black&quot;) + scale_y_continuous(limits = c(30, 140)) + scale_x_continuous(limits = c(90, 210)) + scale_y_continuous(breaks = c(40, 60, 80, 100, 120, 140, 160)) + scale_x_continuous(breaks = c(100, 120, 140, 160, 180, 200)) + scale_size(range = c(1, 11), name = &quot;age&quot;) + theme_bw() + labs(title = &quot;Mass vs. height of humans and droids in Star Wars&quot;, x = &quot;Height (cm)&quot;, y = &quot;Weight (kg)&quot;) + facet_wrap(~species) Congratulations! You have just managed to recreate the plot from the beginning of this tutorial! The only thing we havent covered yet is the labeling of all data points, because for that youd need the ggrepel package to not mess up the labels - and thats not part of ggplot2. So lets skip that and save our graph. To use your graph in another document, e.g. a theses written in Word, youll have to export the plot first. Therefore, you must assign your plot to a new object and call the ggsave() function on that object. The plot will be saved to your working directory and formatted according to the file extension you specified (for example: .jpeg or .png). plot &lt;- human_droid_data %&gt;% ggplot(aes(x = height, y = mass, size=birth_year, fill=species)) + geom_point(shape = 21, alpha = 0.25, color = &quot;black&quot;) + scale_y_continuous(limits = c(30, 140)) + scale_x_continuous(limits = c(90, 210)) + scale_y_continuous(breaks = c(40, 60, 80, 100, 120, 140, 160)) + scale_x_continuous(breaks = c(100, 120, 140, 160, 180, 200)) + scale_size(range = c(1, 11), name = &quot;age&quot;) + theme_bw() + labs(title = &quot;Mass vs. height of humans and droids in Star Wars&quot;, x = &quot;Height (cm)&quot;, y = &quot;Weight (kg)&quot;) + facet_wrap(~species) ggsave(filename = &quot;mass_vs_height.jpeg&quot;, plot) 9.5 Other common plot types I cant give an overview of all possible types of plots, but I can at least touch a bit on how other common types of geom_ behave. 9.5.1 bar plots Bar plots are very common. They are either (1) used to display the frequency with which a certain factor level of a categorical variable occurs or (2) to display relationships between a categorical variable and a metric variable. So lets create a quick bar plot using the sex variable (categorical, three factor levels) and get an overview on how many human and droidic Star Wars characters are male, female, or do not have a sex. human_droid_data %&gt;% ggplot(aes(x = sex)) + # We only have to specify the variable that we want to get the count for (i.e. number of observations) geom_bar() Next, lets look at the relationship between sex and the height (metric) variable. We will produce a bar plot that displays the mean height of each group: human_droid_data %&gt;% ggplot(aes(x = sex, y=height)) + # Now we need to specify the variable that we want to summarize with mean statistics geom_bar(stat = &quot;summary&quot;, fun.y = &quot;mean&quot;) # apply the summary statistic of y (mean) to the geom_bars Maybe we want to sort the bars according to their mean. Lets reorder the factor levels manually. human_droid_data %&gt;% mutate(sex = factor(sex, levels = c(&quot;male&quot;, &quot;female&quot;, &quot;none&quot;))) %&gt;% ggplot(aes(x = sex, y=height)) + stat_summary(geom = &quot;bar&quot;, fun = &quot;mean&quot;) And if we would like to have a horizontal bar plot, we can use the coord_ component of ggplot() to flip the coordinates. human_droid_data %&gt;% mutate(sex = factor(sex, levels = c(&quot;male&quot;, &quot;female&quot;, &quot;none&quot;))) %&gt;% ggplot(aes(x = sex, y=height)) + stat_summary(geom = &quot;bar&quot;, fun = &quot;mean&quot;) + coord_flip() 9.5.2 box plots Box plots are a great option to summarize metric variables (by groups). They provide you with the Five-number-summary: the sample minimum (smallest observation)  lower whisker the lower quartile  lower end of the box the median (the middle value)  thick black line the upper quartile  upper end of the box the sample maximum (largest observation)  upper whisker Lets create box plots of the height for human and droidic Star Wars characters who are male, female, or do not have a sex. human_droid_data %&gt;% ggplot(aes(x = sex, y=height)) + geom_boxplot() 9.6 Take Aways graph creation: ggplot() mapping variables to aesthetics: aes(x, y, color, fill, size, etc.) chart type: geom_bar(), geom_line(), geom_point(), geom_boxplot() (for example) titles: labs() axis limits/ticks: scale_x_continuous(), scale_y_continuous() mapping variables to geom size: scale_size() themes: theme_classic(), theme_light(), theme_bw() (for example) faceting: facet_wrap() or facet_grid save images: ggsave() 9.7 More tutorials on this You still have questions? The following tutorials &amp; papers can help you with that: Chang, W. R (2021). R Graphics Codebook. Practical Recipes for Visualizing Data. Link Wickham, H., Navarro, D., &amp; Pedersen, T. L. (2021). ggplot2: elegant graphics for data analysis. Online, work-in-progress version of the 3rd edition. Link Hehman, E., &amp; Xie, S. Y. (2021). Doing Better Data Visualization. Advances in Methods and Practices in Psychological Science. DOI: 10.1177/25152459211045334 Link R Codebook by J.D. Long and P. Teetor, Tutorial 10 Now lets see what youve learned so far: Exercise 3: Test your knowledge. "],["exercise-3-test-your-knowledge.html", " 10 Exercise 3: Test your knowledge 10.1 Task 1 10.2 Task 2 10.3 Task 3", " 10 Exercise 3: Test your knowledge After working through Exercise 3, youll be able to see a graph and recreate it with ggplot2 be able to see a problem and customize a plot with ggplot2 to solve it 10.1 Task 1 The data set glbwarm comes pre-installed with the processR package that we will be working with soon. The data comprises 815 US individuals (417 females, 398 males) who agreed to engage in online questionnaires. They roughly represent the population of the United States. Lets install / activate the processR package first and assign the glbwarm data to a source object. # installing/loading the package: if(!require(processR)) { install.packages(&quot;processR&quot;); require(processR) } #load / install+load processR data &lt;- processR::glbwarm Familiarize yourself with the data set (Hint: use the help() / ? function) and then try to reproduce this plot with dplyr and ggplot2. (Hint: You can hide the legend by adding theme(legend.position = \"none\") to your plot.) 10.2 Task 2 Now, try to reproduce this graph. (Hint: You will need to recode the ideology variable in a way that higher values represent stronger attitudes, independent of partisanship.) 10.3 Task 3 Can you make a chart that breaks down the relationship between age, negative emotions about climate change, and ideological extremity for the different sexes AND parties? When youre ready to look at the solutions, you can find them here: Solutions for Exercise 3. We have officially finished our chapters on data management and visualization! We can move on to data analysis and a new Tutorial: Linear regression. "],["tutorial-linear-regression.html", " 11 Tutorial: Linear regression 11.1 Knowing your data 11.2 Visual inspection of linear trends 11.3 Pearson´s r 11.4 OLS regression 11.5 Standardizing coefficients 11.6 Multiple regression 11.7 Standardized multiple regression 11.8 Take Aways 11.9 More tutorials on this", " 11 Tutorial: Linear regression After working through Tutorial 11, youll know what a linear trend and a linear regression is know how to visualize linear relationships know how to run linear models 11.1 Knowing your data Starting in this chapter, we turn away from data management and data visualization and toward data analysis. Although you are all certainly already well acquainted with linear regression, it is worth repeating a few of the basic concepts: you will need them to understand moderation and mediation analysis. In this chapter, we will use the glbwarm data set, which you have just tackled in Exercise 3: Test your knowledge. The data set comprises 815 US individuals who roughly represent the population of the United States. All of the following analyses can be found in Hayes, 2022, which inspired this seminar. If any of my explanations seem incomprehensible to you, or if you want to know more details and develop even greater skills, the book is more than worth the buy! Lets load Andrew F. Hayes package and data and assign them to a source object. # installing/loading the package: if(!require(processR)) { install.packages(&quot;processR&quot;); require(processR) } #load / install+load processR data &lt;- processR::glbwarm You´ll find two variables in this data set that will be of great interest to us because they are great to learn linear regression with: govact (= support for government action): The variable is an index of how each participant answered five questions regarding how supportive he or she is of different policies or activities taken by the US government to address the threat of global climate change (e.g. How much do you support or oppose increasing government investment for developing alternative energy like biofuels, wind, or solar by 25%?). Response options were scaled from Strongly opposed (coded 1) or Strongly support (7). negemot (= negative emotions about climate change): This index measures participants´ negative emotional reactions to the potential of climate change. Participants´ responses to a question asking how often they experience each of three emotions while thinking about global warming: worried, alarmed, and concerned. Not at all, somewhat, a little bit, some, a fair lot, and a great deal were among the response alternatives. These replies were numerically coded from 1 to 6, and the average of each participant´s responses across all three emotions was calculated. Higher scores reect feeling stronger negative emotions. Throughout this chapter, the following research question will guide us on our way to master linear regression: Research Question: Do people who feel stronger negative emotions about the prospect of climate change report greater support for government action than those who feel such negative emotions to a lesser extent? 11.2 Visual inspection of linear trends Before doing any data analysis, its always an excellent idea to first visually inspect the variables you want to use for analysis. Thus, to answer our research question, we should first visually examine the relationship between these two variables. To this end, well use a bubble plot: data %&gt;% ggplot(aes(x=negemot, y=govact)) + geom_count() + theme_bw() + labs(x=&quot;Negative emotions about climate change&quot;, y=&quot;Support for government action against climate change&quot;) Evaluation: It seems that people who express larger negative sentiments about climate change are also more supportive of government action. This correlation is far from perfect, but the pattern is evident. 11.3 Pearson´s r Since visual inspections can take us only this far, we should quantify the association. We´ll use Pearson´s r for that since we look at two quasi-metric variables. If you need a more extensive recap / explanation of the Pearsons r test, check out this video by TipTopBio, 2020 (first 9+1/2 min). cor.test(data$govact,data$negemot, method=&quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: data$govact and data$negemot ## t = 20.183, df = 813, p-value &lt; 0.00000000000000022 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.5301050 0.6217505 ## sample estimates: ## cor ## 0.5777458 Evaluation: The association between negative emotions about climate change and the support for government action is medium and positive (r = 0.57, p &lt; 0.001). We can conclude that U.S. Americans who have more negative feelings about climate change are also more likely to support government action against climate change. 11.4 OLS regression A linear regression model is an equation that uses information contained in the relationship between an explanatory variable X (also called predictor or independent variable) and an outcome variable Y (also called dependent variable) to make predictions. When doing a linear regression analysis, we try to fit a regression line through all data points (X[i] , Y[i]) of the scatter or bubble plot. We could use many different lines, but we´ll always use that one that minimizes the distance between each point of (X[i] , Y[i]) and the line. Speaking in Math, the aim is to estimate the regression model parameters such that the resulting equation produces estimates of the dependent variable that have the lowest residuals (as measured by the ordinary least squares criterion). A generic linear regression equation looks like this: 1. Y = i[Y] + b[1] * X, where i[Y] is the intercept (often called b[0]) and b[1] is the regression coefficient of X. A generic interpretation of a simple linear regression would be that two individuals who differ by one unit on X are estimated to differ by Xs regression coefficient units on Y. For a dichotomous / binary X variable, the regression coefficient estimates the difference between both group means instead (because a one unit increase means changing groups, X = 1 and X = 0). If you need a more extensive recap / explanation of linear regressions and the ordinary least squares criterion, check out this video by Khan Academy, 2017 (8 min). 1) Fit a regression line Well now fit a regression line to our data. To print beautiful labels to this line, install the ggpubr package, which is a ggplot2 based package that creates publication ready plots that follow scientific conventions. # installing/loading the package: if(!require(ggpubr)) { install.packages(&quot;ggpubr&quot;); require(ggpubr) } #load / install+load ggpubr data %&gt;% ggplot(aes(x=negemot, y=govact)) + geom_count() + theme_bw() + xlim(0,7) + # This command is a shortcut for scale_x_continuous(limits=c(0,7)) ylim(0,7.5) + labs(x=&quot;Negative emotions about climate change&quot;, y=&quot;Support for government action&quot;) + geom_smooth(method=&#39;lm&#39;, formula= y~x, color = &quot;darkred&quot;) + # This command adds the regression line to the bubble plot stat_cor(aes(label = ..rr.label..)) + # This command adds the r-squared to the graph stat_regline_equation(label.y = 6.6) # Controls the placement of the regression equation Evaluation: As you can see, the equation is Y = 2.8 + 0.51*X. For a person with a negative emotions score of X = 2, the model predicts Y = 2.8 + 0.51*2 = 2.8 + 1.02 = 3.82. Let´s suppose this persons actual support for government action, Y, is 4.0. If so, then this persons residual would be e = 4.0 - 3.82 = 0.18. Thus, the model very slightly overestimates this persons support for government action by 0.18 units. Finally, R2 provides an estimation of how much of the variance in Y can be explained by X. In this case, 33% of the variance in the support for government action can be explained by negative emotions about climate change. 2) Fit a model You can also directly run the linear model in R: summary(model&lt;-lm(govact ~ negemot,data=data)) ## ## Call: ## lm(formula = govact ~ negemot, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.3285 -0.6731 0.1018 0.7554 3.2142 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.75732 0.09866 27.95 &lt;0.0000000000000002 *** ## negemot 0.51424 0.02548 20.18 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.111 on 813 degrees of freedom ## Multiple R-squared: 0.3338, Adjusted R-squared: 0.333 ## F-statistic: 407.3 on 1 and 813 DF, p-value: &lt; 0.00000000000000022 # lm -&gt; stands for linear model and tells R to fit an OLS regression # govact ~ negemot -&gt; tells are to predict govact by using the values of negemot # summary(model) -&gt; tells are to print a summary of the OLS regression results to the console Evaluation: Again, we can extract the parameters for the intercept (b[0] = 2.8), the regression coefficient (b[1] = 0.51), and R2: 0.33. 11.5 Standardizing coefficients The process of putting differently scaled variables on the same scale is called standardization. Thus, standardization allows you to compare results from differently scaled variables. For example, suppose you want to compare the effects of annual income (in $) and negative emotions about climate change (6-point scale) on support for government action (7-point scale). It is difficult to compare the effect sizes of these variables when they are scaled differently. For standardization, you remove the mean from each observed value of the variable and divide by its standard deviation. The resulting variable will have a mean of 0 and a standard deviation of 1. When all variables of a regression model are first standardized before the model is estimated, we speak of a standardized regression model. In a standardized regression model, all measurements are expressed in units of standard deviations from the sample mean and the intercept is always 0, i.e. the regression lines goes through the center of the coordinate system. A generic interpretation of a standardized simple linear regression would be that two individuals who differ by one standard deviation on X are estimated to differ by Xs regression coefficients standard deviations on Y. If you need a more extensive recap / explanation of variable standardization, check out this video by D. Caughlin, 2020 (start at 4 min). Lets standardize our variables before calculating the linear model: data$negemot_st &lt;- scale(data$negemot) # adds a new variable to the dataset that is the standardized version of negemot data$govact_st &lt;- scale(data$govact) # same for govact data %&gt;% ggplot(aes(x=negemot_st, y=govact_st)) + geom_count() + theme_bw() + xlim(-2,2.0) + ylim(-2,2.0) + labs(x=&quot;Standardized negative emotions about climate change&quot;, y=&quot;Standardized support for government action against climate change&quot;) + geom_smooth(method=&#39;lm&#39;, formula= y~x, color = &quot;darkred&quot;) summary(model&lt;-lm(govact_st ~ negemot_st,data=data)) # Finally, we run the actual model. ## ## Call: ## lm(formula = govact_st ~ negemot_st, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.1817 -0.4948 0.0748 0.5553 2.3626 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.000000000000005405 0.028608403540518926 0.00 1 ## negemot_st 0.577745819891247070 0.028625970876249084 20.18 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8167 on 813 degrees of freedom ## Multiple R-squared: 0.3338, Adjusted R-squared: 0.333 ## F-statistic: 407.3 on 1 and 813 DF, p-value: &lt; 0.00000000000000022 Evaluation: Now, the intercept goes through the center of the coordinate system (b[0] = 0.0). Two persons with one standard deviation difference in their negative sentiments about climate change (X) are expected to have b[1] = 0.57 standard deviations difference in their support for government action. The R2 remains untouched, of course: 0.33. 11.6 Multiple regression Multiple regression is a statistical extension of simple linear regression that allows to examine the association between multiple independent variables and a single dependent variable. Why not just run multiple simple regressions? Often the relationships between two variables X and Y are not absolutely clear. This is due to the fact that persons who vary in X are more likely to differ in Z, too. As a result, the changes in Y might also be caused by Z instead of X, or they might be caused by a combination of X AND Z. Multiple regression allows to extract the effect of X on Y while controlling for the effect of Z (i.e. partial effect), which is what makes multiple regression models more powerful than simple regression models. A typical multiple linear regression equation looks like this: 1. Y = i[Y] + b[1] * X + b[2] * Z where i[Y] is the intercept (often called b[0]),b[1] is the regression coefficient of X, and b[2] is the regression coefficient of Z. A generic interpretation of a multiple linear regression would be that two individuals who differ by one unit on X but are equal on Z are estimated to differ by Xs regression coefficient units on Y. For a dichotomous / binary X variable, this regression coefficient estimates the difference between both group means while holding Z constant (because a one unit increase means changing groups, X = 1 and X = 0). In the given example, Z is called a covariate because it is the variable that is controlled for. But X can also be called a covariate of Z, provided Z is the variable of main interest. All in all, multiple regression allows to rule out alternative explanations for an observed relationship between two variables by controlling for other variables that might also impact the dependent variable. To run our own multiple regression model, we will predict support for government action against climate change by negative emotions about climate change, while controlling for age and political ideology (scale from 1 Very Liberal to 7 Very Conservative). summary(lm(govact ~ negemot + age + ideology ,data=data)) ## ## Call: ## lm(formula = govact ~ negemot + age + ideology, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.8789 -0.6843 0.0749 0.6967 3.4815 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.994172 0.192375 20.762 &lt; 0.0000000000000002 *** ## negemot 0.437918 0.026103 16.777 &lt; 0.0000000000000002 *** ## age -0.001461 0.002343 -0.623 0.533 ## ideology -0.218672 0.026963 -8.110 0.00000000000000186 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.067 on 811 degrees of freedom ## Multiple R-squared: 0.3876, Adjusted R-squared: 0.3853 ## F-statistic: 171.1 on 3 and 811 DF, p-value: &lt; 0.00000000000000022 Evaluation: Compared to our simple model, the R2 has increased, namely from 0.33 to 0.38. This is great news because it means that we can explain more of the variance of Y with the newly introduced covariates. The equation for this model is Y = 3.99 + 0.43*X - 0.00*Z1 - 0.21*Z2. Accordingly, the intercept is at 3.99 (b[0] = 3.99) and the regression parameters are b[1] = 0.43, p &lt; 0.001, b[2] = -0.00, p = 0.533 and b[3] = -0.21, p &lt; 0.001. Obviously, all variables have a significant effect on Y when controlling for the other variables, except age. The intercept can be interpreted as the value of Y when all other variables in the equation are equal to 0. That is, we would assume that the support for government action is 3.99 when a respondent has a negative emotion about climate change that equals 0 (which is not possible, since the lowest value on the scale is 1), is 0 years old (not possible for obvious reasons) and has an ideology of 0 (also not possible on a scale from 1 to 7). Because the interpretation of the intercept in multiple regression models is often nonsensical, researchers focus primarily on the interpretation of the regression coefficients. The regression coefficients are interpreted as the difference between two people who are equal with regard to all variables, but differ in one unit on the variable of interest. For example, two persons of the same age and ideology who disagree regarding their negative emotions about climate change by one point differ by an estimated 0.43 point in their support for government action. You can enter these numbers into the regression equation. Lets do that and compare two 20 year-olds who are both political moderates, but differ in their emotions by one point: Y = 3.99 + 0.43*2 - 0.00*20 - 0.21*4 = 3.99 + 0.86 - 0 - 0.84 = 4.01 Y = 3.99 + 0.43*3 - 0.00*20 - 0.21*4 = 3.99 + 1.29 - 0 - 0.84 = 4.44 As you can see, both differ by 0.43 points! 11.7 Standardized multiple regression If you standardize the variables before including them in the model, you make the effect sizes of the variables comparable to each other. However, this also changes the interpretation of the model, i.e. how the change of the variable of interest by one unit is interpreted. After standardization, the regression coefficients are interpreted as the difference between two people who are equal with regard to all variables, but differ in one standard deviation on the variable of interest. If X and Y are both standardized and X changes by one standard deviation while all other variables remain constant, then Y also changes by one standard deviation (even if other variables in the model were not standardized). A generic interpretation of a standardized multiple regression would be that two individuals who differ by one standard deviation on X but are equal on Z are estimated to differ by Xs regression coefficients standard deviations on Y. Lets run the standardized model and try to interpret it. data$negemot_st &lt;- scale(data$negemot) data$age_st &lt;- scale(data$age) data$ideology_st &lt;- scale(data$ideology) data$govact_st &lt;- scale(data$govact) summary(lm(govact_st ~ negemot_st + age_st + ideology_st,data=data)) # Finally, we run the actual model. ## ## Call: ## lm(formula = govact_st ~ negemot_st + age_st + ideology_st, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.5863 -0.5030 0.0550 0.5121 2.5591 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.000000000000004232 0.027462514788610635 0.000 1.000 ## negemot_st 0.491995051947269890 0.029325815779064967 16.777 &lt; 0.0000000000000002 *** ## age_st -0.017533280621912107 0.028126011198986705 -0.623 0.533 ## ideology_st -0.242994585930391804 0.029961998428491336 -8.110 0.00000000000000186 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.784 on 811 degrees of freedom ## Multiple R-squared: 0.3876, Adjusted R-squared: 0.3853 ## F-statistic: 171.1 on 3 and 811 DF, p-value: &lt; 0.00000000000000022 Evaluation: As you can see, the intercept goes through the center of the coordinate system (b[0] = 0.0) again. b[1] = 0.49 implies that two persons of the same age and ideology who disagree regarding their negative emotions about climate change by one standard deviation are estimated to differ 0.49 standard deviations in their support for government action. Since a one standard deviation change in negative emotions produces the largest standard deviation change in support for government action, we can conclude that negative emotions are the best predictor of support for government action(at least with respect to the variables that we selected for the model). Tip for advanced students: Always be careful not to standardize dichotomous variables. Since calculating standard deviations for dichotomous variables is nonsensical (what is the standard deviation of a variable that can only be 0 or 1, but nothing in between?), interpreting standardized dichotomous variables is illogical and should be avoided. Include the unstandardized version of these variables in your models. 11.8 Take Aways visualize linear trends: always visualize the relationship between your variables before data analysis quantify associations: use Pearsons r to quantify linear associations linear regression: predicts an outcome / dependent variable Y by the values of an independent variable X standardization: allows you to compare results from differently scaled variables by binding all variables to a mean of 0 and a standard deviation of 1 multiple regression: predicts an outcome / dependent variable Y by the values of an independent variable X while controlling for other covariates 11.9 More tutorials on this You still have questions? The following tutorials &amp; books can help you with that: Hayes, A. F. (2022). Introduction to Mediation, Moderation and Conditional Process Analysis (p. 29-76) Link Learning statistics with R: A tutorial for psychology students and other beginners by D.J. Navarro, Tutorial 15 Now lets see what youve learned so far: Exercise 4: Test your knowledge. "],["exercise-4-test-your-knowledge.html", " 12 Exercise 4: Test your knowledge 12.1 Task 1 12.2 Task 2 12.3 Task 3 12.4 Task 4 12.5 Task 5 12.6 Task 6", " 12 Exercise 4: Test your knowledge After working through Exercise 4, youll have practiced to run simple and multiple linear regression models in R know how to use coefficient standardization to compare effect sizes know how to interpret the results of these models 12.1 Task 1 Lets use the data set glbwarm again, which you should know well by now. Install / activate the processR package and assign the glbwarm data to a source object. # installing/loading the package: if(!require(processR)) { install.packages(&quot;processR&quot;); require(processR) } #load / install+load processR data &lt;- processR::glbwarm In this task, we want to tackle simple linear regression. More specifically, we want to predict the ideology of our respondents by their age because we assume that older respondents will hold more conservative viewpoints. The higher the values of the ideology variable, the more conservative the respondents are (coded from 1 very liberal to 7 very conservative). Research question: Do older U.S. Americans hold more conservative viewpoints than younger U.S. Americans? To answer this question, prepare a visual inspection of this relationship without fitting a regression line. Can you recognize a relationship? What is its nature? 12.2 Task 2 Next, try to quantify the association using Pearsons r. Interpret the result. 12.3 Task 3 Using your graph from Task 1, fit a regression line to your data points (Hint: You will need to load the ggpubr package). Interpret the parameters of the regression line. 12.4 Task 4 Run a linear model in R using the ideology and age variables. Interpret the results. 12.5 Task 5 Since age alone does not seem to be a good predictor of conservatism, we want to introduce other predictors into the model and run a Multiple Linear Regression. This means that we will predict the effect of age on conservatism while controlling for the effect of third variables. For example, the respondents gender (sex, 0 = female, 1 = male) and their party preference (partyid, 1 = Democrat, 2 = Independent, 3 = Republican) might be great predictors of conservatism. Note: In a linear regression model, we can only include metric variables and variables that are binary coded (0/1). However, partyid is a categorical, i.e. factor variable, since Democrats are coded 1, Independents 2, and Republicans 3. Therefore, you need to mutate partyid and create two new binary variables democrat (0/1) and republican (0/1), where 1 indicates that the respondent identifies with that political party. (You dont need to create a variable independent, since that information would be redundant: someone who has a value of 0 for both republican AND democrat MUST be an independent, so you can derive party preference with just two variables). Then, run a multiple linear model that predicts ideology by sex, democrat, republican, and age. Interpret the results and the meaning of the age coefficient. 12.6 Task 6 Standardize all relevant variables and run the model again (note that binary variables shouldnt be standardized). How does the interpretation of the age coefficient change? "],["tutorial-mediation-analysis.html", " 13 Tutorial: Mediation analysis 13.1 Introduction to mediation 13.2 Difference between mediation and moderation 13.3 Two-step process of mediation 13.4 Statistical representation and equations 13.5 Example 13.6 Take Aways 13.7 More tutorials on this", " 13 Tutorial: Mediation analysis After working through Tutorial 13, youll know the difference between linear regression, mediation analysis, and moderation analysis have a good idea about what problems are best solved with mediation analysis know how to perform and interpret a mediation analysis 13.1 Introduction to mediation With linear regression we have already learned a statistical method that quantifies the relationship between variables. With simple linear regression, we can answer the question whether two variables share a linear relationship. With multiple linear regression, we can answer the question whether two variables share a linear relationship while controlling for the effect of third variables. However, we still have limited benefit from knowing that two variables are associated, i.e., from asking whether two variables share a relationship. Instead, it is much more conclusive to ask why and how two variables are related: What is the mechanism that triggers/transports the effect of the independent variable X on the dependent variable Y? The objective of mediation analysis is to answer this question of how? / whats the mechanism that drives this relationship? A common workflow is to (1) run a linear regression and see whether there is a relationship, and (2) then run a mediation analysis to examine the nature of that relationship. Note: The implementation of a mediation analysis in your workflow should always be based on a careful theoretical argumentation! That is, you should be able to name a theory that provides a plausible mechanism by which X affects Y. 13.2 Difference between mediation and moderation Whats the difference between mediation and moderation analysis? While mediation analysis answers the question of how / through which (psychological) mechanisms effects occur, moderation analysis answers the question of when / for whom effects occur. Circumstances and certain personality traits can change the effect of X on Y  and thats what moderation analysis is about. On the other hand, mediation analysis is a causal explanation. In a mediation model, relationships are thought to be causal, and the mediator variable M is assumed to lie on a causal path between X and Y. In a very simplified way, mediation analysis describes the emotional, cognitive, or behavioral responses (the dependent variable Y) following some kind of psychological processing (mediator variable M, state) that was triggered by a certain event (the independent variable X). Moderation analysis, on the other hand, describes how an event (the independent variable X) triggers group-specific emotional, cognitive, or behavioral responses (the dependent variable Y) due to varying circumstances/personality traits that are dominant in these subgroups of the population (moderator variable W, trait). Always remember that a strong moderator variable can turn a positive predictor variable into a negative predictor variable, depending on the observed subgroups (and vice versa)!6 Example for mediation: Let´s take an example from selective exposure theory: According to selective exposure theory, reading opinion-inconsistent information (independent variable X) can trigger a psychological process called cognitive dissonance (mediator variable M). Cognitive dissonance is a psychological state that is very unpleasant and stressful, which is why individuals start to avoid opinion-inconsistent messages (dependent variable Y) to reduce cognitive dissonance. 13.3 Two-step process of mediation Mediation analysis always assumes a two-step process. In the first step, an independent variable X influences a mediator variable M. In the second step, this mediator variable M then influences the dependent variable Y. However, in addition to the two-step effect mediated by M, which we call indirect effect, there will almost always be a direct effect of X on Y as well. To illustrate, see this conceptual diagram of a simple mediation model: Image: A simple mediation model (Source: Andrew F. Hayes, p. 81) Andrew F. Hayes explains this graphic representation of a mediation model in great detail, finding simple words, which is why I quote his explanation here: In such a model, there are two pathways by which X can inuence Y. These pathways are found by tracing the ways one can get from X to Y while never tracing in a direction opposite to the direction an arrow points. One pathway leads from X to Y without passing through M and is called the direct effect of X on Y. The second pathway from X to Y is the indirect effect of X on Y through M. It first passes from antecedent X to consequent M and then from antecedent M to consequent Y. The indirect effect represents how Y is influenced by X through a causal sequence in which X inuences M, which in turn influences Y. (Hayes, 2022, p. 80-81) The remaining direct effect of X on Y may come from other mediator variables that have not yet been considered and included in your model. Thus, the direct effect of X on Y becomes smaller as we include more mediator variables in our mediation model. In summary, a simple mediation model contains two dependent variables (M) and (Y) and two independent variables (X) and (M), with X causally inuencing Y and M, and M causally inuencing Y. 13.4 Statistical representation and equations Since we have two dependent variables, M and Y, we also need two equations to make predictions about these two variables. The first equation describes the effect of X on M and the second equation describes the effect of X AND M on Y. This means that the first equation represents a simple linear regression model and the second equation represents a multiple linear regression model. Sounds familiar, right? :) M = i[M] + a * X Y = i[Y] + c * X + b * M, where i[M] and i[Y] represent the intercepts and a, b, and c are the respective regression coefficients. Image: A simple mediation model (Source: Andrew F. Hayes, p. 85) c: measures the direct effect of X on Y while controlling for M ab: measures the indirect effect of X on Y and is the product of (1) the direct effect of X on M (a) and (2) the direct effect of M on Y while controlling for X (b) 13.4.1 Direct effect c Since equation 2. represents a multiple linear regression model, you already know how to interpret the direct effect (try to remember the tutorial on Multiple regression!): The direct effect can be interpreted like this: Two individuals who differ by one unit on X but are equal on M (i.e., while controlling for M!) are estimated to differ by c units on Y. For a dichotomous / binary X variable, c estimates the difference between both group means while holding M constant. 13.4.2 Indirect effect ab To disassemble the indirect effect ab, lets first discuss what the coefficients a and b represent. As you know from the simple linear regression analysis tutorial about OLS regression, a estimates the difference in M between two individuals who differ by one unit on X. And following the Multiple regression tutorial, you know that b estimates the difference in Y between two individuals who differ by one unit on M while controlling for X. Mathematically, the indirect effect of X on Y through M is the product of a and b. Therefore, we call it the indirect effect ab. Lets say that a = 0.5 and b = 0.8, then ab = 0.5 * 0.8 = 0.4. The indirect effect can be interpreted like this: Two individuals who differ by one unit in X are estimated to differ by ab units on Y because of the effect of X on M, which in turn affects Y. Since the indirect effect ab is a product of two separate effects a and b, the mathematical rules of multiplication apply to this product. This means that the indirect effect ab turns positive if both a and b are negative effects (remember: two minus signs cancel each other out in multiplication!). For example, if watching cigarette advertising on TV makes you develop a less healthy life style (i.e., a is negative) and having a healthy life style makes you less prone to become a smoker (i.e., b is negative), then the indirect effect ab of watching cigarette advertising on TV makes you more likely to become a smoker (i.e., ab is positive). Put differently, if cigarette advertising makes you less likely to live a healthy life, but living an unhealthy life makes you more likely to smoke, then cigarette advertising has a positive effect on smoking. Tip for advanced students: Whenever you have a mediation hypothesis that predicts a positive relationship ab between X and Y, add a specification about the signs of a and b to your hypothesis! There are different constellations that can lead to a positive ab, and these constellations have very different implications for the validity of your theory. For example, a good hypothesis should read We assume that watching cigarette advertising on TV increases the likelihood of becoming a smoker. This effect is mediated by life style, i.e., we assume that watching cigarette advertising on TV makes respondents develop less healthy life style habits and that healthy life style habits in turn make respondents less likely to start with smoking. 13.4.3 Total effect The total effect of X on Y is denoted as c. It is the sum of the direct effect of X on Y plus the indirect effect of X on Y that is being mediated by M (for dichotomous / binary variables, c is the difference in gourp means): c = c + ab. The total effect can be interpreted like this: Two individuals who differ by one unit in X are estimated to differ by c units on Y. 13.5 Example Lets look at an actual example and interpret the results. We will work with the glbwarm data set again. Install / load the processR package and assign the glbwarm data to a source object. # installing/loading the package: if(!require(processR)) { install.packages(&quot;processR&quot;); require(processR) } #load / install+load processR data &lt;- processR::glbwarm 13.5.1 Knowing your data You´ll find three variables in this data set that will be of great interest to us because they are great to learn mediation analysis with. You already know two of these variables from our OLS regression tutorial: govact (= support for government action): The variable is an index of how each participant answered five questions regarding how supportive he or she is of different policies or activities taken by the US government to address the threat of global climate change (e.g. How much do you support or oppose increasing government investment for developing alternative energy like biofuels, wind, or solar by 25%?). Response options were scaled from Strongly opposed (coded 1) or Strongly support (7). negemot (= negative emotions about climate change): This index measures participants´ negative emotional reactions to the potential of climate change. Participants´ responses to a question asking how often they experience each of three emotions while thinking about global warming: worried, alarmed, and concerned. Not at all, somewhat, a little bit, some, a fair lot, and a great deal were among the response alternatives. These replies were numerically coded from 1 to 6, and the average of each participant´s responses across all three emotions was calculated. Higher scores reect feeling stronger negative emotions. ideology (= conservatism): This variable measures the political ideology of respondents on a liberalism-conservatism scale. Replies range from 1 to 7. Higher scores reect being more conservative, i.e. 1 Very Liberal, 2 Liberal, 3 Somewhat Liberal, 4 Moderate, Middle of the Road, 5 Somewhat Conservative, 6 Conservative, and 7 Very Conservative. Throughout this chapter, the following research question will guide us on our way to master mediation analysis: Research Question: Is the relationship between conservatism and support for government action against climate change mediated by negative emotions about climate change? That is, do people who are more conservative develop fewer negative feelings about climate change, while negative emotions about climate change increase support for government action against climate change? 13.5.2 Visual inspection of linear trends You will surely remember that it is always a good idea to look at the relationships between our variables before we do any kind of analysis. So lets start by examining the variables of interest. The relationship between conservatism and support for government action against climate change (X on Y without controlling for M) data %&gt;% ggplot(aes(x=ideology, y=govact)) + geom_count() + theme_bw() + labs(x=&quot;Conservatism&quot;, y=&quot;Support for government action against climate change&quot;) Evaluation: It seems that people who are more conservative are less supportive of government action against climate change. This correlation is far from perfect, but the pattern is evident. The relationship between conservatism and negative emotions about climate change (direct effect a of X on M) data %&gt;% ggplot(aes(x=ideology, y=negemot)) + geom_count() + theme_bw() + labs(x=&quot;Conservatism&quot;, y=&quot;Negative emotions about climate change&quot;) Evaluation: This relationship is difficult to capture visually. There seems to be a small effect at play here, with more conservative people expressing fewer negative feelings about climate change. This pattern is not very evident, but you can imagine a diagonal from the top left corner to the bottom right corner through the thicker bubbles. The relationship between negative emotions about climate change and support for government action against climate change (M on Y without controlling for X) data %&gt;% ggplot(aes(x=negemot, y=govact)) + geom_count() + theme_bw() + labs(x=&quot;Negative emotions about climate change&quot;, y=&quot;Support for government action against climate change&quot;) Evaluation: We can assume that people who express larger negative sentiments about climate change are also more supportive of government action. Again, this pattern is evident. 13.5.3 Pearsons r Now quantify these association to check whether they are significant. The relationship between conservatism and support for government action against climate change (X on Y without controlling for M) cor.test(data$govact,data$ideology, method=&quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: data$govact and data$ideology ## t = -13.132, df = 813, p-value &lt; 0.00000000000000022 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.4733936 -0.3599884 ## sample estimates: ## cor ## -0.4183199 Evaluation: The association between conservatism and the support for government action is medium and negative (r = -0.41, p &lt; 0.001). We can conclude that U.S. Americans who are more conservative are less likely to support government action against climate change. The relationship between conservatism and negative emotions about climate change (direct effect a of X on M) cor.test(data$negemot,data$ideology, method=&quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: data$negemot and data$ideology ## t = -10.611, df = 813, p-value &lt; 0.00000000000000022 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.4076942 -0.2869874 ## sample estimates: ## cor ## -0.3487864 Evaluation: The association between conservatism and negative emotions about climate change is medium and negative (r = -0.34, p &lt; 0.001). We can conclude that U.S. Americans who are more conservative are less likely to express negative sentiments about climate change. The relationship between negative emotions about climate change and support for government action against climate change (M on Y without controlling for X) cor.test(data$negemot,data$govact, method=&quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: data$negemot and data$govact ## t = 20.183, df = 813, p-value &lt; 0.00000000000000022 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.5301050 0.6217505 ## sample estimates: ## cor ## 0.5777458 Evaluation: The association between negative emotions about climate change and the support for government action is medium and positive (r = 0.57, p &lt; 0.001). We can conclude that U.S. Americans who have more negative feelings about climate change are more likely to support government action against climate change. 13.5.4 Fit models Now that we know our relationships a bit better, we can fit our models to investigate the actual mediation. Because we need to retrieve two regression equations to explain the two dependent variables M and Y (see Statistical representation and equations), we now will fit and interpret two models, namely a single linear regression model and a multiple linear regression model. The simple linear regression model: The effect of conservatism on negative emotions about climate change (direct effect a of X on M) summary(lm(negemot ~ ideology, data=data)) ## ## Call: ## lm(formula = negemot ~ ideology, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.645 -1.212 0.060 1.083 3.470 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.99794 0.14469 34.54 &lt;0.0000000000000002 *** ## ideology -0.35263 0.03323 -10.61 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.433 on 813 degrees of freedom ## Multiple R-squared: 0.1217, Adjusted R-squared: 0.1206 ## F-statistic: 112.6 on 1 and 813 DF, p-value: &lt; 0.00000000000000022 Evaluation: You can now infer the equation, which is M = 4.99 - 0.35*X. Two U.S. Americans who differ by one point in conservatism are estimated to differ by -0.35 (p &lt; 0.001) points in their negative feelings about climate change. This means that our direct effect a (X on M) = -0.35. Note that the results from the Pearsons r correlation test and the OLS regression are not identical since the OLS regression has not been standardized and Pearsons r is a standardized measure of association. Essentially, Pearsons r is just the standardized slope of a simple linear regression line. If you had run a standardized linear regression, youd retrieve the Pearsons r coefficient: data$ideology_st &lt;- scale(data$ideology) data$negemot_st &lt;- scale(data$negemot) summary(lm(negemot_st ~ ideology_st, data=data)) ## ## Call: ## lm(formula = negemot_st ~ ideology_st, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.38500 -0.79306 0.03923 0.70830 2.27062 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.000000000000008831 0.032848946119645242 0.00 1 ## ideology_st -0.348786426090197332 0.032869117411763973 -10.61 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9378 on 813 degrees of freedom ## Multiple R-squared: 0.1217, Adjusted R-squared: 0.1206 ## F-statistic: 112.6 on 1 and 813 DF, p-value: &lt; 0.00000000000000022 The multiple linear regression model: The effect of conservatism AND negative emotions about climate change on support for government action against climate change (direct effect c of X on Y while controlling for M AND direct effect b of M on Y while controlling for X) summary(lm(govact ~ ideology + negemot, data=data)) ## ## Call: ## lm(formula = govact ~ ideology + negemot, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.9030 -0.6754 0.0788 0.6996 3.5205 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.93703 0.16907 23.286 &lt;0.0000000000000002 *** ## ideology -0.22213 0.02638 -8.422 &lt;0.0000000000000002 *** ## negemot 0.43761 0.02609 16.774 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.066 on 812 degrees of freedom ## Multiple R-squared: 0.3873, Adjusted R-squared: 0.3858 ## F-statistic: 256.6 on 2 and 812 DF, p-value: &lt; 0.00000000000000022 Evaluation: You can now infer the equation, which is Y = 3.93 - 0.22 * X + 0.43 * M. Two U.S. Americans who differ by one point in conservatism, but rate equally high on negative emotions about climate change, are estimated to differ by - 0.22 (p &lt; 0.001) points in their support for government action against climate change. Similarly, two U.S. Americans who differ by one point in their negative emotions about climate change, but are both equally conservative, are estimated to differ by + 0.43 (p &lt; 0.001) points in their support for government action against climate change. This means that our direct effect b (M on Y while controlling for X) = 0.43 and that our direct effect c´ (X on Y while controlling for M) = -0.22. Calculate the indirect effect ab Now that we know the direct effects a and b, it is easy to calculate ab. We have a negative direct effect a = -0.35 of conservatism on negative emotions. In addition, we have a positive direct effect b = +0.43 of negative emotions about climate change on support government action against climate change, while controlling for conservatism. Since we know that the indirect effect ab is a product of the direct effects a and b, we can conclude that the indirect effect ab of conservatism on support for government action against climate change will be negative ( + * - = -). Lets calculate the indirect effect: print(ab &lt;- -0.35*0.43) ## [1] -0.1505 Evaluation: Interpreting the indirect effect, we now know that two U.S. Americans who differ by one point on the conservatism scale are estimated to differ by -0.15 points in their support for government action against climate change because of the effect of conservatism on negative emotions, which in turn affects the support for government action. Calculate the total effect c The total effect c can be estimated via a third simple regression analysis or by adding the direct and indirect effects from our two previous models. Lets first try simple addition: print(c &lt;- -0.22 + ab) ## [1] -0.3705 And now lets run a simple regression analysis that predicts the influence of conservatism (X) on support for government action (Y) without controlling for negative emotions (M): summary(lm(govact ~ ideology, data=data)) ## ## Call: ## lm(formula = govact ~ ideology, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.7477 -0.7595 0.0287 0.8051 3.3109 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.12419 0.12481 49.07 &lt;0.0000000000000002 *** ## ideology -0.37645 0.02867 -13.13 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.236 on 813 degrees of freedom ## Multiple R-squared: 0.175, Adjusted R-squared: 0.174 ## F-statistic: 172.4 on 1 and 813 DF, p-value: &lt; 0.00000000000000022 As you can see, the total effect of X on Y is just the effect of a linear regression that does not account for mediator variables. You can interpret the total effect like every other result from a simple regression analysis, i.e.: Two U.S. Americans who differ by one point in conservatism are estimated to differ by -0.37 points in their support for government action. Summary What have we learned? You now know that you can decompose an overall effect of a simple linear regression analysis into two effects: one direct and one indirect via mediator variables. These mediator variables explain how the effect of X on Y comes about, for example, by eliciting negative emotions about climate change. In our example, we know that conservatism has a negative total effect on support for government action against climate change (r[c] = -0.37, p &lt; 0.001). Part of this effect acts indirectly by decreasing negative emotions about climate change (r[a] = -0.35, p &lt; 0.001), which in turn increase support for government action against climate change (r[b] = 0.43, p &lt; 0.001). However, this indirect effect of negative emotions seems to be small (r[ab] = -0.15) and the effect of conservatism also acts directly (r[c] = -0.22, p &lt; 0.001) on support for government action. Maybe other mediators can explain this remaining direct effect? Thats not part of this tutorial. ;) Tip for advanced students: To compare effect sizes, e.g. to compare the size of the direct effect vs. the indirect effect, standardize your variables and run a standardized mediation analysis (but be careful not to standardize dichotomous / binary variables). We can summarize our results in a graphic representation of our model: Image: A simple mediation model with coefficients (Source: Adapted from Andrew F. Hayes, p. 90) 13.5.5 Using processR You might think that was quite a time-consuming process just to calculate a few coefficients. In this case you are lucky: Andrew F. Hayes has really accelerated the calculation of mediation models with his package processR! However, you will to need manually install the process function. Please visit this link, download the .zip file at the end of the website, unzip it, open up the R-folder, and open up the process script in RStudio. Run the code, this will take a few minutes and create a new function for R, which is called process. You must run this script every time after restarting R if you want to use the process function. Once youve created the process function, perform the mediation analysis again, but this time we will use process: The output of process is pretty much self-explanatory when youve run the mediation model manually before. The first part gives you information about the model that youve run (model 4 = simple mediation) and your variables (X, Y, and M). The second part gives you information about the direct effect of you independent variable X on your mediator variable M (simple linear regression). You can infer the a coefficient from this output, i.e.: a = -0.35. In addition, you access the R2, which tells you how much of the variance of your mediator variable M can be explained by the variance of you independent variable X. Here it is R2 = 0.34, i.e. 34%. The third part gives you information about the direct effect of your independent variable X on your dependent variable Y while controlling for M AND the direct effect of your mediator variable M on your dependent variable Y while controlling for X (multiple linear regression). You can infer the c and b coefficient from this output, i.e.: c = -0.22 and b = 0.43. In addition, you access the R2 = 0.62, i.e. 62% of the variance in support for government action can be explained by conservatism and negative emotions about climate change. The TOTAL EFFECT MODEL part gives you information about the total effect c of X on Y without controlling for M (simple linear regression), i.e. c = -0.37. This effect is the sum of the direct effect c plus the indirect effect ab (c = c + ab). The TOTAL, DIRECT, AND INDIRECT EFFECTS OF X ON Y part gives you information about whether the direct, indirect, and total effects are significant (p-value). You can also extract information about the size of the indirect effect ab under the section Normal theory test for indirect effect(s): and look up whether the indirect effect is significant (p-value). Finally, you can draw a conceptual diagram of your simple mediation model: labels=list(X=&quot;ideology&quot;,M=&quot;negemot&quot;,Y=&quot;govact&quot;) processR::drawModel(labels=labels) In addition, you can also draw a diagram of your estimated simple mediation model, but you need to install/activate the lavaan package first: # installing/loading the package: if(!require(lavaan)) { install.packages(&quot;lavaan&quot;); require(lavaan) } #load / install+load lavaan You dont need to understand this code yet, you just need to insert your variables in the future and use it: labels &lt;- list(X=&quot;ideology&quot;,M=&quot;negemot&quot;,Y=&quot;govact&quot;) model &lt;- tripleEquation(X=&quot;ideology&quot;,M=&quot;negemot&quot;,Y=&quot;govact&quot;) # retrieve the regression equations and save them into model semfit &lt;- sem(model=model,data=data) # use the retrieved equations to fit a lavaan model statisticalDiagram(4,labels=labels,fit=semfit,whatLabel=&quot;est&quot;) # 4 tells R to draw a simple mediation model with the lavaan fit, whatLabel=&quot;est&quot; sets your estimated coefficients as labels 13.6 Take Aways causal claims: a mediation model gives causal explanations and the mediator variable M is assumed to lie on a causal path between X and Y direct and indirect effect: a mediation model divides the total effect into a direct and an indirect effect process: calculate the mediation model either by using the classic lm() (simple and multiple linear regression) or by using the process function of the process script by A. F. Hayes standardization: run standardized mediation models to compare effect sizes 13.7 More tutorials on this You still have questions? The following book can help you with that: Hayes, A. F. (2022). Introduction to Mediation, Moderation and Conditional Process Analysis (p. 79-117) Link Thats why moderator variables can be absolutely critical, especially in clinical medical research! Imagine that a drug shows a strong positive, curative effect in one group of patients (e.g. men) and a small negative, toxic effect in another group (e.g. women). When prescribing this drug, it is essential that doctors pay attention to the moderator variable sex! However, a multiple linear regression may have overlooked this moderating effect of sex, finding only a (small) positive effect of drug use in the overall population, e.g., if your sample includes more men than women. "],["tutorial-moderation-analysis.html", " 14 Tutorial: Moderation analysis 14.1 Introduction to moderation 14.2 One-step process of moderation 14.3 Statistical representation and equations 14.4 Example 14.5 Take Aways 14.6 More tutorials on this", " 14 Tutorial: Moderation analysis After working through Tutorial 15, youll have a good idea about what problems are best solved with moderation analysis know how to probe an interaction, i.e. perform and interpret a moderation analysis know how to visualize interaction effects 14.1 Introduction to moderation Most of you will have had contact with moderation analysis before. Often, students are first exposed to moderation analysis / interactions in Statistics I when they learn how to perform an ANOVA (ANalysis Of VAriance) and how to probe an interaction, i.e., test whether the effect of an independent variable on the dependent variable differs at different levels of a second independent variable. ANOVAs are often used in experimental studies, for example, to test the effect of three different ad placements in newspapers (front page, middle section, ad page) on readers recall. You might probe an interaction in this experiment by adding another independent variable that influences the relationship between ad placement and recall, e.g. color choice (black/white, weakly saturated colors, strongly saturated colors). But even if you havent had any exposure to moderation analysis and experimental designs before, you should become familiar with the procedure very quickly with the following tutorial. In this tutorial, we will continue to use linear regression techniques because ANOVA is basically nothing more than a linear regression with an independent variable that is a categorical/factor variable. Before we start, recall again what linear regression is all about: Linear regression allows you to quantify the relationship between variables and answer the question whether two (or more) variables share a linear relationship. As mentioned in earlier chapters, it is of limited use to know that two variables are related, i.e., to ask whether two variables share a relationship. Just like asking why and how two variables are related (mediation analysis), asking when and under what circumstances two variables are related (moderation analysis) can be much more informative. Therefore, moderation analysis asks whether the effect size of this linear relationship depends on third variables, e.g., events or personality traits. The variable W is called a moderator of the relationship between a focal independent variable X and a dependent variable Y, when the size, sign, or strength of the effect of X on Y is dependent on the size of W. This effect of a moderating variable on the relationship between X and Y is also called interaction effect. Finding a moderator / interaction for an effect can help you to determine the situations, events, or groups of people for which the impact of X on Y is significant or non-significant (i.e. absent vs. present), small or strong, and even positive or negative. Example for moderation: Knowing moderator variables can be critical, especially in clinical medical research. Imagine that a drug shows a strong positive, curative effect in one group of patients (e.g., men) and a small negative, toxic effect in another group (e.g., women). When prescribing this drug, it is essential that doctors pay attention to the moderator variable sex! However, a multiple linear regression may have overlooked this moderating effect of sex, finding only a (small) positive effect of drug use in the overall population, e.g., if your sample includes more men than women. Therefore, searching for moderator variables is an important step for researchers. 14.2 One-step process of moderation Unlike mediation analysis, moderation analysis assumes a one-step process: The moderator W influences the relationship between X and Y directly. To see an illustration of how this relationship is influenced by W, see this conceptual diagram of a simple moderation process7: Image: A simple mediation model (Source: Andrew F. Hayes, p. 235) In summary, a simple moderation model contains a dependent variable (Y), a focal independent variable (X), and a moderator variable (W), where the relationship (e.g., effect size, effect direction, effect significance) between X and Y depends on the magnitude of W. 14.3 Statistical representation and equations When the effect of X is dependent on W, this means that for different values of W, Xs effect on Y is different. Statistically, we can rewrite the values of X, that depend on W, as a multiplication of X and W with an own coefficient (e.g., b * XW). Of course, we want to control for the pure effect of X on Y and for the pure effect of W on Y at the same time, which leads to this statistical representation of moderation effects that looks like this: Y = i[Y] + b1 * X + b2 * W + b3 * XW, where i[Y] represent the intercept, b1, b2, and b3 are the respective regression coefficients, and XW is a variable constructed as the product of X and W. Image: Statistical diagram of a simple moderation model (Source: Andrew F. Hayes, p. 241) Interpreting b3, the interaction effect: Since the product XW will always look the same, no matter whether you consider X as the moderator or W as the moderator, b3 can have two different meanings, depending on your moderator choice (X or W). when W is considered to be the moderator of Xs influence on Y, b3 quantifies how the effect of X on Y changes as W changes by one unit. However, if X is thought of as the moderator of Ws impact on Y, then b3 predicts how the effect of W on Y changes as X changes by one unit. The mathematics are unconcerned with the variable you consider to be the moderator in your study, thats the concern of your theory. So depending on your theoretical assumptions, both interpretations can be accurate. A generic interpretation of the interaction effect in a moderation analysis would be that two individuals who differ by one unit on X are estimated to differ by XWs regression coefficient units on Y as W changes by one unit. Interpreting b1 and b2, the conditional effects: You might have already noticed that when W = 0, then most parts of the above equation can be deleted (because multiplications with 0 result in 0). Therefore, we refer to b1 as the conditional effect of X on Y. This means that b1 quantifies how much two cases that differ by one unit on X, but are W = 0, are estimated to differ on Y. The conditional effect quantifies the association between X and Y conditioned on W = 0. Please, do not interpret b1 as the relationship between X and Y controlling for W on average or controlling for W and XW, which are common phrases in scientific papers but are incorrect. In addition, it is also not correct to call the conditional effect the main effect of X, which is a term that is often used in ANOVA and you might already be faimiliar with it, but it is incorrect. Like b1, b2 is a conditional effect and it quantifies how much two cases that differ by one unit on W are estimated to differ on Y conditioned on X = 0. A generic interpretation of the conditional effect in a moderation analysis would be that two individuals who differ by one unit on X are estimated to differ by Xs regression coefficient units on Y when W = 0. Comparing these interpretations with multiple linear regression: Its worth noting that the interpretations of b1 and b2 in moderation analysis differ significantly from the the interpretation of the b1 and b2 coefficients that you obtain when you run a multiple linear regression, i.e. when XW is not included as an antecedent variable and the regression equation looks like this: Y = i[Y] + b1 * X + b2 * W. In a multiple linear regression, b1 and b2 quantify how much two cases that differ by one unit on X are estimated to differ on Y holding W constant (b1) and how much two cases that differ by one unit on W are estimated to differ on Y holding X constant (b2). The meaning of conditional effects (conditioned on W = 0) and unconditional effects (controlling for W, i.e. holding W constant) are completely different. Please, try to not confuse the concept of conditioning with controlling. Interpretation of controlling, i.e. multiple linear regression: Two individuals who differ by one unit on X but are equal on W are estimated to differ by Xs regression coefficient units on Y. Interpretation of conditioning, i.e. moderation analysis: Two individuals who differ by one unit on X but have a zero score in W (W = 0) are estimated to differ by Xs regression coefficient units on Y. Tip for advanced students: As a general rule, when XW is in the model and b3 is statistically significant, keep X and W in the model as well (even if inferential tests suggest otherwise). There are very little exceptions from this rule and you can read all about them in Hayes, 2022, if this should ever become relevant for you. However, if b3 is not statistically significant, you should omit XW altogether and perform multiple linear regression instead. 14.4 Example That was a lot of high-level statistical input! Lets take a mental break and turn to an actual example to help you understand the main concepts and how to interpret the model. Once again, well use to the glbwarm data. This time, well create a new variable and recode an old one for this tutorial, so please run this code first: library(processR) data &lt;- processR::glbwarm data &lt;- data %&gt;% mutate(ideology_ext = case_when( ideology == 1 ~ 4, ideology == 2 ~ 3, ideology == 3 ~ 2, ideology == 4 ~ 1, ideology == 5 ~ 2, ideology == 6 ~ 3, ideology == 7 ~ 4)) %&gt;% mutate(conservative_voter = case_when( partyid == 1 ~ &quot;Liberal voter&quot;, partyid == 2 ~ &quot;Liberal voter&quot;, partyid == 3 ~ &quot;Conservative voter&quot;)) %&gt;% mutate(conservative_voter = factor(conservative_voter, levels = c(&quot;Liberal voter&quot;, &quot;Conservative voter&quot;))) 14.4.1 Knowing your data You´ll now have three variables in this data set that we want to continue working with because they are great to learn moderation analysis with. govact (= support for government action): The variable is an index of how each participant answered five questions regarding how supportive he or she is of different policies or activities taken by the US government to address the threat of global climate change (e.g. How much do you support or oppose increasing government investment for developing alternative energy like biofuels, wind, or solar by 25%?). Response options were scaled from Strongly opposed (coded 1) or Strongly support (7). ideology_ext (= viewpoint extremity): The political ideology of respondents was measured on a liberalism-conservatism scale. Viewpoint extremity was calculated as the absolute difference of each respondents individual score from the liberalism-conservatism scale mean (4), resulting in an extremity score that ranged from 1 no extremity to 4 maximum extremity, conservative_voter (= being Republican, yes/no): This variable measured self-identification as a voter of the U.S. Republican party, i.e. being a coservative voter. It is a binary / dichotomous variable with 1 = Conservative voter and 0 = Liberal voter. Throughout this chapter, the following research question will guide us on our way to master mediation analysis: Research Question: Is the relationship between viewpoint extremity and support for government action against climate change dependent on party identification, i.e. on voting for the Republican party? In other words, does the relationship between holding extreme viewpoints and support for government action on climate change differ between conservative and liberal voters? 14.4.2 Visual inspection of linear trends Our interest is in whether the effect of viewpoint extremity (X) on support for government action against climate change (Y) depends on on voting for the Republican party (W). Once again, to answer a research question, its extremely useful to have a look at your variables prior to your analysis. So lets start by examining the relationship between viewpoint extremity, i.e. holding extreme political viewpoints, and support for government action against climate change: library(ggpubr) data %&gt;% ggplot(aes(x=ideology_ext, y=govact)) + geom_count() + theme_bw() + labs(x=&quot;Viewpoint extremity&quot;, y=&quot;Support for government action against climate change&quot;) + geom_smooth(method=&#39;lm&#39;, formula= y~x, color = &quot;darkred&quot;) + stat_regline_equation(label.y = 1.4) Evaluation: It is evident that viewpoint extremity and support for government action against climate change share a negative relationship.Individuals who hold more extreme political viewpoints are less likely to support for government action against climate change. What could be the explanation for a negative relationship? Perhaps U.S. citizens with extreme ideological attitudes have a poorer relationship with governments in general? It could be that radicalized citizens distrust government more and therefore do not want to support government action, even in the case of climate change. To test this hypothesis, we would need to conduct a mediation analysis with a mediator variable called distrust in government to determine whether distrust in government is indeed the missing link. However, our data does not provide such a variable. Too bad. But there might also be another explanation. Liberal voters care more about climate change, while conservative voters do not think climate change is an important political issue. Maybe the support for government action on climate change might differ between these to voter groups? Lets make a visual check of this hypothesis by looking at the regression lines of both voter groups: library(ggpubr) data %&gt;% ggplot(aes(x=ideology_ext, y=govact, color = conservative_voter)) + geom_count() + scale_color_manual(values = c(&quot;Liberal voter&quot; = &quot;mediumblue&quot;, &quot;Conservative voter&quot; = &quot;#CC0000&quot;)) + theme_bw() + theme(legend.position = &quot;none&quot;) + labs(x=&quot;Viewpiont extremity&quot;, y=&quot;Support for government action against climate change&quot;) + geom_smooth(method=&#39;lm&#39;, formula= y~x, color = &quot;#404040&quot;) + stat_regline_equation(label.y = 1.4) + facet_wrap(~conservative_voter) Instead of the regressions, you can also compare the mean values of the two groups if you like this form of presentation better: library(ggpubr) data %&gt;% ggline(x = &quot;ideology_ext&quot;, y = &quot;govact&quot;, color = &quot;conservative_voter&quot;, add = c(&quot;mean_ci&quot;), palette = c(&quot;mediumblue &quot;, &quot;#CC0000&quot;), linetype = &quot;conservative_voter&quot;, shape = &quot;conservative_voter&quot;, size=1.0, legend = &quot;right&quot;, ylab = &quot;Support for goverment action against climate change&quot;, xlab = &quot;Viewpiont extremity&quot;, legend.title = &quot;&quot;, title=&quot;&quot;) + theme_bw() Evaluation: Obviously, there is an interaction at play here! The effect of viewpoint extremity on support for government action against climate change seems to be dependent on voting for the U.S. Republican party, i.e. being a liberal or a conservative voter. To be precise, moderate liberal and conservative voters, i.e. those who have low viewpoint extremity, do not differ in their support for government action against climate change. But extreme liberal and conservative voters, i.e. those who have high viewpoint extremity, differ significantly in their support for government action against climate change. While extreme liberal voters become more supportive of government action, extreme conservative voters become more opposed against government actions. In fact, viewpoint extremity has a positive effect on support for government action against climate change for liberal voters (b = 0.18), but a negative effect on support for government action against climate change for conservative voters (b = -0.53). 14.4.3 Fit models Now that we have a visual indicator that voting for the Republican party might be a relevant moderator of the relationship between viewpoint extremity and support for government action against climate change (it does change the direction of influence from negative to positive!), we should run a linear regression model to test these effects for their statistical significance. The linear model must include both the X (viewpoint extremity) and the W variable (conservative_voter) and their product XW (viewpoint extremity * conservative_voter). Lets run such a model with the lm() function. summary(lm(govact ~ ideology_ext+conservative_voter+ideology_ext*conservative_voter,data=data)) ## ## Call: ## lm(formula = govact ~ ideology_ext + conservative_voter + ideology_ext * ## conservative_voter, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.2790 -0.7421 0.0789 0.8579 3.6728 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.56317 0.11337 40.249 &lt; 0.0000000000000002 *** ## ideology_ext 0.17895 0.05268 3.397 0.000715 *** ## conservative_voterConservative voter 0.67547 0.22961 2.942 0.003356 ** ## ideology_ext:conservative_voterConservative voter -0.70680 0.09098 -7.769 0.0000000000000239 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.237 on 811 degrees of freedom ## Multiple R-squared: 0.1767, Adjusted R-squared: 0.1737 ## F-statistic: 58.02 on 3 and 811 DF, p-value: &lt; 0.00000000000000022 Evaluation: We can infer the regression equation from this output: Y = 4.563 + 0.178 * X + 0.675 * W -0.706 * XW, where i[Y] = 4.563, b1 = 0.178, b2 = 0.675, and b3 = -0.706. Most importantly, b3 is statistically different from zero, t(811) = -7.769, p &lt; .001. This means that we can conclude that the effect of viewpoint extremity on support for government action against climate change is moderated by vote choice, i.e. voting for the Republican party. We now know that viewpoint extremity has a different effect on support for government action for different political groups. Thats an important finding! Had there been no evidence of moderation (i.e., b3 was not statistically different from zero), the most sensible approach would be to reestimate the model excluding the product XW (i.e. running a multiple linear regression!), thereby allowing Xs effect to be invariant across W. The interaction effects b3 coefficient quantifies how the effect of X on Y changes as W changes by one unit. Here, as vote choice changes by one point, i.e. from liberal voters (0) to conservative voters (1), the direction of the effect changes from positive (for liberal voters) to negative (for conservative voters), since b3 is negative. Specifically, conservative voters are estimated to drop -0.706 lower in their support for government action when there viewpoint extremity increases by one point. b1 and b2 are conditional effects. These regression coefficients estimate the effect of X when W = 0 and the effect of W when X = 0, respectively. Youll realize soon that these coefficients arent always sensible, for instance, viewpoint extremity (X) cant even become 0 on our chosen scale that ranges from 1 to 4. Well discuss that in a second. The regression coefficient for the conditional effect of X is b1 = 0.178 (p &lt; .001). This is the estimated difference in support for government action against climate change between two individuals who differ by one unit in their viewpoint extremity, but both score zero on the conservative_voter variable, i.e. are liberal voters. This score is positive, which implies that liberals who have a one unit increase in their viewpoint extremity are estimates to have a 0.178 increase in their support for government action. The regression coefficient for the conditional effect of W is b2 = 0.675 (p &lt; .01).This is the estimated difference in support for government action against climate change between liberals and conservatives who score zero on viewpoint extremity (X = 0), i.e. they are estimated to differ by 0.675 points. Although this interpretation is mathematically correct, substantively it is nonsense because the measure of viewpoint extremity in this study is mathematically bound between 1 and 4. An estimate of the effect of vote choice conditioned on a score of zero on viewpoint extremity has no meaning because no such people could even exist. 14.4.4 Standardization You might already have guessed it, but a good way to get meaningful regression coefficients for variables that are not mathematically between 0 and a finite value is to standardize. Lets run the model again with a standardized viewpoint extremity (do not standardize the binary vote choice variable conservative_voter!): data$ideology_ext_st &lt;- scale(data$ideology_ext) summary(lm(govact ~ ideology_ext_st+conservative_voter+ideology_ext_st*conservative_voter,data=data)) ## ## Call: ## lm(formula = govact ~ ideology_ext_st + conservative_voter + ## ideology_ext_st * conservative_voter, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.2790 -0.7421 0.0789 0.8579 3.6728 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.93797 0.05362 92.100 &lt; 0.0000000000000002 *** ## ideology_ext_st 0.18709 0.05508 3.397 0.000715 *** ## conservative_voterConservative voter -0.80491 0.09758 -8.248 0.000000000000000645 *** ## ideology_ext_st:conservative_voterConservative voter -0.73895 0.09512 -7.769 0.000000000000023933 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.237 on 811 degrees of freedom ## Multiple R-squared: 0.1767, Adjusted R-squared: 0.1737 ## F-statistic: 58.02 on 3 and 811 DF, p-value: &lt; 0.00000000000000022 Evaluation: With a standardized viewpoint extremity variable, you can now interpret scores of X = 0 as the mean (because standardization includes centering variables on their mean, i.e. the mean becomes zero!). b2 = -0.804 (p &lt; .001) is the estimated difference in support for government action against climate change between liberals and conservatives who score average on viewpoint extremity (X = 0). Conservatives who score average on viewpoint extremity are estimated to score -0.804 points lower on support for government action than liberals who score average on viewpoint extremity. Note: Please remember that when you standardize a variable, the meaning of a one unit increase in that variable changes, i.e. it becomes a one-standard deviation increase. For example, b1 = 0.187 (p &lt; .001) now changes its meaning: Two liberal voters who have a one point standard deviation difference in their viewpoint extremity are estimated to differ by 0.187 points in their support for government action against climate change. 14.4.5 Using processR Finally, you can use the easy road and do everything with the processR function. Before using the function, you need to (maybe download again, heres the link, and) run the processR script again (this can take a few minutes) that youve already used in the 13th Tutorial: Mediation analysis. Once youve created the process function, well perform the moderation analysis again, but this time we will use processR. Unfortunatey, processR does not like factor variables, so we need to recode our conservative_voter variable to be a binary numerical variable first. data &lt;- data %&gt;% mutate(conservative_voter_bin = case_when( conservative_voter == &quot;Liberal voter&quot; ~ 0, conservative_voter == &quot;Conservative voter&quot; ~ 1)) The output of process is pretty much self-explanatory when youve run the moderation model manually before. We can infer the regression equation: Y = 4.563 + 0.178 * X + 0.675 * W -0.706 * XW, where i[Y] = 4.563, b1 = 0.178, b2 = 0.675, and b3 = -0.706 and you can see which of these coefficients are significant (p-values). In addition, in the section Test(s) of highest order unconditional interaction(s) you get information about whether the inclusion of the interaction effect XW does significantly increase your R2 value. In this case, it does. Compared to a multiple linear regression model, including XW, i.e. assuming a moderation, does enhance R2 by +0.0613 (p &lt; .001). Obviously, the model that includes the interaction is the better fitting model! In the section about the conditional effects, you can see the mean values (conditioned on X = 0, which is nonsensical without standardization!) for liberals (W = 0) and conservatives (W = 1) and whether these mean differences are statistically significant (p-values). Down below, you also get values that show you how X, W, and Y behave on different levels of the other variables (you can use these values for plotting diagrams). 14.5 Take Aways explaning the when and circumstances: a moderation model helps you to determine the situations, events, or groups of people for which the impact of X on Y changes process: calculate the moderation model either by using the classic lm() function (multiple linear regression with a product of X and W) or by using the process function of the process script by A. F. Hayes standardization: Is a great way to get meaningful regression coefficients for variables that are not mathematically between 0 and a finite value. 14.6 More tutorials on this You still have questions? The following book can help you with that: Hayes, A. F. (2022). Introduction to Mediation, Moderation and Conditional Process Analysis (p. 233-281) Link You should be aware that unlike mediation, where statistical equations and conceptual diagrams match perfectly, the conceptual diagram of a moderation model is different from the corresponding statistical diagram. "],["solutions.html", "Solutions Solutions for Exercise 1 Solutions for Exercise 2 Solutions for Exercise 3 Solutions for Exercise 4", " Solutions This is where youll find solutions for all of the tutorials. Solutions for Exercise 1 Task 1 Below you will see multiple choice questions. Please try to identify the correct answers. 1, 2, 3 and 4 correct answers are possible for each question. 1. What panels are part of RStudio? Solution: source (x) console (x) packages, files &amp; plots (x) 2. How do you activate R packages after you have installed them? Solution: library() (x) 3. How do you create a vector in R with elements 1, 2, 3? Solution: c(1,2,3) (x) 4. Imagine you have a vector called vector with 10 numeric elements. How do you retrieve the 8th element? Solution: vector[8] (x) 5. Imagine you have a vector called hair with 5 elements: brown, black, red, blond, other. How do you retrieve the color blond? Solution: hair[4] (x) Task 2 Create a numeric vector with 8 values and assign the name age to the vector. First, display all elements of the vector. Then print only the 5th element. After that, display all elements except the 5th. Finally, display the elements at the positions 6 to 8. Solution: age &lt;- c(65,52,73,71,80,62,68,87) age ## [1] 65 52 73 71 80 62 68 87 age[5] ## [1] 80 age[-5] ## [1] 65 52 73 71 62 68 87 age[6:8] ## [1] 62 68 87 Task 3 Create a non-numeric, i.e. character, vector with 4 elements and assign the name eye_color to the vector. First, print all elements of this vector to the console. Then have only the value in the 2nd element displayed, then all values except the 2nd element. At the end, display the elements at the positions 2 to 4. Solution: eye_color &lt;- c(&quot;blue&quot;, &quot;green&quot;, &quot;brown&quot;, &quot;other&quot;) eye_color ## [1] &quot;blue&quot; &quot;green&quot; &quot;brown&quot; &quot;other&quot; eye_color[2] ## [1] &quot;green&quot; eye_color[-2] ## [1] &quot;blue&quot; &quot;brown&quot; &quot;other&quot; eye_color[2:4] ## [1] &quot;green&quot; &quot;brown&quot; &quot;other&quot; Task 4 Create a data frame called data. The data frame should contain the following variables (in this order): a vector called food. It should contain 5 elements, namely the names of your five favorite dishes. a vector called description. For every dish mentioned in food, please describe the dish in a single sentence (for instance, if the first food you describe is pizza, you could write: This is an Italian dish, which I prefer with a lot of cheese.) a vector called rating. Rate every dish mentioned in food with 1-5 (using every number only once), i.e., by rating your absolute favorite dish out of all five with a 1 and your least favorite dish out of all five with a 5. Solution: data &lt;- data.frame(&quot;food&quot; = c(&quot;pizza&quot;, &quot;pasta&quot;, &quot;ice cream&quot;, &quot;crisps&quot;, &quot;passion fruit&quot;), &quot;description&quot; = c(&quot;Italian dish, I actually prefer mine with little cheese&quot;, &quot;Another Italian dish&quot;, &quot;The perfect snack in summer&quot;, &quot;Potatoes and oil - a luxurious combination&quot;, &quot;A fruit that makes me think about vacation&quot;), &quot;Rating&quot; = c(3,1,2,4,5)) data ## food description Rating ## 1 pizza Italian dish, I actually prefer mine with little cheese 3 ## 2 pasta Another Italian dish 1 ## 3 ice cream The perfect snack in summer 2 ## 4 crisps Potatoes and oil - a luxurious combination 4 ## 5 passion fruit A fruit that makes me think about vacation 5 Task 5 Can you sort the data in your data set by rating - with your favorite dish (i.e., the one rated 1) on top of the list and your least favorite dish (i.e., the one rated 5) on the bottom? Important: You do not yet know this command - youll have to google for the right solution. Please do and note down the exact search terms you used for googling. Solution: library(&quot;dplyr&quot;) data &lt;- data %&gt;% arrange(Rating) data ## food description Rating ## 1 pasta Another Italian dish 1 ## 2 ice cream The perfect snack in summer 2 ## 3 pizza Italian dish, I actually prefer mine with little cheese 3 ## 4 crisps Potatoes and oil - a luxurious combination 4 ## 5 passion fruit A fruit that makes me think about vacation 5 Solutions for Exercise 2 Task 1 Below you will see multiple choice questions. Please try to identify the correct answers. 1, 2, 3 and 4 correct answers are possible for each question. 1. What are the main characteristics of tidy data? Solution: Every observation is a row. (x) 2. What are dplyr functions? Solution: mutate() (x) 3. How can you sort the eye_color of Star Wars characters from Z to A? Solution: starwars_data %&gt;% arrange(desc(eye_color)) (x) starwars_data %&gt;% select(eye_color) %&gt;% arrange(desc(eye_color)) (x) 4. Imagine you want to recode the height of the these characters. You want to have three categories from small and medium to tall. What is a valid approach? Solution: starwars_data %&gt;% mutate(height = case_when(height&lt;=150~\"small\",height&lt;=190~\"medium\",height&gt;190~\"tall\")) (x) 5. Imagine you want to provide a systematic overview over all hair colors and what species wear these hair colors frequently (not accounting for the skewed sampling of species)? What is a valid approach? Solution: starwars_data %&gt;% group_by(hair_color, species) %&gt;% summarize(count = n()) %&gt;% arrange(hair_color) (x) Task 2 Now its you turn. Load the starwars data like this: library(dplyr) # to activate the dplyr package starwars_data &lt;- starwars # to assign the pre-installed starwars data set (dplyr) into a source object in our environment How many humans are contained in the starwars data overall? (Hint: use summarize(count = n()) or count())? Solution: You can use summarize(count = n()): starwars_data %&gt;% filter(species == &quot;Human&quot;) %&gt;% summarize(count = n()) ## # A tibble: 1 x 1 ## count ## &lt;int&gt; ## 1 35 Alternatively, you can use the count() function: starwars_data %&gt;% filter(species == &quot;Human&quot;) %&gt;% count(species) ## # A tibble: 1 x 2 ## species n ## &lt;chr&gt; &lt;int&gt; ## 1 Human 35 Task 3 How many humans are contained in starwars by gender? Solution: You can use summarize(count = n()): starwars_data %&gt;% filter(species == &quot;Human&quot;) %&gt;% group_by(species, gender) %&gt;% summarize(count = n()) ## # A tibble: 2 x 3 ## # Groups: species [1] ## species gender count ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Human feminine 9 ## 2 Human masculine 26 Alternatively, you can use the count() function: starwars_data %&gt;% filter(species == &quot;Human&quot;) %&gt;% count(species, gender) ## # A tibble: 2 x 3 ## species gender n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Human feminine 9 ## 2 Human masculine 26 Task 4 What is the most common eye_color among Star Wars characters? (Hint: use arrange())__ Solution: starwars_data %&gt;% group_by(eye_color) %&gt;% summarize(count = n()) %&gt;% arrange(desc(count)) ## # A tibble: 15 x 2 ## eye_color count ## &lt;chr&gt; &lt;int&gt; ## 1 brown 21 ## 2 blue 19 ## 3 yellow 11 ## 4 black 10 ## 5 orange 8 ## 6 red 5 ## 7 hazel 3 ## 8 unknown 3 ## 9 blue-gray 1 ## 10 dark 1 ## 11 gold 1 ## 12 green, yellow 1 ## 13 pink 1 ## 14 red, blue 1 ## 15 white 1 Task 5 What is the average mass of Star Wars characters that are not human and have yellow eyes? (Hint: remove all NAs)__ Solution: starwars_data %&gt;% filter(species != &quot;Human&quot; &amp; eye_color==&quot;yellow&quot;) %&gt;% summarize(mean_mass = mean(mass, na.rm=TRUE)) ## # A tibble: 1 x 1 ## mean_mass ## &lt;dbl&gt; ## 1 74.1 Task 6 Compare the mean, median, and standard deviation of mass for all humans and droids. (Hint: remove all NAs)__ Solution: starwars_data %&gt;% filter(species==&quot;Human&quot; | species==&quot;Droid&quot;) %&gt;% group_by(species) %&gt;% summarize(M = mean(mass, na.rm = TRUE), Med = median(mass, na.rm = TRUE), SD = sd(mass, na.rm = TRUE) ) ## # A tibble: 2 x 4 ## species M Med SD ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Droid 69.8 53.5 51.0 ## 2 Human 82.8 79 19.4 Task 7 Create a new variable in which you store the mass in gram. Add it to the data frame. Solution: starwars_data &lt;- starwars_data %&gt;% mutate(gr_mass = mass*1000) starwars_data %&gt;% select(name, species, mass, gr_mass) ## # A tibble: 87 x 4 ## name species mass gr_mass ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Luke Skywalker Human 77 77000 ## 2 C-3PO Droid 75 75000 ## 3 R2-D2 Droid 32 32000 ## 4 Darth Vader Human 136 136000 ## 5 Leia Organa Human 49 49000 ## 6 Owen Lars Human 120 120000 ## 7 Beru Whitesun lars Human 75 75000 ## 8 R5-D4 Droid 32 32000 ## 9 Biggs Darklighter Human 84 84000 ## 10 Obi-Wan Kenobi Human 77 77000 ## # ... with 77 more rows Solutions for Exercise 3 Task 1 Try to reproduce this plot with dplyr and ggplot2. (Hint: You can hide the legend by adding theme(legend.position = \"none\") to your plot.) Solution: data %&gt;% mutate(sex = case_when( sex == 0 ~ &quot;Female&quot;, sex == 1 ~ &quot;Male&quot;)) %&gt;% mutate(Party = case_when( partyid == 1 ~ &quot;Democrat&quot;, partyid == 2 ~ &quot;Independent&quot;, partyid == 3 ~ &quot;Republican&quot;)) %&gt;% ggplot(aes(x=Party,y=negemot, fill=Party)) + stat_summary(geom = &quot;bar&quot;, fun = &quot;mean&quot;) + theme_bw() + theme(legend.position = &quot;none&quot;) + labs(title = &quot;Climate change attitudes of U.S. partisans by gender&quot;, y = &quot;Negative emotions about climate change&quot;) + facet_wrap(~sex, nrow=2) Task 2 Now, try to reproduce this graph. (Hint: You will need to recode the ideology variable in a way that higher values represent stronger attitudes, independent of partisanship.) Solution: data &lt;- data %&gt;% mutate(ideology_ext = case_when( ideology == 1 ~ 4, ideology == 2 ~ 3, ideology == 3 ~ 2, ideology == 4 ~ 1, ideology == 5 ~ 2, ideology == 6 ~ 3, ideology == 7 ~ 4)) %&gt;% mutate(sex = case_when( sex == 0 ~ &quot;Female&quot;, sex == 1 ~ &quot;Male&quot;)) %&gt;% mutate(Party = case_when( partyid == 1 ~ &quot;Democrat&quot;, partyid == 2 ~ &quot;Independent&quot;, partyid == 3 ~ &quot;Republican&quot;)) data %&gt;% ggplot(aes(x=Party,y=ideology_ext, fill=Party)) + geom_boxplot() + theme_bw() + theme(legend.position = &quot;none&quot;) + labs(title = &quot;Ideological extremity of U.S. partisans by gender&quot;, y = &quot;Ideological extremity&quot;) + facet_wrap(~sex) Task 3 Can you make a chart that breaks down the relationship between age, negative emotions about climate change, and ideological extremity for the different sexes AND parties? Solution 1: data %&gt;% ggplot(aes(x=age,y=negemot, size=ideology_ext, color = Party)) + geom_point() + scale_size(range = c(0.3, 3), name = &quot;Ideological extremity&quot;) + theme_bw() + labs(title = &quot;Relationship between age, climate change attitudes, \\n and ideological extremity&quot;, x = &quot;Age&quot;, y = &quot;Negative emotions about climate change&quot;) + facet_wrap(~sex, nrow=2) Solution 2: Alternatively, you might enjoy this look that you can create with facet_grid(): data %&gt;% ggplot(aes(x=age,y=negemot, size=ideology_ext, color = Party)) + geom_point() + scale_size(range = c(0.3, 3), name = &quot;Ideological extremity&quot;) + theme_bw() + labs(title = &quot;Relationship between age, climate change attitudes, and ideological extremity&quot;, x = &quot;Age&quot;, y = &quot;Negative emotions about climate change&quot;) + facet_grid(vars(sex), vars(Party)) Solution 3: Or even this look, also done with facet_grid(): data %&gt;% ggplot(aes(x=age,y=negemot, size=ideology_ext, color = Party)) + geom_point() + scale_size(range = c(0.3, 3), name = &quot;Ideological extremity&quot;) + theme_bw() + labs(title = &quot;Relationship between age, climate change attitudes, and ideological extremity&quot;, x = &quot;Age&quot;, y = &quot;Negative emotions about climate change&quot;) + facet_grid(~sex + Party) Solutions for Exercise 4 Task 1 Lets use the data set glbwarm again, which you should know well by now. Install / activate the processR package and assign the glbwarm data to a source object. # installing/loading the package: if(!require(processR)) { install.packages(&quot;processR&quot;); require(processR) } #load / install+load processR data &lt;- processR::glbwarm In this task, we want to tackle simple linear regression. More specifically, we want to predict the ideology of our respondents by their age because we assume that older respondents will hold more conservative viewpoints. The higher the values of the ideology variable, the more conservative the respondents are (coded from 1 very liberal to 7 very conservative). Research question: Do older U.S. Americans hold more conservative viewpoints than younger U.S. Americans? To answer this question, prepare a visual inspection of this relationship without fitting a regression line. Can you recognize a relationship? What is its nature? Solution: data %&gt;% ggplot(aes(x=age, y=ideology)) + geom_count() + theme_bw() + labs(x=&quot;Age&quot;, y=&quot;Conservatism&quot;) Evaluation: We can observe a small, positive relationship between age and conservatism. (This can be inferred from the amount of bigger bubbles clustering on the top right corner of the graph.) Task 2 Next, try to quantify the association using Pearsons r. Interpret the result. Solution: cor.test(glbwarm$ideology,glbwarm$age, method=&quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: glbwarm$ideology and glbwarm$age ## t = 6.1978, df = 813, p-value = 0.0000000009096 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.1458602 0.2770376 ## sample estimates: ## cor ## 0.2124056 Evaluation: There is a small, positive, and highly significant relationship between age and conservatism (r = 0.21, p &lt; 0.001). We can conclude that older U.S. Americans are more conservative. Task 3 Using your graph from Task 1, fit a regression line to your data points (Hint: You will need to load the ggpubr package). Interpret the parameters of the regression line. Solution: # installing/loading the package: if(!require(ggpubr)) { install.packages(&quot;ggpubr&quot;); require(ggpubr) } #load / install+load ggpubr data %&gt;% ggplot(aes(x=age, y=ideology)) + geom_count() + theme_bw() + # xlim(0,7) + # ylim(0,7.5) + labs(x=&quot;Age&quot;, y=&quot;Conservatism&quot;) + geom_smooth(method=&#39;lm&#39;, formula= y~x, color = &quot;darkred&quot;) + stat_cor(aes(label = ..rr.label..)) + stat_regline_equation(label.y = 6.2) Evaluation: The equation is Y = 3.1 + 0.02*X. This means that a person who becomes one year older is estimated to become 0.02 points more conservative. Similarly, two U.S. Americans with an age difference of 10 years are estimated to differ by 0.2 points on the ideology scale. However, age is not a really good predictor of conservatism as the predictor only explains about 4.5% of the observed variance in conservatism scores (R2). Task 4 Run a linear model in R using the ideology and age variables. Interpret the results. Solution: summary(lm(ideology ~ age,data=data)) ## ## Call: ## lm(formula = ideology ~ age, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.7807 -0.8187 0.0647 0.8091 3.3793 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.109423 0.165465 18.792 &lt; 0.0000000000000002 *** ## age 0.019663 0.003173 6.198 0.00000000091 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.478 on 813 degrees of freedom ## Multiple R-squared: 0.04512, Adjusted R-squared: 0.04394 ## F-statistic: 38.41 on 1 and 813 DF, p-value: 0.0000000009096 Evaluation: Again, we can extract the parameters for the intercept (b[0] = 3.1), the regression coefficient (b[1] = 0.019, p &lt; 0.001), and R2 = 0.045. There is a significant, positive relationship between age and conservatism, but the effect is very small (size of b[1]). Task 5 Since age alone does not seem to be a good predictor of conservatism, we want to introduce other predictors into the model and run a Multiple Linear Regression. This means that we will predict the effect of age on conservatism while controlling for the effect of third variables. For example, the respondents gender (sex, 0 = female, 1 = male) and their party preference (partyid, 1 = Democrat, 2 = Independent, 3 = Republican) might be great predictors of conservatism. Note: In a linear regression model, we can only include metric variables and variables that are binary coded (0/1). However, partyid is a categorical, i.e. factor variable, since Democrats are coded 1, Independents 2, and Republicans 3. Therefore, you need to mutate partyid and create two new binary variables democrat (0/1) and republican (0/1), where 1 indicates that the respondent identifies with that political party. (You dont need to create a variable independent, since that information would be redundant: someone who has a value of 0 for both republican AND democrat MUST be an independent, so you can derive party preference with just two variables). Then, run a multiple linear model that predicts ideology by sex, democrat, republican, and age. Interpret the results and the meaning of the age coefficient. Solution: data &lt;- data %&gt;% mutate(democrat = case_when( partyid == 1 ~ 1, partyid == 2 ~ 0, partyid == 3 ~ 0)) %&gt;% mutate(republican = case_when( partyid == 1 ~ 0, partyid == 2 ~ 0, partyid == 3 ~ 1) ) summary(lm(ideology ~ sex + republican + democrat + age,data=data)) ## ## Call: ## lm(formula = ideology ~ sex + republican + democrat + age, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.0975 -0.9313 0.0256 0.7641 3.9739 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.504712 0.146041 23.998 &lt; 0.0000000000000002 *** ## sex 0.158677 0.083620 1.898 0.058105 . ## republican 1.251718 0.113609 11.018 &lt; 0.0000000000000002 *** ## democrat -0.848157 0.105368 -8.049 0.00000000000000296 *** ## age 0.009475 0.002610 3.630 0.000301 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.171 on 810 degrees of freedom ## Multiple R-squared: 0.4031, Adjusted R-squared: 0.4002 ## F-statistic: 136.8 on 4 and 810 DF, p-value: &lt; 0.00000000000000022 Evaluation: Compared to our simple model, the R2 has increased dramatically, from 0.045 to 0.403! We can now explain 40.3% of the variance in conservatism with the newly introduced covariates. While gender is not a significant predictor of conservatism (b[1] = 0.158, p = 0.058), i.e., there is no significant difference in conservatism between men and women, party preference plays a large role in explaining conservative viewpoints. Republicans are more conservative (b[2] = 1.251, p &lt; 0.001) than Independents (reference category, not included as a separate variable), while Democrats are less conservative than Independents (b[3] = -0.848, p &lt; 0.001). The size of the effect that age has on conservatism has decreased further now that we control for sex and party preference (b[4] = 0.009, p &lt; 0.001). This implies that some of the effects of age are now transported through sex and, more likely, party preference. We can conclude that two U.S. citizens who differ by 10 years but have the same gender and party preference are estimated to differ by 0.09 points on the conservatism scale. Thats a really small effect, but its still significant. Task 6 Standardize all relevant variables and run the model again (note that binary variables shouldnt be standardized). How does the interpretation of the age coefficient change? Solution: data$ideology_st &lt;- scale(data$ideology) data$age_st &lt;- scale(data$age) summary(lm(ideology_st ~ sex + republican + democrat + age_st,data=data)) ## ## Call: ## lm(formula = ideology_st ~ sex + republican + democrat + age_st, ## data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.71045 -0.61607 0.01696 0.50543 2.62866 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.07233 0.06386 -1.133 0.257702 ## sex 0.10496 0.05531 1.898 0.058105 . ## republican 0.82799 0.07515 11.018 &lt; 0.0000000000000002 *** ## democrat -0.56104 0.06970 -8.049 0.00000000000000296 *** ## age_st 0.10236 0.02819 3.630 0.000301 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7745 on 810 degrees of freedom ## Multiple R-squared: 0.4031, Adjusted R-squared: 0.4002 ## F-statistic: 136.8 on 4 and 810 DF, p-value: &lt; 0.00000000000000022 Evaluation: The age coefficient is now expressed in standard deviations from the mean: Two U.S. Americans with one standard deviation difference in their age are expected to rank b[4] = 0.102 (p &lt; 0.001) standard deviations higher on the conservatism scale, when controlling for the influence of sex and party preference. If we had a second standardized coefficient in the model, then we could compare their effect sizes directly. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
